\documentclass[a4paper]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\selectlanguage{italian}
\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{circuitikz}
\usetikzlibrary{positioning, circuits.logic.US}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary {shapes.gates.logic.US, shapes.gates.logic.IEC, calc}
\tikzset {branch/.style={fill, shape = circle, minimum size = 3pt, inner sep = 0pt}}
\usetikzlibrary{matrix,calc}
\usepackage{multirow}
\usepackage{float}
\usepackage{geometry}
\usepackage{tabularx}
\usepackage{pgf-pie}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color, soul}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{subfig}
\graphicspath{ {./img/} }
\newtheorem{theorem}{Teorema}[section]
\newtheorem{corollary}{Corollario}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

% Specifiche
\geometry{
 a4paper,
 top=20mm,
 left=30mm,
 right=30mm,
 bottom=30mm
}

\nocite{}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\nouppercase{\leftmark}}
\fancyfoot[CE, CO]{\thepage}
\addtolength{\headheight}{1em}
\addtolength{\footskip}{-0.5em}

\newcommand{\quotes}[1]{``#1''}
\renewcommand\tabularxcolumn[1]{>{\vspace{\fill}}m{#1}<{\vspace{\fill}}}
\renewcommand\arraystretch{}
\newcolumntype{P}{>{\centering\arraybackslash}X}
\newcommand*\dif{\mathop{}\!\mathrm{d}}

\title{\textbf{Università di Trieste\\ \vspace{1em}
Laurea in ingegneria elettronica e informatica}}
\author{Enrico Piccin - Corso di Analisi matematica II - Prof. Franco Obersnel}
\date{Anno Accademico 2022/2023 - 3 Ottobre 2022}

\begin{document}

\vspace{-10mm}
\maketitle

\tableofcontents
\newpage
\noindent
\begin{center}
    3 Ottobre 2022
\end{center}

\vspace{1em}
\noindent
\section{Introduzione}
Considerando un foglio di carta, dividendolo in due metà esatte, si ottiene $\frac{1}{2}$ del profilo quadrato di partenza. Considerando una delle due metà, e suddividendola ancora in due, si ottiene $\frac{1}{4}$ del profilo quadrato di partenza.
Ripetendo questo procedimento, si otterranno le seguenti frazioni del profilo quadrato originario: $\frac{1}{8}, \frac{1}{16}, \frac{1}{32}, \frac{1}{64}, ...$. Sommando tutte le frazioni di profilo quadrato, alla fine si otterrà il profilo quadrato di partenza, ossia la frazione $1$.
Ecco quindi che, contrariamente a quanto voleva sostenere \textbf{Parmenide}, \textbf{Zenone} scoprì che
\[\boxed{\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\frac{1}{16}+\frac{1}{32}+\frac{1}{64}+...=1 \rightarrow \sum_{n=1}^{+\infty} \left(\frac{1}{2}\right)^n=1}\]
Ciò non risulta essere banale: una somma di \textbf{infinite quantità positive} produce una quantità finita. Quello che si è ottenuto è una \textbf{serie (numerica) geometrica di ragione $\frac{1}{2}$}.

\vspace{1em}
\section{Serie numerica}
Di seguito si espone la definizione di \textbf{serie numerica}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SERIE NUMERICA}}\\
    \parbox{\linewidth}{Data una successione $(a_n)_n$ con valori nel campo complesso $a_n \in \mathbb{C}$. Si consideri una nuova successione $(s_n)_n$ definita \textbf{per ricorrenza} come segue
    \[s_{n+1}=s_n+a_{n+1} \hspace{1em} \text{ posto } \hspace{1em} s_0=a_0\]
    Ciò significa che
    \begin{itemize}
        \item $s_0 = a_0$
        \item $s_1 = a_0 + a_1$
        \item $s_2 = a_0 + a_1 + a_2$
        \item e via di seguito...
    \end{itemize}
    La serie $a_0+a_1+a_2+...$ è la \textbf{coppia ordinata} delle due successioni, come mostrato di seguito
    \[\left((a_n)_n, (s_n)_n\right)\]
    ove la successione $(a_n)_n$ prende il nome \textbf{successioni dei termini generali}, mentre la successione $(s_n)_n$ si chiama successione delle \textbf{ridotte} o delle \textbf{somme parziali} della serie. \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio}: Posto $a_1=\frac{1}{2}$ e il termine generale $a_n=\left(\frac{1}{2}\right)^n$, la ridotta sarà
\[s_n=\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+...+\frac{1}{2^n}\]
osservando bene di partire da $n=1$ e non da $0$.

\vspace{1em}
\noindent
\subsection{Convergenza, divergenza e indeterminatezza di una serie}
Data una serie, ossia data una coppia di successioni, è possibile ora andare a studiare il comportamento della successione delle ridotte.

\vspace{1em}
\noindent
\subsubsection{Convergenza di una serie}
Di seguito si espone la definizione di \textbf{convergenza di una serie}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{CONVERGENZA DI UNA SERIE}}\\
    \parbox{\linewidth}{Se la successione delle ridotte di una serie è convergente, si dice che la serie è convergente e il limite della successione delle ridotte prende il nome di \textbf{somma della serie}.\\
    In altre parole, se \textbf{esiste finito} il
    \[\lim_{n \to +\infty} s_n = s \in \mathbb{C}\]
    allora la serie si dice \textbf{convergente} e il limite $s$ si dice \textbf{somma della serie} e si scrive
    \[\sum_{n=0}^{+\infty} a_n = s\]
    \textbf{Attenzione}: Molto spesso si utilizza la notazione sopra esposta per indicare sia la serie stessa, sia la sua somma, per cui può essere fuorviante. Lo si può capire dal contesto: una serie potrebbe non essere convergente, e quindi non avere una somma.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio}: Se si considera $a_n=1, \forall n$, per cui
\[1+1+1+... = \sum_{n=0}^{n} 1\]
allora la somma parziale è $s_n=n+1$, ovvero una successione divergente a $+\infty$:
\[\lim_{n \to +\infty} s_n = +\infty\]
Ciò significa che la serie non converge, ma è \textbf{divergente}, per cui non ha nemmeno una somma.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che la divergenza a $+\infty$ di una serie ha significato solamente quando i termini generali sono sul campo reale: se una serie ha termine generico nel campo complesso, non può essere divergente a $+\infty$, in quanto non esiste un limite infinito nel campo complesso (a meno che non si consideri il modulo).

\vspace{1em}
\noindent
\subsubsection{Divergenza di una serie}
Di seguito si espone la definizione di \textbf{divergenza di una serie}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{DIVERGENZA DI UNA SERIE}}\\
    \parbox{\linewidth}{Se la successione delle ridotte di una serie (a termine generale reale) è divergente, si dice che la serie è divergente; in questo caso, la serie non presenta una somma.\\
    In altre parole, se data $a_n \in \mathbb{R}, \forall n$, e posto
    \[\lim_{n \to +\infty} s_n = +\infty \text{ o } - \infty\]
    la serie si dice \textbf{divergente}.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio}: Se $a_n = a \in \mathbb{R}$ \textbf{costante}, allora la serie con termine generale $a_n$
\[a_0+a_1+a_2+...\]
è necessariamente 
\begin{itemize}
    \item divergente a $+\infty$ se $a > 0$
    \item divergente a $-\infty$ se $a < 0$
    \item convergente, con somma $0$, se $a = 0$
\end{itemize}
\textbf{Attenzione}: se $a \neq 0$, ma $a \in \mathbb{C} - \mathbb{R}$, si dice semplicemente che la serie \textbf{non converge} (non ha senso parlare di divergenza).


\vspace{1em}
\noindent
\subsubsection{Indeterminatezza di una serie}
Di seguito si espone la definizione di \textbf{serie indeterminata}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SERIE INDETERMINATA}}\\
    \parbox{\linewidth}{Una serie si dice \textbf{indeterminata} se non converge e non diverge.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio 1}: Per quello che si è visto, una serie a termine generale costante, complesso e non reale, è indeterminata.

\vspace{1em}
\noindent
\textbf{Esempio 2}: Un esempio di serie a termini reali, ma indeterminata, è la \textbf{serie di Grandi}, definita così:
\[\sum_{n=0}^{+\infty} (-1)^n\]
per cui $s_0=(-1)^0=1$ e $s_1=a_0+a_1=1+(-1)^1=0$. Pertanto si ha che
\begin{itemize}
    \item $s_n=1$ se $n$ è pari
    \item $s_n=0$ se $n$ è dispari
\end{itemize}
Per cui si ha che
\[\lim_{n \to +\infty} s_0 = ? \text{ non esiste}\]
E per dimostrare che non esiste, si può semplicemente dimostrare che due sotto-successioni della successione delle somme parziali convergono a limiti diversi (ossia la sotto-successioni degli indici pari e quella dei dispari); infatti:
\begin{itemize}
    \item $\displaystyle{\lim_{k \to +\infty} s_{2k} = 1}$
    \item $\displaystyle{\lim_{k \to +\infty} s_{2k+1} = 0}$
\end{itemize}
per cui sono state ottenute due sotto-successioni che presentano limite differente: per il teorema dell'unicità del limite e il teorema del limite delle sotto-successioni di una successione, si conclude che la successione delle somme parziali è indeterminata.

\vspace{1em}
\noindent
\textbf{Osservazione}: La serie di Grandi è una serie che può essere usata per dimostrare l'esistenza di Dio, in quanto commutando fra di loro i differenti termini può essere fatta convergere a qualsiasi (o quasi) numero finito.\\
Se, infatti, si considerano le somme
\begin{itemize}
    \item $(1-1)+(1-1)+(1-1)+...=0$
    \item $1+(-1+1)+(-1+1)+...=1$
    \item $(1+1)+(-1+1)+(-1+1)=2$
\end{itemize}
si ottengono serie che convergono a qualunque valore (tranne uno). In generale, infatti, se una serie è indeterminata, si possono commutare gli addendi della stessa e ottenere la convergenza a qualunque numero.

\vspace{1em}
\noindent
\subsection{Serie geometrica}
Si è osservato che
\[\sum_{n=1}^{+\infty} \left(\frac{1}{2}\right)^n=1\]
per cui è ovvio che partendo con $n=0$, si ottiene
\[\sum_{n=0}^{+\infty} \left(\frac{1}{2}\right)^n=2\]
Più in generale, si fornisce di seguito la definizione di \textbf{serie geometrica}: 

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SERIE GEOMETRICA}}\\
    \parbox{\linewidth}{Si dice \textbf{serie geometrica} di ragione $z \in \mathbb{C}$ la serie del tipo
    \[1+z+z^2+z^3+... \rightarrow \sum_{n=0}^{+\infty} z^n\]
    che, tuttavia, palesa un problema di fondo: se si sceglie $z=0$, naturalmente si incorre nell'ambiguità
    \[0^0 + 0^1 + ...\]
    ma $0^0$ è una scrittura che non ha significato. Tuttavia, in questo particolare caso, si considera $0^0=1$, in modo tale da essere coerenti con la scrittura $1+z+z^2+z^3+...$ impiegata in precedenza.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Osservazione}: Data la serie seguente
\[\sum_{n=0}^{+\infty} z^n\]
per cui la ridotta è
\[s_n=1+z+z^2+...+z^n\]
che può anche essere riscritto come
\[s_n=1+z+z^2+...+z^n=1+z \cdot \left(1+z+...+z^{n-1}\right)\]
dove $1+z+...+z^{n-1}=s_{n-1}$. Da cui si evince che, sommando e sottraendo per la medesima quantità $z^n$, si ottiene
\[s_n = 1+z \cdot \left(\underbrace{1+z+...+z^{n-1}+z^n}_{s_n} - z^n\right)\]
che diviene, quindi:
\[s_n = 1 + z \cdot s_n - z^{n+1} \hspace{1em} \rightarrow \hspace{1em} s_n - z \cdot s_n = 1 - z^{n+1} \hspace{1em} \rightarrow \hspace{1em} s_n \cdot (1-z) = 1 - z^{n+1} \hspace{1em} \rightarrow \hspace{1em} s_n = \frac{1-z^{n+1}}{1-z}\]
posto $z \neq 1$ (ma il caso $z=1$ è facilmente risolubile, per quanto osservato nel caso di una serie a termine generale costante).\\
Di seguito si espone, quindi, il comportamento della serie geometrica a seconda della sua ragione $z$:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{COMPORTAMENTO DELLA SERIE GEOMETRICA}}\\
    \parbox{\linewidth}{Per quanto osservato in precedenza, si ha che:
    \[\sum_{n=0}^{+\infty} z^n = \lim_{n \to +\infty} s_n = \lim_{n \to + \infty} \frac{1-z^{n+1}}{1-z}\]
    posto $z \neq 1$, che diviene
    \begin{itemize}
        \item $\displaystyle{\frac{1}{1-z}}$ se $\vert z \vert < 1$.
        \item non converge se $\vert z \vert > 1$, tuttavia, si può dire che
        \begin{itemize}
            \item se $z \in \mathbb{R}$ e $z \geq 1$, diverge a $+\infty$
            \item se $z \in \mathbb{C}$ e $\vert z \vert \geq 1$ (ovvero può essere anche un numero negativo), posto $z \notin \left]1,+\infty \right[$ (ossia diverso dal caso precedente), nel caso di $n$ pari si sommano quantità positive, nel caso di $n$ dispari si sommano quantità negative, per cui la serie oscilla e quindi è indeterminata.
        \end{itemize}
    \end{itemize}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione}: Si osservi che la serie geometrica è l'unica per cui si riesce a calcolare la somma, in quanto è l'unica di cui è possibile esprimere la ridotta in modo generale.\\
Altrimenti, gestire le ridotte diviene molto complesso.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la seguente serie
\[\sum_{n=2}^{+\infty} \cos^{n}(1)\]
che è una serie geometrica di ragione $\cos(1)$, ove $\left \vert \cos(1) \right \vert < 1$, per cui converge. La somma di tale serie, quindi, è facilmente determinabile secondo quanto visto in precedenza, tenendo conto che $n$ parte da 2, per cui bisogna sottrarre $\cos^0(1)=1$ e $\cos^1(1)=\cos(1)$. Da ciò si evince che la serie converge a 
\[\frac{1}{1 - \cos(1)} - 1 - \cos(1) = \frac{1 - 1 + \cos(1) - \cos(1) + \cos^2(1)}{1 - \cos(1)} = \frac{\cos^2(1)}{1 - \cos(1)}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: La somma della serie geometrica di ragione $z \in \mathbb{C}$ è indeterminata se $\vert z \vert > 1$, per quanto già visto.\\
Inoltre si ha che la serie
\[\sum_{n=1}^{+\infty} \left(\frac{2i + x}{4}\right)^n\]
è convergente se
\[\left \vert \frac{2i + x}{4}\right \vert < 1\]
ma ricordando come si calcola il modulo di un numero complesso si ottiene
\[\left \vert 2i + x \right \vert = \sqrt{4+x^2}\]
e quindi 
\[\sqrt{4+x^2} < 4 \hspace{1em} \rightarrow \hspace{1em} 4 + x^2 < 16 \hspace{1em} \rightarrow \hspace{1em} x^2 < 12 \hspace{1em} \rightarrow \hspace{1em} \vert x \vert < \sqrt{12} \hspace{1em} \rightarrow \hspace{1em} \vert x \vert < 2 \sqrt{3}\]
E poi, ovviamente, la serie di Grandi è il tipico esempio di serie indeterminata, per cui la sua somma non può essere definita.

\newpage
\noindent
\begin{center}
    5 Ottobre 2022
\end{center}
Una serie è costituita da $2$ successioni: la successione dei termini generali e la successione delle ridotte o somme parziali: quando si opera con le serie, risulta fondamentale distinguere le due successioni.\\
Una tra le serie più note è la serie geometrica, di ragione $z \in \mathbb{C}$, la quale converge se il modulo della ragione è minore di $1$. Non converge in caso contrario, ma in particolare
\begin{itemize}
    \item se la ragione $z$ è un numero reale, $z \in \mathbb{R}$, e $z \geq 1$, allora la serie diverge a $+\infty$;
    \item se la ragione $z$ è un numero complesso, con $\vert z \vert \geq 1$ e $z \notin ]1,+\infty[$, allora la serie è indeterminata.
\end{itemize} 
In generale, non si può parlare di divergenza a $+\infty$ o $-\infty$ in campo complesso, in quanto in esso è \textbf{assente la relazione d'ordine} e quindi non esiste un limite infinito.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri l'esempio seguente:
\[\sum_{n=0}^{+\infty} \frac{\cos(n)}{2^n}\]
Tale serie presenta come termine generale
\[a_n = \frac{\cos(n)}{2^n}\]
ma è vero che $-1 \leq \cos(n) \leq 1$, per cui
\[-\frac{1}{2^n} \leq a_n \leq \frac{1}{2^n}\]
Per dimostrare che anche la serie in esame converge, è sufficiente considerare $s_n^-$ e $s_n^+$, rispettivamente la ridotta $n$-esima della serie geometrica di ragione $-\frac{1}{2}$ e $\frac{1}{2}$, come segue
\[s_n^- = -1 - \frac{1}{2} - ... - \frac{1}{2^n} \hspace{1em} \text{e} \hspace{1em} s_n^+ = 1 + \frac{1}{2} + ... + \frac{1}{2^n}\]
per cui
\[s_n^- \leq s_n \leq s_n^+\]
e per il \textbf{teorema del confronto esiste finito} il seguente limite
\[\lim_{n \to +\infty} s_n \in \mathbb{R}\]
e quindi la serie
\[\sum_{n=0}^{+\infty} \frac{\cos(n)}{2^n}\]
converge.

\vspace{1em}
\noindent
\subsection{Teorema del confronto per le serie}
Di seguito si espone il fondamentale \textbf{teorema del confronto per le serie}:

\vspace{1em}
\noindent
\begin{theorem} \textbf{Teorema del confronto per le serie}\\
    Siano $a_n,b_n,c_n \in \mathbb{R}$ tali che $a_n \leq b_n \leq c_n, \forall n$ (anche se sarebbe sufficiente richiedere che ciò sia vero \textbf{definitivamente}, ossia $\exists n_0 \in \mathbb{N}$ tale che la disuguaglianza di cui sopra è valida $\forall n \geq n_0$). Siano convergenti le serie
    \[\sum a_n \hspace{1em} \text{e} \hspace{1em} \sum c_n\]
    allora è convergente anche la serie
    \[\sum b_n\]
    ed è tale la stima della somma della serie:
    \[\sum a_n \leq \sum b_n \leq c_n\]
    che è una stima valida $\forall n$, oppure $\forall n \geq n_0$ (a seconda che sia stato richiesto $\forall n$, oppure definitivamente).
\end{theorem}

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi il caso particolare per cui $a_n=0, \forall n$ (ossia il caso in cui la serie con termine generale $b_n$ è a termini positivi, cioè una serie per cui tutti i termini della successione dei termini generali sono positivi), allora è sufficiente che la serie con termine generale $c_n$ converga per concludere la convergenza.\\
Similmente, se $c_n=0, \forall n$ (ossia la serie con termine generale $b_n$ è a termini negativi, vale a dire una serie per cui tutti i termini della successione dei termini generali sono negativi), è sufficiente che la serie con termine generale $a_n$ converga per concludere la convergenza.\\
In questi casi, infatti, è sufficiente considerare un limitazione superiore (o inferiore, rispettivamente) per concludere la convergenza.

\vspace{1em}
\noindent
\textbf{Osservazione}: È facile capire che \textbf{il carattere di una serie non dipende da quello che accade su un numero finito di termini}, in quanto
\[\sum_{n=k}^{+\infty} a_n \hspace{1em} \text{ e } \hspace{1em} \sum_{n=0}^{+\infty} a_n\]
differiscono solamente per $k$ termini, ove $k$ è una \textbf{costante}.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la serie
\[\sum_{n=0}^{+\infty} \frac{1}{2^n} e^{100-n^2}\]
Si può facilmente capire che
\[e^{100-n^2} \leq 1 \hspace{1em} \text{ se } \hspace{1em} n \geq 10\]
per cui
\[\frac{1}{2^n} e^{100-n^2} \leq \frac{1}{2^n}  \hspace{1em} \text{ se } \hspace{1em} n \geq 10\]
Pertanto, essendo essa a termini positivi e maggiorata definitivamente, la serie di partenza converge per il teorema del confronto. Tuttavia, la stima seguente
\[\sum_{n=0}^{+\infty} \frac{1}{2^n} e^{100-n^2} \leq 2\]
ove $2$ è la somma della serie geometrica, è vera solamente definitivamente, per $n \geq 10$. Per avere una stima della somma più accurata, naturalmente, è possibile considerare quello che accade per i primi $9$ termini, per cui:
\[\sum_{n=0}^{+\infty} \frac{1}{2^n} e^{100-n^2} < a_0+a_1+\dots+a_9+2\]
dove $a_0+a_1+\dots+a_9$ sono i primi $9$ termini della serie stessa. Ma per migliorare la stima è possibile anche considerare i primi $9$ termini della serie geometrica, da cui
\[\sum_{n=0}^{+\infty} \frac{1}{2^n} e^{100-n^2} < a_0+a_1+\dots+a_9+\left(2-1-\frac{1}{2}-\dots-\frac{1}{2^9}\right)\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la serie seguente:
\[\sum_{n=1}^{+\infty} \cos \left(\frac{1}{n}\right)\]
Essa naturalmente diverge, in quanto il limite per $n \to +\infty$ del suo termine generale è
\[\lim_{n \to +\infty} \cos \left(\frac{1}{n}\right)=1\]
ossia, per $n$ molto grande, nella serie si somma sempre $1$, per cui diverge. Infatti, affinché una serie converga, il suo termine generale deve essere infinitesimo.

\vspace{1em}
\noindent
\begin{theorem} \textbf{Condizione necessaria per la convergenza}\\
    Sia
    \[\sum_{n=0}^{+\infty} a_n\]
    una serie \textbf{convergente} (in generale a termini complessi), allora
    \[\lim_{n \to \infty} a_n=0\]
    ossia la successione dei termini generali è \textbf{infinitesima}.
\end{theorem}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si consideri la ridotta di indice $n+1$, ossia
\[s_{n+1} = s_n + a_{n+1} \hspace{1em} \text{ tale per cui } \hspace{1em} a_{n+1} = s_{n+1} - s_n\]
Siccome la serie è convergente per ipotesi ($s_{n+1}$ e $s_n$ convergono allo stesso limite), per la linearità del limite, il limite della differenza è uguale alla differenza dei limiti, per cui:
\[\lim_{n \to +\infty} a_{n+1} = s_{n+1} - s_n = 0\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che la condizione per la convergenza esposta in precedenza è necessaria, ma non sufficiente. Infatti, esistono delle serie
\[\sum a_n\]
non convergenti, dove il
\[\lim_{n \to +\infty} a_n = 0\]
per questo si parla di condizione necessaria, e non sufficiente. Infatti è importante definire con quale velocità la successione dei termini generali vada a $0$: se è troppo lenta, nonostante sia infinitesima, la serie associata divergerà.

\vspace{1em}
\noindent
\subsection{Serie armonica}
Si consideri la serie seguente
\[\sum_{n=1}^{+\infty} \frac{1}{n}\]
che prende il nome di \textbf{serie armonica}. Per studiarne il comportamento, è sufficiente capire che \textbf{ogni serie può essere considerata come un integrale generalizzato}. Infatti, per definizione di integrale generalizzato di una funzione definita su una semiretta reale localmente integrabile:
\[\int_a^{+\infty} f(x) \dif x = \lim_{b \to +\infty} \int_a^b f(x) \dif x\]
allora se si considera la serie $a_1+a_2+a_3+\dots+a_n$, si definisce una funzione $f$ dipendente dalla serie stessa:
\[f : [1,+\infty[ \hspace{0.5em} \longmapsto \mathbb{R}\]
nel modo seguente: essendo una successione una funzione (definita sui numeri naturali), la funzione $f$ deve interpolare i valori della successione dei termini generali, assumendo il valore costante $a_n$ quando $x \in [n,n+1[$, come nel seguito:
\[f(x)=a_n \hspace{1em} \text{ se } \hspace{1em} x \in [n,n+1[, \hspace{1em} \forall n \geq 1\]
ottenendo una funzione che rappresenta la successione degli $a_n$ sotto forma di funzione.\\
Se $f$ è la successione degli $a_n$, la serie con termine generale $a_n$ non è altro che l'integrale generalizzato di tale funzione. Infatti, si ha che
\[\int_{n}^{n+1} f(x) \dif x = a_n \cdot (n+1-n) = a_n\]
per cui è ovvio che
\[s_n=a_1+a_2+\dots+a_n=\int_1^{n+1} f(x) \dif x\]
Se la funzione $f$ è integrabile (ossia esiste il limite dell'integrale di cui sopra), allora
\[\int_1^{+\infty} f(x) \dif x = \lim_{b \to +\infty} \int_1^b f(x) \dif x\]
e per quanto appena osservato,
\[s_n=a_1+a_2+\dots+a_n=\int_1^{n+1} f(x) \dif x \hspace{1em} \text{ allora } \hspace{1em} \lim_{n \to +\infty} s_n = \lim_{n \to +\infty} \int_1^{n+1} f(x) \dif x\]
per cui, per il teorema del limite delle successioni, ogni successione in cui $n$ tende a $+\infty$, avrà lo stesso limite della funzione $f$, ossia
\[\lim_{n \to +\infty} s_n = \lim_{n \to +\infty} \int_1^{n+1} f(x) \dif x = \int_1^{+\infty} f(x) \dif x\]
Pertanto, se la funzione $f$ così definita è integrabile e l'integrale ha un valore finito, allora la serie è convergente e la somma della serie è il valore di tale integrale.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che se la serie converge, per cui
\[\lim_{n \to +\infty} \int_1^{n+1} f(x) \dif x = s\]
è anche vero che $f$ è integrabile, ovvero
\[\int_1^{+\infty} f(x) \dif x = s\]
Ciò è vero in quanto la serie converge, e per la condizione necessaria vista in precedenza,
\[\lim_{n \to +\infty} a_n = 0\]
Pertanto, studiando l'integrale
\[\int_1^b f(x) \dif x\]
presa la parte intera di $b$, ossia $[b]=n$, essendo $b < n+1$ (in quanto la sua parte intera è $n$), si evince che 
\[\int_1^b f(x) \dif x = \int_1^n f(x) \dif x + \int_n^b f(x) \dif x\]
Dovendo studiare il limite per $b \to + \infty$ di tale integrale, è molto utile scomporlo in questo modo. Così facendo, siccome la serie converge, si ha che
\[\lim_{n \to +\infty} \int_1^n f(x) \dif x = s\]
mentre
\[\left \vert \int_n^b f(x) \dif x \right \vert = \left \vert a_n \cdot (b-n) \right \vert \leq \left \vert a_n \right \vert\]
in quanto $b<n+1$, per cui $b-n<1$, essendo $[b]=n$. Ma siccome la serie converge, allora il limite del termine generale è $0$, quindi
\[\lim_{n \to +\infty} \int_n^b f(x) \dif x \leq \lim_{n \to +\infty} a_n = 0\]
per cui, per la linearità del limite, si ha
\[\lim_{n \to +\infty} \int_1^b f(x) \dif x = \lim_{n \to +\infty} \int_1^n f(x) \dif x + \lim_{n \to +\infty} \int_n^b f(x) \dif x = s+0=s\]
come esposto da teorema seguente:

\vspace{2em}
\noindent
\begin{theorem}
    Sia $a_1+a_2+\dots+a_n$ una serie e sia $f$ la funzione associata definita come
    \[f(x)=a_n \hspace{1em} \text{ se } \hspace{1em} x \in [n,n+1[, \hspace{1em} \forall n \geq 1\]
    allora $f$ è integrabile in senso generalizzato sull'intervallo $[1,+\infty[$ \textbf{se e solo} se la serie converge. In questo caso si ha che la somma della serie è uguale al valore dell'integrale generalizzato, per cui
    \[\sum_{n=1}^{+\infty} a_n = \int_1^{+\infty} f(x) \dif x\]
\end{theorem}

\vspace{1em}
\noindent
\textbf{Osservazione}: Tale risultato è fondamentale per studiare il carattere della serie armonica. Infatti, se si considera la funzione
\[g(x)=\frac{1}{x}\]
essa non è integrabile in senso generalizzato sull'intervallo $[1,+\infty[$. Allora, presa $f(x)$ la funzione definita a tratti rispetto alla serie armonica, è facle capire che
\[g(x) \leq f(x), \hspace{1em} \forall x \in [1,+\infty[\]
Dal momento che $g(x)$ non è integrabile, non lo è nemmeno la $f$ (per il teorema del confronto degli integrali generalizzati).\\
Ma siccome, per il teorema precedentemente esposto, è noto che una serie converge se e solo se la funzione $f$ ad essa associata converge, si capisce immediatamente che la serie
\[\sum_{n=1}^{+\infty} \frac{1}{n}\]
non converge. Essendo una serie a termini positivi, per l'aut-aut si vedrà immediatamente che, non convergendo, dovrà necessariamente essere divergente a $+\infty$.

\vspace{1em}
\noindent
\subsubsection{Serie armonica generalizzata}
È noto che la serie armonica non converge. Non sorprende, però, sapere che tale serie è divergente a $+\infty$, ovvero
\[\sum_{n=1}^{+\infty} \frac{1}{n} = + \infty\]
come conseguenza diretta dell'aut-aut. Pertanto, se si considera
\[\sum_{n=1}^{+\infty} \frac{1}{\sqrt{n}}\]
è evidente capire che
\[\frac{1}{\sqrt{n}} \geq \frac{1}{n}, \hspace{1em} \forall n \geq 1\]
per cui, per il teorema del confronto, diverge a $+\infty$. Ciò risulta vero per ogni
\[\frac{1}{n^\alpha} \geq \frac{1}{n},\hspace{1em} \forall n \geq 1 \hspace{1em} \text{ se } 0 < \alpha \leq 1\]
Nel caso $\alpha > 1$, invece, è possibile studiare l'integrale generalizzato associato, da cui:
\[\int_1^{+\infty} \frac{1}{x^\alpha} \dif x = \left[\frac{1}{-\alpha+1} \cdot x^{-\alpha+1}\right]_1^{+\infty} = \frac{1}{\alpha-1}\]
Tuttavia, ciò non risulta essere sufficiente per dimostrare che la serie converge. Infatti, in questo caso, si è studiato l'integrale generalizzato di una funzione $g(x)$, ben diversa dalla funzione $f$ definita a tratti in precedenza.\\
Se ora si impiegasse la funzione $f$ definita in precedenza (da $n$ a $n+1$), siccome essa sarà inevitabilmente maggiore della funzione $g$ (di cui è nota l'integrabilità), ovvero $f(x) \geq g(x)$, non è possibile stabilire se essa sia integrabile o meno tramite il criterio del confronto per l'integrale generalizzato.\\
Per tale ragione si definisce
\[h(x)=a_n \hspace{1em} \text{ se } \hspace{1em} x \in ]n-1,n]\]
tale per cui $h(x) \leq g(x), \hspace{0.5em} \forall n \geq 1$. Allora è noto che
\[\int_{n-1}^n h(x) \dif x = a_n\]
Da ciò segue che
\[\int_1^{+\infty} h(x) \dif x=a_2+a_3+\dots=\sum_{n=2}^{+\infty} a_n\]
che parte da $n=2$, per come è stata definita $h(x)$. Pertanto, si ha che
\[\int_1^{+\infty} h(x) \dif x \leq \int_1^{+\infty} \frac{1}{x^\alpha} \dif x = \frac{1}{\alpha-1}\]
e quindi, per il teorema del confronto dell'integrale generalizzato, la funzione $h$ è integrabile. Inoltre, per il teorema precedentemente esposto, siccome la funzione $h$ associata alla serie è integrabile, la serie armonica generalizza converge; non solo, la somma della serie è
\[\sum_{n=1}^{+\infty} \frac{1}{n^\alpha} \leq \frac{1}{\alpha-1}+1\]

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{COMPORTAMENTO DELLA SERIE ARMONICA GENERALIZZATA}}\\
    \parbox{\linewidth}{La serie armonica generalizzata
    \[\sum_{n=1}^{+\infty} \frac{1}{n^\alpha}\]
    con $\alpha>$ è
    \begin{itemize}
        \item divergente a $+\infty$ se $\alpha \in ]0,1]$
        \item convergente se $\alpha>1$ con somma
        \[s \leq 1 + \frac{1}{\alpha - 1} = \frac{\alpha}{\alpha-1}\]
        dal momento che l'integrale
        \[\int_1^{+\infty} h(x) \dif x = \sum_{n=2}^{+\infty} \frac{1}{n^\alpha} \hspace{1em} \text{ in particolare } \hspace{1em} \int_1^{+\infty} \frac{1}{x^\alpha} = \frac{1}{\alpha-1} \geq \sum_{n=2}^{+\infty} \frac{1}{n^\alpha}\]
        e siccome parte da $n=2$, è necessario aggiungere $1$, da cui la disuguaglianza esposta.
    \end{itemize}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esercizio 1}: Si consideri la serie
\[\sum_{n=2}^{+\infty} \frac{1}{\log(n)}\]
che, ovviamente, diverge in quanto
\[\frac{1}{\log(n)} \geq \frac{1}{n}, \hspace{1em} \forall n \geq e\]
e siccome $\dfrac{1}{n}$ diverge, per il teorema del confronto, diverge anche $\dfrac{1}{\log(n)}$.

\vspace{2em}
\noindent
\textbf{Esercizio 2}: Si consideri la serie
\[\sum_{n=1}^{+\infty} \frac{1}{n \cdot (\log(n))^\alpha}\]
Per capire se essa diverga o meno, si considera l'integrale
\[\int_1^{+\infty} \frac{1}{n \cdot (\log(n))^\alpha} \dif x = \lim_{b \to +\infty} \int_1^b \frac{1}{n \cdot (\log(n))^\alpha} \dif x = \lim_{b \to +\infty} \left[\frac{1}{-\alpha+1} \log^{-\alpha+1}(x)\right]_1^b\]
in cui
\begin{itemize}
    \item se $\alpha > 1$, allora la funzione non è integrabile in senso generalizzato;
    \item se $\alpha = 1$, l'integrale è nullo e la funzione è integrabile in senso generalizzato.
\end{itemize}

\vspace{2em}
\noindent
\textbf{Esercizio 3}: Si consideri la serie
\[\sum_{n=2}^{+\infty} \frac{\arctan(n^2)}{n \cdot \sqrt{n}}\]
È ovvio che il numeratore è limitato, in quanto
\[\arctan(n^2) \leq \frac{\pi}{2}, \hspace{1em} \forall n\]
e quindi si evince che
\[\left \vert \frac{\arctan(n^2)}{n \cdot \sqrt{n}} \right \vert \leq \frac{\pi}{2} \frac{1}{n \cdot \sqrt{n}}\]
ove
\[\sum_{n=2}^{+\infty}  \frac{1}{n \cdot \sqrt{n}}\]
è una serie armonica generalizzata di ragione $\dfrac{3}{2} > 1$ che converge. Per il criterio del confronto per le serie, anche la serie di partenza converge.

\vspace{1em}
\noindent
\subsection{Serie a termini (reali) positivi}
Si consideri una serie a termini (reali) positivi, tale che $a_n \geq 0, \forall n$ (anche se sarebbe sufficiente \textbf{definitivamente}, ossia da un certo $n$ in poi).\\
Allora, per il \textbf{teorema dell'Aut-Aut}, tale serie o converge, o diverge, ma non può essere indeterminata.\\
Ciò spiega perché la serie armonica diverga a $+\infty$, in quanto si è dimostrato che non converge ed è una serie a termini (reali) positivi; naturalmente, il teorema dell'Aut-Aut si aggiunge al teorema del confronto.\\
Un altro importante criterio è l'ordine di infinitesimo che, tuttavia, non risulta efficace quando si considerano serie il cui termine generale presenta un ordine infrareale, ossia maggiore di $\alpha$, ma più piccolo di $\alpha+\epsilon, \hspace{0.5em} \forall \epsilon > 0$.

\newpage
\noindent
\begin{center}
    7 Ottobre 2022
\end{center}
\noindent
Dopo aver analizzato la condizione necessaria per la convergenza, è stato anche considerato il fatto che una serie può essere sempre considerata come un integrale generalizzato. Un esempio fondamentale di serie che può essere considerata come termine di confronto è anche la serie armonica.\\
Di seguito, inoltre, si espongono alcuni teoremi fondamentali per decretare la convergenza/divergenza di una serie, specialmente se le serie sono a \textbf{termini (reali) positivi}.

\vspace{1em}
\noindent
\subsection{Teorema dell'Aut-Aut per le serie a termini (reali) positivi}
Si supponga che la serie
\[a_1+a_2+\dots+a_n+\dots=\sum_{n=1}^{+\infty} a_n\]
abbia termini positivi ($a_n > 0)$ o al più non negativi ($a_n \geq 0$), questo, in generale, $\forall n$, ma è sufficiente richiedere che valga definitivamente. Allora essa converge o diverge; in altre parole, una serie a termini non negativi non può essere indeterminata.

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Supposto $a_n \geq 0, \forall n$ (anche se sarebbe sufficiente richiederlo definitivamente), la successione delle ridotte è \textbf{monotona crescente (anche in senso debole)}, ovverosia
\[s_{n+1}=s_n+a_{n+1} \geq s_n\]
Per il \textbf{teorema di esistenza del limite delle successioni monotone}, la successione delle ridotte ammette limite, ed esso è
\[\lim_{n \to +\infty} s_n = \text{ sup } \{s_n : n \in \mathbb{N}^+\}\]
Pertanto
\begin{itemize}
    \item se la successione delle ridotte $(s_n)_n$ è superiormente limitata, ovvero $\text{sup } \{s_n\} \in \mathbb{R}$, la serie è ovviamente convergente.
    \item se la successione delle ridotte $(s_n)_n$ è superiormente illimitata, per cui $\text{sup } \{s_n\} = +\infty$, la serie diverge a $+\infty$.
\end{itemize}
In ogni caso, però, \textbf{la serie non può essere indeterminata}.

\vspace{1em}
\noindent
\textbf{Osservazione 1}: Naturalmente la stessa cosa vale anche per successioni a termini negativi. L'importante è che sia verificata la condizione $a_n \geq 0$ oppure $a_n \leq 0$ definitivamente.

\vspace{1em}
\noindent
\textbf{Osservazione 2}: Il criterio dell'aut-aut è molto potente, in quanto, data una serie è a termini positivi, come la serie armonica
\[\sum_{n=1}^{+\infty} \frac{1}{n}\]
dimostrato che essa non converge, è immediato evincere che essa diverge per l'aut-aut.

\vspace{1em}
\noindent
\subsection{Criterio dell'ordine di infinitesimo per le serie a termini positivi}
Il teorema dell'Aut-Aut permette di dimostrare anche un altro importante criterio, il \textbf{criterio dell'ordine di infinitesimo per le serie a termini positivi}, che permette di desumere il carattere di una serie in modo molto veloce:

\begin{theorem}\textbf{Criterio dell'ordine di infinitesimo per le serie a termini positivi}\\
    Sia
    \[\sum_{n=0}^{+\infty} a_n\]
    una serie a termini positivi ($a_n \geq 0, \forall n$) con termine generale infinitesimo, ovverosia
    \[\lim_{n \to +\infty} a_n = 0\]
    allora
    \begin{itemize}
        \item se esiste $\alpha \in \mathbb{R}, \alpha > 1$ tale che \textbf{ord} $a_n \geq \alpha$, la serie converge;
        \item se \textbf{ord} $a_n \leq 1$, la serie diverge.
    \end{itemize}
\end{theorem}

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Supposto che il termine generale $a_n$ abbia come ordine di infinitesimo $\alpha$, con $\alpha > 1$, ossia
\[\lim_{n \to + \infty} \left \vert a_n \cdot n^\alpha \right \vert = l \hspace{1em} \text{ posto } \hspace{1em} l \in \mathbb{R} - \{0\}\]
allora, per definizione stessa di limite,
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{ tale che } \hspace{0.5em} \forall n > n_\epsilon \hspace{0.5em} \text{ si ha che } \hspace{0.5em} \left \vert a_n \cdot n^\alpha - l\right \vert < \epsilon\]
Scritta in modo esplicito la disuguaglianza, è facile capire
\[l-\epsilon < a_n \cdot n^\alpha < l+\epsilon\]
Per comodità, si sceglie $\epsilon = 1$, da cui, per la definizione di limite, si perviene a
\[a_n \cdot n^\alpha< l+1\]
Ciò consente di affermare che $\forall n > n_\epsilon$ si ha che
\[0 \leq a_n \leq (l+1) \cdot \frac{1}{n^\alpha}\]
In questo modo si sta confrontando il termine generale $a_n$ con il termine generale della serie armonica generalizzata. Per il criterio del confronto, siccome definitivamente (ossia $\forall n > n_\epsilon$)
\[a_n \leq (l+1) \cdot \frac{1}{n^\alpha}\]
e la serie armonica generalizzata converge, in quanto la ragione $\alpha > 1$. Allora, per il criterio del confronto, la serie con termine generale $a_n$
\[\sum_{n=1}^{+\infty} a_n\]
converge.

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Si supponga, ora che ord $a_n \leq 1$, si dimostri che la serie
\[\sum_{n=1}^{+\infty}a_n\]
diverge. Il fatto che ord $a_n \leq 1$, significa che
\[\lim_{n \to +\infty} \left \vert a_n \cdot n \right \vert = l\]
per cui se $l \in \mathbb{R} - \{0\}$ significa che ord $a_n = 1$, se $l = +\infty$, allora ord $a_n < 1$. Nell'ipotesi in cui $l \in \mathbb{R} - \{0\}$, ovvero
\[\lim_{n \to +\infty} n \cdot a_n = l \hspace{1em} \text{con} \hspace{1em} l \in \mathbb{R} - \{0\}\]
per la definizione stessa di limite, si può affermare che
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{ tale che } \hspace{0.5em} \forall n \geq n_\epsilon \hspace{0.5em} \text{ si ha che } \hspace{0.5em} \left \vert a_n \cdot n-l \right \vert < \epsilon\]
Scelto, per comodità, $\epsilon=\frac{l}{2}$, si ha in particolare che, definitivamente 
\[l-\frac{l}{2} < a_n \cdot n < l+\frac{l}{2}\]
Ma quindi si è ottenuto che definitivamente (ossia $\forall n > n_\epsilon$)
\[\frac{l}{2} \cdot \frac{1}{n} < a_n\]
e la serie armonica diverge. Allora, per il criterio del confronto, la serie con termine generale $a_n$
\[\sum_{n=1}^{+\infty} a_n\]
diverge.

\vspace{2em}
\noindent
\textbf{Osservazione}: In particolare, se $\exists \alpha \in \mathbb{R}, \alpha>1$, e si ha
\begin{itemize}
    \item $\text{ord } a_n \geq \alpha$, la serie converge;
    \item $\text{ord } a_n \leq 1$, la serie diverge.
\end{itemize}
Tuttavia, è fondamentale capire che dire che $\text{ord } a_n \geq \alpha$ è differente dal dire che $\text{ord } a_n > 1$. Infatti, quest'ultima informazione non è sufficiente ad affermare la convergenza, in quanto è possibile considerare anche ordini infrareali che non sono inclusi nelle casistiche di tale teorema. Infatti, per quanto riguarda la serie
\[\sum \frac{1}{n \log(n)}\]
presenta un termine generale di ord $a_n > 1$; tuttavia, è anche vero che $a_n < 1 + \epsilon, \forall \epsilon > 0$: pertanto, non esistendo un numero reale maggiore di $1$ e più piccolo di tutti i numeri reali più piccoli di $1$, non è possibile attribuire un ordine reale a tale termine generale. Tuttavia, se tale serie diverge (calcolabile tramite l'integrale generalizzato), la serie 
\[\sum \frac{1}{n \log^2(n)}\]
pur avendo ordine infrareale, esattamente come la serie precedente, converge (sempre tramite l'integrale).

\vspace{1em}
\noindent
\textbf{Esercizio 1}: La serie
\[\sum \frac{5n + \cos(n)}{3 + 2n^3}\]
è ovviamente convergente, in quanto $\text{ord } a_n = 2 > 1$. In realtà bisogna stare attenti al fatto che $\cos(n)$ non è sempre positivo, però è ovvio che
\[\left \vert \frac{5n + \cos(n)}{3 + 2n^3} \right \vert < \frac{5n + 1}{3 + 2n^3}\]
e siccome
\[\lim_{n \to +\infty} \frac{5n + 1}{3 + 2n^3} \cdot n^2 = \lim_{n \to +\infty} \dfrac{n^3 \cdot \left(5 + \dfrac{1}{n} \right)}{n^3 \cdot \left(2 + \dfrac{3}{n^3} \right)} = \frac{5}{2} \in \mathbb{R} - \{0\}\]
per confronto con una serie convergente, la serie di partenza converge.

\vspace{1em}
\noindent
\textbf{Esercizio 2}: La serie
\[\sum \frac{2\sqrt{n}}{n^2+n+1}\]
è a termini positivi e ovviamente convergente, in quanto $\text{ord } a_n = \dfrac{3}{2} > 1$.

\vspace{1em}
\noindent
\textbf{Esercizio 3}: La serie
\[\sum \log \left(1-\frac{1}{n}\right)\]
non converge. Attenzione, però, che la serie è a termini negativi, tuttavia si può fare
\[-\lim_{n \to +\infty} -\frac{\log \left(1-\frac{1}{n}\right)}{\frac{1}{n}}=1\]
per cui $\text{ord } a_n = 1$.

\vspace{1em}
\noindent
\textbf{Esercizio 4}: La serie
\[\sum 1 - \cos \left(\frac{1}{n}\right)\]
è ovviamente convergente, in quanto $\text{ord } a_n = 2 > 1$.

\vspace{1em}
\noindent
\textbf{Esercizio 5}: La serie
\[\sum \frac{2^n}{(\log(n))^n} = \sum \left(\frac{2}{\log(n)}\right)^n\]
è ovviamente convergente, in quanto
\[\frac{2}{\log(n)} < \frac{2}{3} \rightarrow \log(n) > 3\]
per $n > e^3$, ma l'importante è che accada definitivamente, per cui la serie converge per confronto con la serie geometrica.

\vspace{1em}
\noindent
\textbf{Esercizio 6}: La serie
\[\sum \frac{\sqrt{n} + 1}{n \cdot \left(n-10\pi\right)}\]
non è una serie a termini positivi, in generale, ma lo è quando $n - 10\pi>0 \rightarrow n > 10 \pi$, ossia definitivamente. Posta tale condizione, la serie è ovviamente convergente, in quanto $\text{ord } a_n = \dfrac{3}{2} > 1$.

\vspace{1em}
\noindent
\textbf{Esercizio 7}: La serie
\[\sum \frac{n^n}{(n!)^2}\]
è difficile da studiare, in quanto analizzare la differenza di velocità con cui le due successioni tendono all'infinito non è banale. Tuttavia, sfruttando il criterio del rapporto, si può osservare che
\[\lim_{n \to +\infty} \dfrac{\dfrac{(n+1)^{n+1}}{\left[(n+1)!\right]^2}}{\dfrac{n^n}{(n!)^2}} = \lim_{n \to +\infty} \dfrac{(n+1)^{n+1}}{\left[(n+1)!\right]^2} \cdot \dfrac{(n!)^2}{n^n} = \lim_{n \to +\infty} \dfrac{(n+1) \cdot (n+1)^{n}}{(n+1)^2 \cdot (n!)^2} \cdot \dfrac{(n!)^2}{n^n}\]
Semplificando ulteriormente si arriva alla forma finale
\[\lim_{n \to +\infty} \frac{1}{n+1} \cdot \left(1+\frac{1}{n}\right)^n = 0\]
ed essendo $0 < 1$ la serie converge. Convergendo la serie, è anche immediato evincere che il termine generale sia infinitesimo.

\newpage
\noindent
\subsection{Criterio del rapporto}
Presa una serie a termini positivi, ma non nulli (in quanto bisogna dividere per il termine $a_n$), per cui $a_n>0, \forall n$ (anche se sarebbe sufficiente definitivamente), come la seguente
\[\sum_{n=0}^{+\infty} a_n\]
tale per cui
\[\exists \lim_{n \to +\infty} \frac{a_{n+1}}{a_n} = k\]
Allora
\begin{itemize}
    \item se $k < 1$ la serie converge
    \item se $k > 1$ la serie diverge
    \item se $k = 1$ non è possibile dire nulla in merito al carattere della serie
\end{itemize}

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Si consideri 
\[\lim_{n \to +\infty} \frac{a_{n+1}}{a_n}=k\]
con $k<1$. Allora, per la definizione di limite
\[\exists \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n \geq n_\epsilon, \hspace{0.5em} \text{si ha che} \hspace{0.5em} k - \epsilon < \frac{a_{n+1}}{a_n} < k+\epsilon\]
Allora, preso un $\epsilon$ sufficientemente piccolo tale che $k+\epsilon<1$ (in altre parole, si sceglie $0 < \epsilon < 1-k$, essendo $k<1$ per ipotesi), si ottiene
\[\frac{a_{n+1}}{a_n} < k+\epsilon < 1 \hspace{1em} \rightarrow \hspace{1em} a_{n+1} < (k+\epsilon) \cdot a_n\]
E avendo supposto $a_n>0$, si ottiene la catena di disuguaglianze seguente
\[0 < a_n < a_{n-1} \cdot (k+\epsilon) <  a_{n-2} \cdot (k+\epsilon)^2 < \dots\]
Senza perdita di generalità (in quanto si richiederebbe $\forall n \geq n_\epsilon$), è possibile supporre che
\[a_{n+1} < (k+\epsilon) \cdot a_n\]
vale $\forall n$. Per induzione, si ottiene che
\begin{align*}
    &a_1 < (k+\epsilon) \cdot a_0\\
    &a_2 < (k+\epsilon) \cdot a_1 < (k+\epsilon)^2 \cdot a_0\\
    &\dots\\
    &a_n < (k+\epsilon)^n \cdot a_0
\end{align*}
per cui
\[a_n < (k+\epsilon)^n \cdot a_0\]
e, quindi, essendo $a_0$ costante e $(k+\epsilon)^n$ il termine generale della serie geometrica di ragione $k+\epsilon$, con $\left \vert k+\epsilon \right \vert < 1$ per costruzione, si può concludere, per il teorema del confronto, che la serie
\[\sum_{n=1}^{+\infty} a_n\]
converge.

\newpage
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Si consideri 
\[\lim_{n \to +\infty} \frac{a_{n+1}}{a_n}=k\]
con $k>1$. Allora, per la definizione di limite
\[\exists \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n \geq n_\epsilon, \hspace{0.5em} \text{si ha che} \hspace{0.5em} k - \epsilon < \frac{a_{n+1}}{a_n} < k+\epsilon\]
Allora, preso un $\epsilon$ sufficientemente piccolo tale che $k-\epsilon>1$ (in altre parole, si sceglie $0 < \epsilon < k-1$, essendo $k>1$ per ipotesi), si ottiene
\[1 < k-\epsilon < \frac{a_{n+1}}{a_n} \hspace{1em} \rightarrow \hspace{1em} a_{n+1} > a_n\]
Ciò significa che la successione $(a_n)_n$ è definitivamente strettamente crescente. Pertanto, la successione $a_n$ \textbf{non può essere infinitesima}, essendo positiva crescente, pertanto la serie
\[\sum_{n=1}^{+\infty} a_n\]
diverge, per l'aut-aut.

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la serie
\[\sum_{n=1}^{+\infty} \frac{n^n}{(n!)^2}\]
Allora, applicando il teorema del rapporto, si ottiene 
\[\lim_{n \to +\infty} \dfrac{\dfrac{(n+1)^{n+1}}{\left[(n+1)!\right]^2}}{\dfrac{n^n}{(n!)^2}} = \lim_{n \to +\infty} \dfrac{(n+1)^{n+1}}{\left[(n+1)!\right]^2} \cdot \dfrac{(n!)^2}{n^n} = \lim_{n \to +\infty} \dfrac{(n+1) \cdot (n+1)^{n}}{(n+1)^2 \cdot (n!)^2} \cdot \dfrac{(n!)^2}{n^n}\]
Semplificando ulteriormente si arriva alla forma finale
\[\lim_{n \to +\infty} \frac{1}{n+1} \cdot \left(1+\frac{1}{n}\right)^n = 0\]
ed essendo $0 < 1$ la serie converge. Convergendo la serie, è anche immediato evincere che il termine generale sia infinitesimo.

\vspace{2em}
\noindent
\subsection{Criterio della radice $n$-esima}
Sia data una serie a termini positivi, con $a_n \geq 0, \forall n$ (anche se sarebbe sufficiente richiederlo definitivamente), come la seguente
\[\sum_{n=1}^{+\infty} a_n\]
Supposto che esista
\[\lim_{n \to +\infty} \left(\sqrt[n]{a_n}\right)=l\]
Allora si considerano le seguenti casistiche
\begin{itemize}
    \item se $l>1$ la serie diverge
    \item se $l<1$ la serie converge
    \item se $l=1$ non si può dire nulla sul carattere della serie
\end{itemize}

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Si consideri il caso in cui 
\[\lim_{n \to +\infty} \left(\sqrt[n]{a_n}\right)=l\]
com $l>1$, per la definizione di limite
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n > n_\epsilon, \hspace{0.5em} \text{si ha che} \hspace{0.5em} \left \vert \sqrt[n]{a_n} - l \right \vert < \epsilon\]
Per cui, per $n \geq n_\epsilon$ si ha che
\[l-\epsilon<\sqrt[n]{a_n}<l+\epsilon\]
Pertanto, essendo $l>1$ per ipotesi, si sceglie $\epsilon$ sufficientemente piccolo tale che $\epsilon<l-1$, per cui $l-\epsilon>1$. Ciò significa che 
\[\sqrt[n]{a_n}>l-\epsilon>1\]
Definitivamente, quindi, $a_n>1$ (cioè non è infinitesimo), pertanto la serie non può convergere. Essendo una serie a termini positivi, per l'aut-aut, diverge.

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Si consideri il caso in cui 
\[\lim_{n \to +\infty} \left(\sqrt[n]{a_n}\right)=l\]
con $l<1$. Allora, per la definizione di limite
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n > n_\epsilon, \hspace{0.5em} \text{si ha che} \hspace{0.5em} \left \vert \sqrt[n]{a_n} - l \right \vert < \epsilon\]
Per cui, per $n \geq n_\epsilon$ si ha che
\[l-\epsilon<\sqrt[n]{a_n}<l+\epsilon\]
Pertanto, essendo $l<1$ per ipotesi, si considera $\epsilon$ sufficientemente piccolo tale che $0 < \epsilon < 1-l$, ossia $l+\epsilon<1$. Ciò permette di concludere che
\[\sqrt[n]{a_n} < l+\epsilon < 1 \hspace{1em} \rightarrow \hspace{1em} a_n < (l+\epsilon)^n\]
e siccome si è preso $\vert l+\epsilon \vert < 1$, per confronto con la serie geometrica, la serie di partenza converge.

\vspace{2em}
\noindent
\textbf{Osservazione 1}: Si consideri la serie armonica generalizzata con $\alpha=2$:
\[\sum_{n=1}^{+\infty} \frac{1}{n^2}\]
Per studiarne il carattere, si applica il criterio del rapporto e si ottiene:
\[\lim_{n \to +\infty} \frac{a_{n+1}}{a_n} = \lim_{n \to +\infty} \dfrac{\dfrac{1}{(n+1)^2}}{\dfrac{1}{n^2}} = \lim_{n \to +\infty} \frac{n^2}{(n+1)^2} = 1\]
per cui per tale criterio non è possibile dire nulla, ma è noto che la serie converge.\\
Analogamente si ha che il carattere della serie
\[\sum_{n=1}^{+\infty} \frac{1}{n}\]
non può essere determinato con il criterio del rapporto, in quanto
\[\lim_{n \to +\infty} \frac{a_{n+1}}{a_n} = \lim_{n \to +\infty} \dfrac{\dfrac{1}{n+1}}{\dfrac{1}{n}} = \lim_{n \to +\infty} \frac{n}{n+1} = 1\]
ma è noto che tale serie diverge. Per cui il criterio del rapporto, quando si ottiene $k=1$ non fornisce alcuna informazione in merito al carattere della serie studiata.

\vspace{2em}
\noindent
\textbf{Osservazione 2}: Si consideri la serie armonica:
\[\sum_{n=1}^{+\infty} \frac{1}{n}\]
Allora, applicando il criterio della radice $n$-esima, si calcola il limite seguente
\[\lim_{n \to +\infty} \sqrt[n]{\frac{1}{n}} = \lim_{n \to +\infty} \left(\frac{1}{n}\right)^{\frac{1}{n}} = \lim_{n \to +\infty} e^{\frac{1}{n} \cdot \log \left(\frac{1}{n}\right)} = \lim_{n \to +\infty} e^{-\frac{\log \left(n\right)}{n}} = 1\]
Similmente accadrebbe con la serie armonica generalizzata di ragione $\alpha=2$.

\vspace{1em}
\noindent
\subsection{Serie a termini qualsiasi}
Se si considera una serie a termine generale qualsiasi
\[\sum a_n, \hspace{1em} \text{ con } \hspace{1em} a_n \in \mathbb{C}\]
non è possibile dire molto sul suo carattere. Tuttavia, ad essa è possibile associare la serie
\[\sum \vert a_n \vert\]
che è una serie a termine generale positivo, a cui è possibile applicare i criteri noti.

\vspace{1em}
\subsubsection{Serie assolutamente convergente}
Di seguito viene esposta la definizione di \textbf{serie assolutamente convergente}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SERIE ASSOLUTAMENTE CONVERGENTE}}\\
    \parbox{\linewidth}{Una serie
    \[\sum a_n\]
    si dice \textbf{assolutamente convergente}, se è convergente la serie dei suoi moduli seguente
    \[\sum \vert a_n \vert\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\begin{theorem}
    Una serie assolutamente convergente è convergente. Tuttavia, non è vero il viceversa.
\end{theorem}

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Si consideri il caso in cui $a_n \in \mathbb{R}$, allora, per definizione di parte positiva e parte negativa si ha, rispettivamente:
\begin{align*}
    &a_n^+ = \left\{
\rowcolors{1}{white}{white}    
\begin{array}{lll}
    a_n & \text{se} & a_n \geq 0\\
    0   & \text{se} & a_n < 0\\
\end{array}
\right.\\
&a_n^- = \left\{
\rowcolors{1}{white}{white}     
\begin{array}{lll}
    -a_n & \text{se} & a_n < 0\\
    0    & \text{se} & a_n \geq 0\\
\end{array}
\right.
\end{align*}
ma ciò significa che la parte positiva e negativa di un reale è sempre $\geq 0$. Pertanto si può affermare che
\[a_n=a_n^+-a_n^- \hspace{1em} \text{e} \hspace{1em} \vert a_n \vert = a_n^++a_n^-\]
Quindi, in particolare, si ha che
\[0 \leq a_n^+ \leq \vert a_n \vert\]
\[0 \leq a_n^- \leq \vert a_n \vert\]
Per il criterio del confronto, quindi, se la serie dei valori assoluti 
\[\sum \left \vert a_n \right \vert\]
è convergente, anche le serie di parte positiva e parte negativa
\[\sum a_n^+ \hspace{1em} \text{e} \hspace{1em} \sum a_n^-\]
convergono. Pertanto la serie di partenza
\[\sum a_n^+-a_n^- = \sum a_n\]
è convergente.

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Si consideri il caso in cui $z_n \in \mathbb{C}$ e si supponga che 
\[\sum \left \vert z_n \right \vert\]
sia convergente. Allora, posto
\[z_n = x_n + i \cdot y_n\]
e, per definizione di modulo di un numero complesso, si ha
\[\left \vert z_n \right \vert = \sqrt{x_n^2+y_n^2}\]
è immediato evincere che
\begin{itemize}
    \item $\vert x_n \vert \leq \vert z_n \vert$
    \item $\vert y_n \vert \leq \vert z_n \vert$
\end{itemize}
in cui $\vert x_n \vert$ e $\vert y_n \vert$ sono valori assoluti, mentre $\vert z_n \vert$ è un modulo. Per le disuguaglianze di cui sopra, si evince che le serie
\[\sum \vert x_n \vert \hspace{1em} \text{e} \hspace{1em} \sum \vert y_n \vert\]
convergono. Ma ciò significa che la serie con termine generale $x_n$ converge assolutamente, così come quella con termine generale $y_n$.\\
Siccome $x_n$ rappresenta la parte reale di $z_n$ e $y_n$ rappresenta la parte immaginaria di $z_n$, per il teorema successivamente esposto sulla convergenza di una serie di numeri complessi, si conclude che la serie
\[\sum z_n\]
converge.

\vspace{2em}
\noindent
\textbf{Osservazione}: Tuttavia, non è vero il viceversa: una serie convergente non è detto sia assolutamente convergente.\\
Si consideri, a tal proposito, la \textbf{serie di Leibniz} seguente:
\[\sum_{n=1}^{+\infty} (-1)^n \cdot \frac{1}{n}\]
che, per il criterio di Leibniz, risulta convergente. Tuttavia, la corrispondente serie dei moduli
\[\sum_{n=1}^{+\infty} \left \vert (-1)^n \cdot \frac{1}{n} \right \vert = \sum_{n=1}^{+\infty} \frac{1}{n}\]
non è convergente, in quanto è la serie armonica. Questo è il tipico esempio di una serie convergente, ma non assolutamente convergente.

\vspace{1em}
\subsubsection{Serie semplicemente convergente}
Di seguito viene esposta la definizione di \textbf{serie semplicemente convergente}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SERIE SEMPLICEMENTE CONVERGENTE}}\\
    \parbox{\linewidth}{Una serie convergente, ma non assolutamente convergente, si dice \textbf{semplicemente convergente}.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\subsection{Limiti di successioni in $\mathbb{C}$}
Sia $(z_n)_n$ una successione in $\mathbb{C}$, con $\gamma \in \mathbb{C}$. Allora si dirà che
\[\lim_{n \to +\infty} z_n = \gamma\]
se
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n \geq n_\epsilon \hspace{0.5em} \text{si ha che} \hspace{0.5em} \vert z_n - \gamma \vert < \epsilon\]
in cui è da intendersi $\vert \dots \vert$ come modulo di un numero complesso che è da intendersi come \quotes{distanza dall'origine}; preso un numero complesso $z_0$, si ha che
\[\mathcal{B}(z_0,r) = \{z \in \mathbb{C} : \left \vert z  - z_0 \right \vert < r\}\]

\vspace{1em}
\noindent
\subsubsection{Convergenza di Re e Im}
Com'è noto, un numero complesso può essere descritto in forma cartesiana come $z=x+i \cdot y$, con $z,y \in \mathbb{R}$: esiste una relazione tra la successione di un numero complesso e la successione della sua parte reale e immaginaria, esposta dal seguente teorema:

\vspace{1em}
\begin{theorem}
    La successione $(z_n)_n$, posto $z_n=x_n+i \cdot y_n$, converge a $\gamma = \alpha + i \cdot \beta$ \textbf{se e solo se}
    \[\lim_{n \to +\infty} x_n = \alpha \hspace{1em} \text{ e } \hspace{1em} \lim_{n \to + \infty} y_n = \beta\]
\end{theorem}

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Dato $z_n=x_n+i \cdot y_n$ e $\gamma = \alpha + i \cdot \beta$, è evidente come
\[z_n-\gamma = x_n+i \cdot y_n - (\alpha + i \cdot \beta) = x_n-\alpha + i \cdot (y_n-\beta)\]
Dalla definizione di modulo, si ha, quindi, che
\[\vert z_n - \gamma \vert = \sqrt{(x_n-\alpha)^2+(y_n-\beta)^2}\]
Da ciò appare evidente che
\[\vert x_n - \alpha \vert \leq \vert z_n - \gamma \vert\]
\[\vert y_n - \beta  \vert \leq \vert z_n - \gamma \vert\]
Pertanto, siccome per ipotesi si ha che 
\[\lim_{n \to +\infty} z_n = \gamma\]
ovvero
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n \geq n_\epsilon \hspace{0.5em} \text{si ha che} \hspace{0.5em} \vert z_n - \gamma \vert < \epsilon\]
è immediato capire che definitivamente, per $n \geq n_\epsilon$, anche
\[\vert x_n - \alpha \vert \leq \vert z_n - \gamma \vert < \epsilon\]
\[\vert y_n - \beta  \vert \leq \vert z_n - \gamma \vert < \epsilon\]

\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Sia, per ipotesi, che
\[\lim_{n \to +\infty} x_n = \alpha \hspace{1em} \text{ e } \hspace{1em} \lim_{n \to + \infty} y_n = \beta\]
per cui
\[\forall \epsilon_1 > 0, \exists n_{\epsilon_1} \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n \geq n_{\epsilon_1} \hspace{0.5em} \text{si ha che} \hspace{0.5em} \vert x_n - \alpha \vert < \epsilon_1\]
e, analogamente
\[\forall \epsilon_2 > 0, \exists n_{\epsilon_2} \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n \geq n_{\epsilon_2} \hspace{0.5em} \text{si ha che} \hspace{0.5em} \vert y_n - \beta \vert < \epsilon_2\]
Pertanto, posto $n_\epsilon = \max \{n_{\epsilon_1},n_{\epsilon_2}\}$ e $\epsilon = \min \{\epsilon_1,\epsilon_2\}$ si ha che, definitivamente, per $n \geq n_\epsilon$
\[\vert x_n - \alpha \vert < \epsilon\]
\[\vert y_n - \beta  \vert < \epsilon\]
Ma dal momento che
\[\vert z_n - \gamma \vert = \sqrt{(x_n-\alpha)^2+(y_n-\beta)^2} < \sqrt{(\epsilon)^2+(\epsilon)^2} = \sqrt{2} \epsilon^2\]
che, chiaramente, può essere resa piccola quanto si vuole.

\vspace{1em}
\noindent
\textbf{Osservazione}: Dal momento che le serie sono particolari successioni, i risultati visti per le successioni si applicano in modo identico anche alle serie: una serie a termini complessi
\[\sum_{n=0}^{+\infty} z_n\]
converge \textbf{se e solo se} convergono le serie
\[\sum_{n=0}^{+\infty} \text{Re}(z_n) \hspace{1em} \text{e} \hspace{1em} \sum_{n=0}^{+\infty} \text{Im}(z_n)\]
e si ha che la somma della serie $z_n$ è data dalla somma delle somme della parte reale e immaginaria, come esposto di seguito:
\[\sum_{n=0}^{+\infty} z_n = \sum_{n=0}^{+\infty} \text{Re}(z_n) + i \cdot \sum_{n=0}^{+\infty} \text{Im}(z_n)\]

\vspace{1em}
\noindent
\textbf{Esercizio 1}: Si consideri la seguente serie
\[\sum \left(2 i^n-\frac{3 i}{5^n}\right)\]
Allora, ovviamente, il termine generale non è infinitesimo, per cui la serie non converge.

\vspace{1em}
\noindent
\textbf{Esercizio 2}: Si consideri la seguente serie
\[\sum \left(\frac{\sin(n)}{n^2 \cdot i} + \frac{n}{\sin(n)+3n^3}\right)\]
Dal momento che si ha che
\[\frac{1}{i}=-i\]
è facile capire che la serie di partenza diviene
\[\sum \left(-\frac{\sin(n)}{n^2}\cdot i + \frac{n}{\sin(n)+3n^3}\right)\]
Allora il termine generale è infinitesimo e, in particolare, si osserva che
\[\left \vert \frac{\sin(n)}{n^2} \right \vert \leq \frac{1}{n^2}\]
e siccome la serie armonica generalizzata di ragione $2$ converge, per il criterio del confronto, converge assolutamente anche la parte immaginaria della serie di partenza e, quindi, anche la parte immaginaria stessa.\\
Similmente è possibile affermare che
\[\frac{n}{\sin(n)+3n^3}\]
è infinitesima di ord $2 \geq \alpha > 1$, per cui è convergente per il criterio dell'ordine di infinitesimo.

\vspace{1em}
\noindent
\textbf{Esercizio 3}: Si consideri la seguente serie
\[\sum \frac{3n+i}{n^3+n\cdot i}\]
Un modo immediato per semplificare l'espressione del termine generale è moltiplicare e dividere per il coniugato del denominatore, da cui
\[\frac{3n+i}{n^3+n\cdot i} \cdot \frac{n^3 - n \cdot i}{n^3 - n \cdot i} = \frac{3n^4 + n}{n^6+n^2} + \frac{n^3 - 3n^2}{n^6+n^2} \cdot i\]
In questo modo è facile capire che tanto la parte immaginaria, quanto la parte reale del termine generale di partenza sono infinitesime, la prima di ord $2$, la seconda di ord $3$. Per il criterio dell'ordine di infinitesimo, convergono.\\
Un altro modo per studiare tale successione è quello di considerare il modulo del termine generale. Pertanto:
\[\left \vert \frac{3n+i}{n^3+n\cdot i} \right \vert = \frac{ \vert 3n+i \vert}{\vert n^3+n\cdot i \vert} = \frac{\sqrt{9n^2 + 1}}{\sqrt{n^6 +n^2}} = \dfrac{\left \vert n \right \vert \cdot \sqrt{9 + \dfrac{1}{n^2}}}{\vert n^3 \vert \cdot \sqrt{1 + \dfrac{1}{n^4}}}\]
Ancora una volta, il termine generale è infinitesimo di ord $2$, per cui la serie converge assolutamente e quindi è convergente.

\newpage
\begin{center}
    10 Ottobre 2022
\end{center}
Le serie numeriche sono delle coppie di successioni: una è la successione dei termini generali, l'altra è la successione delle somme parziali.\\
Se una successione è convergente, allora il suo termine generale è infinitesimo. Una serie può essere sempre pensata come un integrale generalizzato.\\
Le serie a termini (reali) positivi sono le serie più facili da studiare, in forza del teorema dell'aut-aut che afferma che una serie di questo tipo o converge o diverge.\\
Il criterio di convergenza più importante è il criterio dell'ordine di infinitesimo, a cui si aggiunge il criterio del rapporto e il criterio della radice $n$-esima.\\
Tuttavia, se una serie non è a termini (reali) positivi, si può associare ad essa la serie dei suoi moduli, che è a termini positivi, quindi più facile da studiare: una serie si dice assolutamente convergente se la serie dei suoi moduli è convergente; in particolare, una serie assolutamente convergente è anche convergente, ma non è vero il viceversa.

\vspace{1em}
\subsection{Serie semplicemente convergenti}
Serie non assolutamente convergenti vengono chiamate serie \textbf{semplicemente convergenti} e sono le più difficili da studiare. Di seguito vengono esposti alcuni criteri che possono essere impiegati per studiare la convergenza semplice di tali serie.

\vspace{1em}
\noindent
\subsubsection{Criterio di Leibniz per le serie a termini alterni}
Di seguito si espone il \textbf{criterio di Leibniz per le serie a termini alterni}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{CRITERIO DI LEIBNIZ PER LE SERIE A TERMINI ALTERNI}}\\
    \parbox{\linewidth}{Si consideri $(a_n)_n$ una successione a termini reali, con $a_n \in \mathbb{R}$, tale che
    \begin{itemize}
        \item $a_n > 0, \hspace{0.5em} \forall n \in \mathbb{N}$
        \item $a_{n+1} \leq a_n, \hspace{0.5em} \forall n \in \mathbb{N}$
        \item il termine $a_n$ deve essere infinitesimo:
        \[\lim_{n \to +\infty} a_n = 0\]
    \end{itemize}
    Allora la serie costruita come
    \[\sum_{n=0}^{+\infty} (-1)^n \cdot a_n\]
    converge. Inoltre, detta $s$ la somma della serie, si ha che
    \[\forall n \hspace{1em} \left \vert s_n-s \right \vert \leq a_{n+1}\]
    secondo la cosiddetta \textbf{formula di approssimazione}.\vspace{3mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Si consideri la ridotta $n$-esima $s_n$. Posto $k \in \mathbb{N}$ tale per cui $k \geq 0$, allora studiando la sottosuccessione dei termini pari e quella dei termini dispari, si ha
\begin{enumerate}
    \item Per i termini pari
    \[s_{2k+2} = s_{2k} - a_{2k+1} + a_{2k+2} = s_{2k} - \underbrace{(a_{2k+1}-a_{2k+2})}_{\geq 0} \leq s_{2k}\]
    Infatti, per ipotesi, $a_{n+1} \leq a_n$, e quindi si ha che $a_{2k+1}-a_{2k+2} \geq 0$.\\
    Per tale ragione, tale sottosuccessione è \textbf{monotona decrescente}.

    \item Per i termini dispari
    \[s_{2k+3} = s_{2k+1} + a_{2k+2} - a_{2k+3} = s_{2k+1} + \underbrace{(a_{2k+2}-a_{2k+3})}_{\geq 0} \geq s_{2k+1}\]
    Infatti, per ipotesi, $a_{n+1} \leq a_n$, e quindi si ha che $a_{2k+2}-a_{2k+3} \geq 0$.\\
    Per tale ragione, tale sottosuccessione è \textbf{monotona crescente}.
\end{enumerate}
È noto, per ipotesi, che
\[s_{2k+1} - s_{2k} = (-1)^{2k+1} \cdot a_{2k+1} = -a_{2k+1} \leq 0 \hspace{1em} \text{ e quindi } \hspace{1em} s_{2k+1} \leq s_{2k}, \hspace{1em} \forall k \geq 0\]
ciò significa che, per ogni $n$, la ridotta pari è maggiore della ridotta dispari (ma la prima è decrescente, la seconda è crescente), rimbalzando progressivamente attorno al limite delle due sottosuccessioni:

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \draw[->,ultra thick] (-5,0)--(5,0) node[right]{};
        \draw[->,ultra thick,red] (-4,-1.5)--(-1,-1.5) node[below]{dispari};
        \draw[->,ultra thick,blue] (4,-1.5)--(1,-1.5) node[below]{pari};
        
        \draw[thick] (-4,0.25)--(-4,-0.25) node[right]{};
        \draw[] (-4,-0.5) node[circle, below]{$0$};

        \draw[thick,red] (-3,0.25)--(-3,-0.25) node[right]{};
        \draw[] (-3,-0.5) node[circle, below]{\textcolor{red}{$s_1$}};

        \draw[thick,red] (-2,0.25)--(-2,-0.25) node[right]{};
        \draw[] (-2,-0.5) node[circle, below]{\textcolor{red}{$s_3$}};

        \draw[thick,red] (-1,0.25)--(-1,-0.25) node[right]{};
        \draw[] (-1,-0.5) node[circle, below]{\textcolor{red}{$s_5$}};

        \draw[thick,blue] (2,0.25)--(2,-0.25) node[right]{};
        \draw[] (2,-0.5) node[circle, below]{\textcolor{blue}{$s_4$}};

        \draw[thick,blue] (3,0.25)--(3,-0.25) node[right]{};
        \draw[] (3,-0.5) node[circle, below]{\textcolor{blue}{$s_2$}};

        \draw[thick,blue] (4,0.25)--(4,-0.25) node[right]{};
        \draw[] (4,-0.5) node[circle, below]{\textcolor{blue}{$s_0$}};

        \draw[thick,orange] (0,0.25)--(0,-0.25) node[right]{};
        \draw[] (0,-0.5) node[circle, below]{\textcolor{orange}{$s$}};
        \end{tikzpicture}
\end{figure}

\vspace{1em}
\noindent
Dalle disuguaglianze di cui sopra si ha che
\begin{align*}
    s_{2k} \geq s_{2k+1} \geq s_1 = a_0-a_1 & \hspace{1em}\forall k > 0\\
    s_{2k+1} \leq s_{2k} \leq s_2 = a_0-a_1+a_2 & \hspace{1em} \forall k > 0
\end{align*}
ovverosia la \textbf{sottosuccessione degli indici pari}, \textbf{decrescente}, è \textbf{limitata dal basso}; similmente, la \textbf{sottosuccessione degli indici dispari}, \textbf{crescente}, è \textbf{limitata dall'alto}. Ciò permette di affermare che \textbf{esiste} per entrambe un \textbf{limite finito}:
\[\lim_{k \to +\infty} s_{2k} = \beta \hspace{1em} \text{e} \hspace{1em} \lim_{k \to + \infty} s_{sk+1} = \alpha\]
e, per il \textbf{teorema del confronto dei limiti}, si ha che $\alpha \leq \beta$.\\
Essendo il termine $a_n$ infinitesimo, si ha che
\[0 = \lim_{n \to +\infty} a_n = \lim_{k \to +\infty} a_{2k+1} = \lim_{k \to +\infty} s_{2k} - s_{sk+1} = \alpha - \beta = 0\]
Dal momento che le \textbf{sottosuccessioni sono complementari}, la serie di partenza converge.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: La formula di approssimazione del criterio di Leibniz
\[\forall n \hspace{1em} \left \vert s_n-s \right \vert \leq a_{n+1}\]
funziona in quanto
\begin{itemize}
    \item se $n$ è dispari ($n=2k+1$)
    \[\left \vert s_{2k+1} - s \right \vert = s-s_{2k+1}\]
    in quanto la successione dei dispari è crescente e sempre minore di quella dei pari, per cui $s \geq s_{2k+1}$. Allora si ha che
    \[s-s_{2k+1}\leq s_{2k+2} - s_{2k+1}\]
    in quanto $s \leq s_{2k+2}$. Ma siccome $s_{2k+2}=s_{2k+1}+a_{2k+2}$ è facile capire che
    \[s_{2k+2} - s_{2k+1} = a_{2k+2} = a_{(2k+1)+1}= a_{n+1}\]
    
    \item se $n$ è pari ($n=2k$)
    \[\left \vert s_{2k} - s \right \vert = s_{2k+1}-s\]
    in quanto la successione dei pari è decrescente e sempre maggiore di quella dei dispari, per cui $s_{2k} \geq s$. Allora si ha che
    \[s_{2k}-s\leq s_{2k} - s_{2k+1}\]
    in quanto $s \geq s_{2k+1}$. Ma siccome $s_{2k+1}=s_{2k}-a_{2k+1}$ è facile capire che
    \[s_{2k} - s_{2k+1} = a_{2k+1} = a_{n+1}\]
\end{itemize}

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la serie di Leibniz:
\[\sum_{n=1}^{+\infty} (-1)^n \frac{1}{n} = s\]
Allora tale serie converge per il criterio di Leibniz, essendo $a_n$ positivo, decrescente e infinitesimo. Dalla formula di approssimazione si ha che
\[\left \vert s_n - s \right \vert < \frac{1}{n+1}\]
Allora, per conoscere la somma della serie con un errore di $\dfrac{1}{10}$, è sufficiente considerare
\[s_9 = -1 + \frac{1}{2} - \frac{1}{3} + \dots - \frac{1}{9}\]

\vspace{1em}
\noindent
\textbf{Esercizio 1}: Si consideri la serie seguente
\[\sum_{n=1}^{+\infty} (-1)^n \cdot \frac{\log_{10}(n)}{n}\]
Si controlli se sono verificate le condizioni di Leibniz seguenti:
\begin{itemize}
    \item Il termine $a_n$ non è strettamente maggiore di zero per ogni $n$. Tuttavia è vero che
    \[\frac{\log_{10}(n)}{n} > 0 \hspace{1em} \forall n \geq 2\]
    che è sufficiente, in quanto basta che sia verificata la condizione definitivamente.

    \item Si ha che
    \[\lim_{n \to +\infty} a_n = \lim_{n \to +\infty} \frac{\log_{10}(n)}{n} = 0\]
    per cui il termine $a_n$ è infinitesimo.

    \item La successione
    \[\frac{\log_{10}(n)}{n}\]
    è decrescente $\forall n$? Basterebbe verificare che lo sia definitivamente.
\end{itemize}
Per verificare l'ultimo punto, si considera la funzione
\[f(x) = \frac{\log_{10}(x)}{x}\]
e se ne calcola la derivata (cosa che non è possibile fare con una successione), da cui
\[f'(x) = \dfrac{\dfrac{1}{x \cdot \log(10)} \cdot x - \log_{10}(x) \cdot 1}{x^2}\]
Se ne studia il segno, che dipende solamente dal numeratore, da cui
\[\frac{1}{x \cdot \log(10)} \cdot x - \log_{10}(x) \cdot 1 > 0 \hspace{1em} \rightarrow \hspace{1em} \log_{10}(x) < \frac{1}{\log(10)} \hspace{1em} \rightarrow \hspace{1em} x < 10^{\frac{1}{\log(10)}}\]
Per cui per $x > 10^{\frac{1}{\log(10)}}$, la funzione è decrescente. Essendo soddisfatte tutte e tre le condizioni del criterio di Leibniz, la serie
\[\sum_{n=3}^{+\infty} \frac{\log_{10}(n)}{n}\]
converge ad $s$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che nell'esempio precedente, non è possibile applicare la formula di approssimazione del criterio di Leibniz, in quanto le condizioni di Leibniz non sono soddisfatte per tutti gli $n$ (infatti i primi termine della serie potrebbero andare ad alterare il valore della somma finale). Se la serie partisse da $n=n_\epsilon$ con $n_\epsilon>10^{\frac{1}{\log(10)}}$, allora si potrebbe applicare la stima dell'errore di Leibniz.

\vspace{2em}
\noindent
\textbf{Esercizio 2}: Si consideri la serie seguente
\[\sum_{n=0}^{+\infty} \dfrac{\sin \left(\dfrac{\pi}{3} \cdot (1+3n)\right)}{1+3n}\]
che, in prima approssimazione, sembra non essere assolutamente convergente, in quanto il suo comportamento asintotico risulta essere simile a quello della serie armonica.\\
Per verificare se essa sia convergente semplicemente, si verifica se essa soddisfa le tre condizioni di Leibniz; riscrivendo il numeratore del termine generale si ha
\[\sin \left(\dfrac{\pi}{3} \cdot (1+3n)\right) = \sin \left(\frac{\pi}{3} + \pi n\right) = (-1)^n \cdot \sin\left(\frac{\pi}{3}\right) = (-1)^n \cdot \frac{\sqrt{3}}{2}\]
Ecco, quindi, che la serie può essere riscritta come
\[\sum_{n=0}^{+\infty} (-1)^n \cdot \underbrace{\dfrac{\dfrac{\sqrt{3}}{2}}{1+3n}}_{a_n}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si presti particolare attenzione che, in questo ultimo caso, è stato fondamentale riscrivere il termine generale, mettendo in evidenza il fattore $(-1)^n$, in quanto per verificare le $3$ ipotesi del criterio di Leibniz, bisogna studiare il termine
\[\dfrac{\dfrac{\sqrt{3}}{2}}{1+3n}\]
che risulta essere
\begin{enumerate}
    \item a termini positivi
    \item infinitesimo
    \item decrescente
\end{enumerate}
Se ne evince che la serie di partenza è convergente per il criterio di Leibniz.

\newpage
\noindent
\textbf{Esercizio}: Si consideri la seguente serie, posto $\alpha \in \mathbb{R}$:
\[\sum_{n=1}^{+\infty} \frac{\alpha^n + (-5)^n}{5^n} \cdot \sin \left(\pi + \frac{1}{n}\right)\]
Analizzando il termine generale, è immediato osservare che in esso può essere eseguita la semplificazione seguente:
\[\sin \left(\pi + \frac{1}{n}\right) = - \sin \left(\frac{1}{n}\right)\]
Pertanto si ottiene
\[\sum_{n=1}^{+\infty} - \frac{\alpha^n+(-5)^n}{5^n} \cdot \sin \left(\frac{1}{n}\right)\]
Tuttavia, si può osservare immediatamente che se $\left \vert \alpha \right \vert > 5$, la serie non converge, in quanto il termine generale non è infinitesimo.\\
Se $\alpha=-5$, si ottiene il termine generale
\[- \frac{2 \cdot (-1)^n \cdot 5^n}{5^n} \cdot \sin \left(\frac{1}{n}\right) = -2 \cdot (-1)^n \cdot \sin \left(\frac{1}{n} \right)\]
in cui il termine
\[a_n = \sin \left(\frac{1}{n} \right)\]
soddisfa le $3$ condizioni del criterio di Leibniz, in quanto sempre positivo, infinitesimo e decrescente, quindi la serie di partenza converge.\\
Nel caso in cui $\alpha=5$, si ottiene il termine generale
\[- \frac{5^n + (-1)^n \cdot 5^n}{5^n} \cdot \sin \left(\frac{1}{n}\right) = - \left( \sin \left(\frac{1}{n}\right) (-1)^n \cdot \sin \left(\frac{1}{n}\right)\right)\]
È immediato evincere che il secondo addendo converge per il criterio di Leibniz per quanto già osservato; tuttavia, il primo addendo non converge, in quanto infinitesimo di ordine $1$. Ciò porta a formulare le considerazioni seguenti:
\begin{itemize}
    \item se una serie presenta un termine generale che può essere scritto come somma di più termini e ciascuno di tali addendi, preso singolarmente, porta alla convergenza del proprio \quotes{pezzo} di serie, allora anche la serie di partenza converge.
    \item se una serie presenta un termine generale che può essere scritto come somma di più termini, ma alcuni convergono e altri non convergono (ma divergono tutti a $+\infty$ o $-\infty$) allora la serie di partenza non può convergere. Questo perché se essa convergesse, la somma dei termini divergenti potrebbe essere ottenuta come differenza tra la somma finita delle serie principale e dei \quotes{pezzi} convergenti, che è assurdo.
\end{itemize}
Nel caso in cui $\vert \alpha \vert < 5$, spezzando il termine generale si ottiene
\[\left(\frac{\alpha}{5}\right)^n \cdot \left(-\sin \left( \frac{1}{n}\right)\right) + (-1)^n \cdot \sin \left(\frac{1}{n}\right)\]
in cui il primo addendo porta alla convergenza della serie corrispondente, in base al confronto con la geometrica, in quanto:
\[\left \vert \left(\frac{\alpha}{5}\right)^n \cdot \left(-\sin \left( \frac{1}{n}\right)\right) \right \vert \leq \left( \frac{\alpha}{5}\right)^n \hspace{1em} \text{siccome} \hspace{1em} \left \vert \frac{\alpha}{5} \right \vert < 1 \hspace{1em} \text{converge assolutamente e quindi semplicemente.}\]
Convergendo anche il secondo addendo per il criterio di Leibniz, si evince che la serie di partenza converge (semplicemente, e non assolutamente, in quanto solo uno dei due termini converge assolutamente).

\newpage
\noindent
\subsection{Successione di Cauchy}
Di seguito si espone la definizione di \textbf{successione di Cauchy}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SUCCESSIONE DI CAUCHY}}\\
    \parbox{\linewidth}{Sia $(z_n)_n$ una successione in $\mathbb{C}$; si dirà che $(z_n)_n$ è una successione di Cauchy (o soddisfa il criterio di Cauchy) se
    \[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n \geq n_\epsilon, \forall p \in \mathbb{N} \rightarrow \left \vert z_{n+p} - z_n \right \vert < \epsilon\]
    in cui è da intendersi $\vert \cdot \vert$ come modulo, essendo in $\mathbb{C}$. Tale definizione rassomiglia quella di limite, ma il vantaggio è che non c'è il limite.
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\begin{theorem} 
    Se esiste finito
    \[\lim_{n \to +\infty} z_n = l\]
    allora la successione è di Cauchy.
\end{theorem}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Se esiste finito
\[\lim_{n \to +\infty} z_n = l\]
allora, per la definizione di limite, fissato $\epsilon > 0, \exists n \epsilon \in \mathbb{N}$ tale che $\forall n \geq n_\epsilon$, si ha che
\[\left \vert z_n - l \right \vert < \frac{\epsilon}{2}\]
Allora, $\forall n \geq n_\epsilon$, $\forall p \in \mathbb{N}$, si ottiene
\[\left \vert z_{n+p} - z_n \right \vert \leq \underbrace{\left \vert z_{n+p} - l\right \vert}_{< \frac{\epsilon}{2}} + \underbrace{\left \vert l - z_n\right \vert}_{< \frac{\epsilon}{2}} < \epsilon\]

\vspace{1em}
\noindent
\subsubsection{Teorema di completezza dello spazio $\mathbb{C}$ (o $\mathbb{R}$)}
Si espone di seguito il \textbf{teorema di completezza dello spazio $\mathbb{C}$ (o $\mathbb{R}$)}:

\vspace{1em}
\noindent
\begin{theorem}
    Ogni successione di Cauchy in $\mathbb{C}$ (o in $\mathbb{R}$) è convergente.
\end{theorem}

\vspace{1em}
\noindent
\textbf{Osservazione 1}: Si osservi che la convenienza nel dimostrare che una successione è di Cauchy in luogo del fatto che sia convergente è che non si richiede, nel primo caso, di conoscere quale sia il limite.\\
Tale teorema, però, non vale in $\mathbb{Q}$ in quanto una successione di razionali che tende ad un razionale è ovviamente di Cauchy, in quanto lo è in $\mathbb{R}$, ma non è convergente, in quanto tende ad un irrazionale.

\vspace{2em}
\noindent
\textbf{Osservazione 2}: Tale teorema si applica anche alle serie. Per esse, infatti, dire che la successione delle ridotte $(s_n)_n$ è di Cauchy significa che
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{0.5em} \text{tale che} \hspace{0.5em} \forall n \geq n_\epsilon, \forall p \in \mathbb{N} \rightarrow \left \vert s_{n+p} - s_n \right \vert < \epsilon\]
ma esplicitando i termini $s_{n+p}$ e $s_n$ si ottiene:
\[\left \vert \sum_{k=0}^{n+p} a_k - \sum_{k=0}^n a_k \right \vert < \epsilon \hspace{1em} \rightarrow \hspace{1em} \left \vert \sum_{k=n+1}^{n+p} a_k \right \vert < \epsilon\]
in cui per le serie a termini positivi, può essere rimosso il modulo.

\newpage
\noindent
\subsubsection{Criterio di Cauchy per la convergenza di una serie}
Di seguito si espone il \textbf{criterio di Cauchy per la convergenza di una serie}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{CRITERIO DI CAUCHY PER LA CONVERGENZA DELLE SERIE}}\\
    \parbox{\linewidth}{Una serie
    \[\sum a_n\]
    converge \textbf{se e solo se} $\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N}$ tale che $\forall n \geq n_\epsilon$ e $\forall p \in \mathbb{N}$ vale
    \[\left \vert \sum_{k=n+1}^{n+p} a_k \right \vert < \epsilon\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\subsubsection{Applicazioni del criterio di Cauchy}
\textbf{Concetto 1}: Si applichi il criterio di Cauchy per dimostrare che se una serie è assolutamente convergente, allora lo è anche semplicemente:

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Si consideri la serie
\[\sum_{n=1}^{+\infty} a_n\]
allora si dirà che la serie converge assolutamente se la serie dei moduli è convergente, ossia la serie
\[\sum_{n=1}^{+\infty} \left \vert a_n \right \vert\]
è convergente.\\
Se una serie
\[\sum_{n=1}^{+\infty} a_n\]
converge assolutamente, allora la serie dei moduli
\[\sum_{n=1}^{+\infty} \left \vert a_n \right \vert\]
è di Cauchy (per il teorema esposto in precedenza), ossia, secondo la definizione del criterio Cauchy, si ha che
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{1em} \text{tale che} \hspace{1em} \forall n \geq n_\epsilon, \forall p \in \mathbb{N} \hspace{1em} \text{ si ha che } \hspace{1em} \left \vert \sum_{k=n+1}^{n+p} \left \vert a_k \right \vert \right \vert= \sum_{k=n+1}^{n+p} \left \vert a_k \right \vert < \epsilon\]
Per dimostrare che anche la serie di partenza (priva del modulo) è di Cauchy, in quanto è noto che una serie è convergente se e solo se è di Cauchy, si sfrutta la disuguaglianza triangolare (ossia il modulo della somma è minore della somma dei moduli), per cui
\[\left \vert \sum_{k=n+1}^{n+p} a_k \right \vert \leq \sum_{k=n+1}^{n+p} \vert a_k \vert < \epsilon\]
e quindi, essendo la serie degli $a_k$ di Cauchy, è convergente.

\newpage
\noindent
\textbf{Concetto 2}: Si dimostri, tramite il criterio di Cauchy, che la serie armonica è divergente a $+\infty$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Considerando la ridotta $n$-esima della serie armonica, si ha
\[s_n = \sum_{k=1}^{n} \frac{1}{k}\]
che, per come è stata costruita, è positiva e crescente, per cui, per il \textbf{teorema di esistenza del limite delle successioni monotone}, si può affermare che
\[\exists \lim_{n \to + \infty} s_n\] 
finito o infinito (che è praticamente l'aut-aut, essendo una serie a termini positivi). Supponendo, ora, per assurdo, che la serie armonica sia convergente, si dimostri che la serie non può essere di Cauchy, ovvero che sia verificata
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{1em} \text{tale che} \hspace{1em} \forall n \geq n_\epsilon, \forall p \in \mathbb{N} \hspace{1em} \text{si ha che} \hspace{1em} \sum_{k=n+1}^{n+p} \frac{1}{k} < \epsilon\]
Allora, posto $n \geq n_\epsilon$ e $p$ qualsiasi, si ha, per la disuguaglianza appena esposta che
\[\sum_{k=n+1}^{n+p} \frac{1}{k} = \underbrace{\frac{1}{n+1} + \frac{1}{n+2} + \dots + \frac{1}{n+p}}_{p\text{ elementi}} < \epsilon\]
in cui è facile capire che il numero di addendi sommati è pari a $p$, in quanto $(n+p)-(n+1)+1=p$. Dal momento che la definizione di Cauchy richiede qualsiasi valore di $p$ naturale, sia fissato $p=n$, per cui
\[\frac{1}{n+p}=\frac{1}{2n} \hspace{1em} \rightarrow \hspace{1em} \epsilon > \underbrace{\frac{1}{n+1}}_{>\frac{1}{2n}} + \underbrace{\frac{1}{n+2}}_{>\frac{1}{2n}} + \dots + \frac{1}{2n} > \underbrace{\frac{1}{2n}+\frac{1}{2n}+\dots+\frac{1}{2n}}_{n\text{ elementi}}\]
ma essendo $n$ addendi, si ottiene che
\[\frac{1}{2n} \cdot n = \frac{1}{2} < \epsilon\]
Ma siccome tale ragionamento vale qualunque sia $\epsilon$, se si fosse fissato, all'inizio della dimostrazione, $\epsilon>0$, come $\epsilon=\frac{1}{10}$, sarebbe stato ottenuto l'assurdo cercato.

\newpage
\section{Successioni e serie di funzioni}
Di seguito si introduce l'importante tema delle successioni e delle serie di funzioni, in cui a ogni indice $n$ naturale viene associata una funzione.

\vspace{1em}
\subsection{Successioni di funzioni}
Se, per esempio, si introduce una successione di funzioni come la seguente
\[f_n(x) = x^n\]
si ottiene, per diversi $n$, che
\begin{align*}
    &f_0(x)=1&&f_1(x)=x&&f_2(x)=x^2&&f_3(x)=x^3&&\text{etc\dots}
\end{align*}
o ancora, nel caso della successione di funzioni
\[f_n(x) = \cos(nx)\]
si ottiene
\begin{align*}
    &f_0(x)=\cos(0)=1&&f_1(x)=\cos(x)&&f_2(x)=\cos(2x)&&f_3(x)=\cos(3x)&&\text{etc\dots}
\end{align*}
o ancora, nel caso della successione di funzioni
\[f_n(x) = \frac{1}{x^2+n}\]
si ottiene
\begin{align*}
    &f_0(x)=\frac{1}{x^2}&&f_1(x)=\frac{1}{x^2+1}&&f_2(x)=\frac{1}{x^2+2}&&f_3(x)=\frac{1}{x^2+3}&&\text{etc\dots}
\end{align*}

\vspace{1em}
\noindent
\subsubsection{Limite puntuale di una successione di funzioni}
Di seguito si espone la definizione di \textbf{limite puntuale di una successione di funzioni}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{LIMITE PUNTUALE DI UNA SUCCESSIONE DI FUNZIONI}}\\
    \parbox{\linewidth}{Siano
    \[f_n : E \longmapsto \mathbb{R} \hspace{1em} \text{e} \hspace{1em} f : E \longmapsto \mathbb{R}\]
    Si dice che la successione $\left(f_n\right)_n$ \textbf{converge puntualmente} a $f$ se, $\forall x \in E$
    \[\lim_{n \to +\infty} f_n(x) = f(x)\] \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Esempio 1}: Si consideri la successione di funzioni
\[f_n(x) = \cos(nx)\]
allora tale successione ammette limite $0$ se $x=0$, non esiste altrimenti.

\vspace{1em}
\noindent
\textbf{Esempio 2}: Si consideri la successione di funzioni
\[f_n(x) = \frac{1}{x^2+n}\]
tale per cui, $\forall x \in \mathbb{R}$
\[\lim_{n \to +\infty} \frac{1}{x^2+n} = 0\]

\vspace{1em}
\noindent
\textbf{Esempio 3}: Si consideri la successione di funzioni
\[f_n : [0,1] \longmapsto \mathbb{R} \hspace{1em} \text{ con } \hspace{1em} f_n(x)=x^n\]
allora
\[\lim_{n \to +\infty} f_n(x) = \left\{
    \rowcolors{1}{white}{white}
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{array}{lll}
        0 & \text{se} & x \in [0,1[\\
        1 & \text{se} & x = 1
    \end{array}
\right.\]

\vspace{1em}
\noindent
\textbf{Esempio 4}: Si consideri la successione di funzioni
\[f_n(x) = \frac{n x}{nx^2 + 1}\]
allora si ha che
\[\lim_{n \to +\infty} f_n(x) = \left\{
    \rowcolors{1}{white}{white}
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{array}{lll}
        0           & \text{se} & x = 0\\
        \dfrac{1}{x} & \text{se} & x \neq 0
    \end{array}
\right.\]

\newpage
\noindent
\begin{center}
    11 Ottobre 2022
\end{center}
Il criterio di Leibniz è un criterio fondamentale per capire la convergenza semplice di una serie a termini alternativamente positivi e negativi. Dopodiché sono state introdotte le successioni di Cauchy e il criterio di Cauchy associato, il quale consente di capire se esiste un limite, senza conoscere il valore del limite, il che risulta fondamentale per decretare la convergenza di una serie.

\vspace{1em}
\noindent
\subsubsection{Limite uniforme di una successione di funzioni}
Si consideri la successione di funzioni
\[f_n : [0,1[ \hspace{1em} \text{ definita come } f_n(x) = x^n\]
Allora $\forall x \in [0,1[$, si ha che
\[\lim_{n \to +\infty} x^n = 0\]
e, per la definizione di limite, si ha
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{1em} \text{tale che} \hspace{1em} \forall n \geq n_\epsilon \hspace{1em} \text{si ha che} \hspace{1em} \vert x^n \vert < \epsilon\]
Per determinare $n_\epsilon$ tale per cui $\forall n \geq n_\epsilon, \vert x^n \vert < \epsilon$, si osserva che
\[x^n < \epsilon \hspace{1em} \rightarrow \hspace{1em} e^{n \cdot \log(x)} < e^{\log(\epsilon)} \hspace{1em} \rightarrow \hspace{1em}n \cdot \log(x) < \log(\epsilon)\]
Ma essendo $\log(x) < 0$ in quanto $x \in [0,1[$, quando si divide per $\log(x)$ negativo, cambia il segno della disuguaglianza. Allora sarà sufficiente considerare $n$ che soddisfa la proprietà
\[n > \frac{\log(\epsilon)}{\log(x)}\]
Fissato $\epsilon=e^{-10}$ e $x=\frac{1}{2}$, allora l'$n_\epsilon$ cercato è
\[n_\epsilon = \frac{\log(e^{-10})}{\log \left(\dfrac{1}{2}\right)}= \frac{10}{\log(2)} \cong 33\]
Per cui sarà necessario considerare una potenza $n>33$ al fine di vedere soddisfatta la proprietà
\[\frac{1}{2}^n < e^{-10}\]
In particolare si ha che
\[\lim_{x \to 1^-} \frac{\log(\epsilon)}{\log(x)} = +\infty\]
che significa che più ci si avvicina con $x$ a $1$, maggiore dovrà essere considerata la potenza di $n$ per vedere soddisfatta la disuguaglianza del limite.\\
In altre parole, \textbf{$n$ dipende fortemente da $x$}: più $x$ tende a $1$ da sinistra, più $n$ deve essere grande al fine di soddisfare il limite di partenza:
\[\lim_{n \to +\infty} x^n = 0\]
Questo perché $0$ è limite puntuale e non uniforme per la successione $f_n$. Ciò porta alla definizione di \textbf{limite uniforme per una successione di funzioni}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{LIMITE UNIFORME}}\\
    \parbox{\linewidth}{Sia $(f_n)_n$ una successione di funzioni, con
    \[f_n : E \longmapsto \mathbb{R} \hspace{1em} \text{e} \hspace{1em} f : E \longmapsto \mathbb{R}\]
    Si dirà che $f$ è limite uniforme della successione $(f_n)_n$, e si scriverà
    \[\lim_{n \to +\infty} f_n = f\]
    \textbf{uniforme}, se
    \[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{1em} \text{ tale che } \hspace{1em} \forall n \geq n_\epsilon, \boxed{\bf{\forall x \in E}} \hspace{1em} \text{ si ha che } \hspace{1em} \left \vert f_n(x) - f(x)\right \vert < \epsilon\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione}: Nel caso di \textbf{limite puntuale}, invece, si ha che
\[\boxed{\bf{\forall x \in E}}, \forall \epsilon > 0, \exists n_{\epsilon, x} \in \mathbb{N} \text{ tale che } \forall n \geq n_{\epsilon, x}, \left \vert f_n(x) - f(x) \right \vert < \epsilon\]
ovvero il $\forall x$ è posto all'inizio della definizione. Ciò implica la forte dipendenza da $x$ (per questo si scrive $n_{\epsilon, x}$) nel caso di limite puntuale, cosa che invece non accade nel caso di un limite uniforme, in cui $n_\epsilon$ si mantiene costante e lo stesso, indipendentemente dalla scelta di $x$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Sia data la successione di funzioni seguente
\[f_n(x) = \dfrac{1}{n+x^2}\]
È facile capire che posto $x=0$ si ottiene il valore massimo della successione, ovvero $\dfrac{1}{n}$. Ciò significa che, per $n$ sufficientemente grande, tutto il grafico della funzione è interamente contenuto nella fascia $<\dfrac{1}{n}$: pertanto la successione converge uniformemente, in quanto
\[\left \vert f_n(x) - 0 \right \vert \leq \dfrac{1}{n} < \epsilon\]
Ciò significa che, fissato $\epsilon$, basterà scegliere $n>\dfrac{1}{\epsilon}$ e il grafico di tutte le funzioni in dipendenza da $n$ sarà contenuto all'interno di una fascia di ampiezza $2\epsilon$ (a causa della presenza del valore assoluto) da avvolgere intorno al limite, dovendo essere $l-\epsilon<f_n(x)<l+\epsilon$.

\vspace{1em}
\noindent
\textbf{Esercizio 1}: Si consideri la successione di funzioni:
\[f_n(x) = \frac{n}{x^2+n}\]
allora
\[\lim_{n \to +\infty} f_n(x) = 1\]
Naturalmente si ha convergenza puntuale, ma non uniforme. Infatti, se fosse uniforme, fissato $\epsilon=\dfrac{1}{100}$ dovrebbe esistere $n_\epsilon \in \mathbb{N}$ tale che $\forall n \geq n_\epsilon, \forall x \in \mathbb{R}$
\[\left \vert \frac{n}{x^2+n} - 1\right \vert < \frac{1}{100}\]
Per dimostrare che ciò non è possibile $\forall x \in \mathbb{R}$, si sviluppa, ottenendo
\[\left \vert \frac{n-x^2-n}{x^2+n} \right \vert = \frac{x^2}{x^2 + n}\]
allora basta scegliere $x=\sqrt{n}$, per ottenere l'assurdo
\[\frac{n}{n+n}=\frac{1}{2} < \frac{1}{100}\]

\vspace{1em}
\noindent
\textbf{Esercizio 2}: Si consideri la successione di funzioni
\[f_n(x) = \frac{nx}{nx^2+1}\]
allora
\[\lim_{n \to +\infty} f_n(x) = \frac{1}{x}\]
che è una convergenza puntuale, ma non uniforme, in quanto, fissato $\epsilon=\dfrac{1}{100}$ dovrebbe esistere $n_\epsilon \in \mathbb{N}$ tale che $\forall n \geq n_\epsilon, \forall x \in \mathbb{R}$
\[\left \vert \frac{nx}{nx^2+1} - \frac{1}{x}\right \vert < \frac{1}{100}\]
e sviluppando si ottiene
\[\frac{1}{x \cdot (nx^2+1)} < \frac{1}{100}\]
Ora, al fine di contraddire tale disuguaglianza, è fondamentale scegliere un $x$ piccolo al fine di ottenere una quantità molto grande che ovviamente non può essere maggiorata da $\epsilon$. Allora basta scegliere $x=\dfrac{1}{\sqrt{n}}$, ottenendo
\[\frac{\sqrt{n}}{2} < \frac{1}{100}\]
che, ovviamente, non è vero $\forall n \in \mathbb{N}$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che se il limite di una successione di funzioni non è continuo (come nel caso del limite $\dfrac{1}{x}$), allora la successione di funzioni non converge uniformemente. Tuttavia, se il limite è continuo, come nel caso di una costante, allora non si può dire nulla sulla tipologia di convergenza della successione di funzioni.

\vspace{1em}
\noindent
\subsection{Teorema di inversione di due limiti}
Si consideri il seguente \textbf{teorema di inversione di due limiti}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{TEOREMA DI INVERSIONE DI DUE LIMITI}}\\
    \parbox{\linewidth}{Sia $f(n)_n$ una successione di funzioni
    \[f_n : E \longmapsto \mathbb{R}\]
    tale che $(f_n)_n$ \textbf{converge uniformemente} a
    \[f : E \longmapsto \mathbb{R}\]
    con $x_0$ punto di accumulazione per $E$. Si supponga, inoltre, che $\forall n$ esista
    \[\exists \lim_{x \to x_0} f_n(x) = l_n\]
    Allora
    \[\exists \lim_{n \to +\infty} l_n = l \hspace{1em} \text{e} \hspace{1em} \exists \lim_{x \to x_0} f(x) = l\]
    pertanto si può affermare che
    \[\lim_{n \to +\infty} \left(\lim_{x \to x_0} f_n(x)\right) = \lim_{x \to x_0} \left(\lim_{n \to +\infty} f_n(x)\right)\] \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione}: Si osservi che il teorema appena esposto vale solamente per successioni di funzioni con convergenza uniforme, non puntuale. Infatti, date
\[f_n : [0,1] \longmapsto \mathbb{R} \hspace{1em} \text{e} \hspace{1em} f : [0,1] \longmapsto \mathbb{R}\]
con $f_n=x^n$, si ottiene l'inesattezza seguente
\[\lim_{n \to +\infty} \left(\lim_{x \to 1} x^n\right) = 1 \neq 0 = \lim_{x \to 1} \left(\lim_{n \to +\infty} x^n\right)\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Per la dimostrazione si considera il criterio di Cauchy, fondamentale per dimostrare l'esistenza del limite
\[\lim_{n \to +\infty} l_n\]
senza conoscerlo. Bisogna, dunque, dimostrare che la successione $(l_n)_n$ è di Cauchy, ossia che
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{1em} \text{tale che} \hspace{1em} \forall n \geq n_\epsilon, \forall p \in \mathbb{N} \hspace{1em} \text{si ha che} \hspace{1em} \left \vert l_{n+p} - l_n \right \vert < \epsilon\]
In particolare, $\forall x \in E$, il valore assoluto di cui sopra può essere riscritto aggiungendo e sottraendo le quantità $f_{n+p}(x)$ e $f_n(x)$ come segue:
\[\left \vert l_{n+p} - l_n\right \vert = \left \vert l_{n+p} - l_n - f_{n+p}(x) + f_{n+p}(x) - f_n(x) + f_n(x)\right \vert\]
Sfruttando la disuguaglianza triangolare, si può maggiorare tale valore assoluto come
\[\leq \left \vert l_{n+p} - f_{n+p}(x)\right \vert + \left \vert f_{n+p}(x) - f_n(x) \right \vert + \left \vert f_n(x) - l_n\right \vert\]
Ora bisogna dimostrare che ogni singolo addendo è $< \dfrac{\epsilon}{3}$. Si procede per osservazioni successive:
\begin{enumerate}
    \item Siccome $(f_n)_n$ è uniformemente convergente, in particolare è una successione di Cauchy. Ciò significa che
    \[\exists n_\epsilon \in \mathbb{N} \hspace{1em} \text{tale che} \hspace{1em} \forall n \geq n_\epsilon, \forall p \in \mathbb{N} \text{ e } \boxed{\bf{\forall x \in E}} \hspace{1em} \text{si ha che} \hspace{1em} \left \vert f_{n+p}(x) - f_n(x)\right \vert < \frac{\epsilon}{3}\]
    in cui è fondamentale osservare che ciò vale $\forall x \in E$, in cui l'$n_\epsilon$ considerato dipende solamente da $n$ e non da $x$.

    \item Fissato un qualsiasi $\hat n \geq n_\epsilon$ e un qualsiasi $\hat p \in \mathbb{N}$, è noto per ipotesi che 
    \[\lim_{x \to x_0} f_{\hat n}(x) = l_{\hat n} \hspace{1em} \text{ e } \hspace{1em} \lim_{x \to x_0} f_{\hat n + \hat p}(x) = l_{\hat n + \hat p}\]
    Allora, dalla definizione di limite si ha che
    \begin{align*}
        &\exists \delta_{\hat n+\hat p} > 0 \hspace{1em} &\text{tale che} \hspace{1em} &\forall x \in E, x \neq x_0, \left \vert x - x_0 \right \vert < \delta_{\hat n + \hat p} \hspace{1em} &\text{ si ha che } \hspace{1em} &\left \vert f_{\hat n + \hat p}(x) - l_{\hat n + \hat p}\right \vert < \frac{\epsilon}{3}\\
        &\exists \delta_{\hat n} > 0 \hspace{1em} &\text{tale che} \hspace{1em} &\forall x \in E, x \neq x_0, \vert x - x_0 \vert < \delta_{\hat n} \hspace{1em} &\text{ si ha che } \hspace{1em} &\left \vert f_{\hat n}(x) - l_{\hat n} \right \vert < \frac{\epsilon}{3}
    \end{align*}
    Ma ciò consente di affermare che, preso il $\delta$ più piccolo di entrambi, ovvero $\delta < \min\left\{\delta_{\hat n + \hat p}, \delta_{\hat n}\right\}$, valgono ambedue le affermazioni precedenti, ovverosia
    \[\left \vert l_{\hat n + \hat p} - l_{\hat n} \right \vert \leq \left \vert l_{\hat n + \hat p} - f_{\hat n + \hat p}(x)\right \vert + \left \vert f_{\hat n + \hat p}(x) - f_{\hat n}(x)\right \vert + \left \vert f_{\hat n}(x) - l_{\hat n}\right \vert < \frac{\epsilon}{3}+\frac{\epsilon}{3}+\frac{\epsilon}{3} < \epsilon\]
\end{enumerate}
È fondamentale capire, tuttavia, che i valori $\hat n$ e $\hat n + \hat p$ sono fissati e non vengono più modificati nel corso della dimostrazione, ma sono sempre $\hat n$ e $\hat n + \hat p$ che soddisfano la condizione iniziale $n \geq n_\epsilon$; questo è fondamentale perché il $\delta$ impiegato alla fine è il minimo tra due valori che dipende fortemente dagli indici: se si volesse generalizzare la dimostrazione per tutti gli $n$ non è detto che esista il minimo degli opportuni $\delta_n$.\\
Ciò, quindi, consente di affermare che
\[\exists \lim_{n \to +\infty} l_n = l \rightarrow \left \vert l_{n+p} - l_n \right \vert < \epsilon\]
come richiesto dal criterio di Cauchy.

% Formattazione per la dimostrazione, etc.
\newpage
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Ripetendo la dimostrazione appena eseguita, si dimostri il limite seguente
\[\lim_{x \to x_0} f(x) = l\]
ovverosia
\[\forall \epsilon>0,\exists \delta > 0 \hspace{1em} \text{tale che} \hspace{1em} \forall x \in E, x \neq x_0, \left \vert x - x_0 \right \vert < \delta \hspace{1em} \text{ si ha che } \hspace{1em} \left \vert f(x) - l \right \vert < \epsilon\]
Sommando e sottraendo i termini $l_n$ e $f_n(x)$ e sfruttando la disuguaglianza triangolare, si ha che
\[\left \vert f(x) - l \right \vert \leq \left \vert f(x) - l + l_n - l_n + f_n(x) - f_n(x) \right \vert \leq \left \vert - \left(f_n(x) - f(x)\right) \right \vert + \left \vert f_n(x) - l_n \right \vert + \left \vert l_n - l \right \vert < \epsilon\]
Per dimostrare che ciascuno degli addendi, di fatto, è minore di $\dfrac{\epsilon}{3}$ si formulano le seguenti osservazioni:
\begin{enumerate}
    \item In particolare è noto che
    \[\lim_{n \to +\infty} l_n = l\]
    per cui, fissato $\dfrac{\epsilon}{3}$ è noto che esiste $n^1_\epsilon$ tale che $\forall n \geq n^1_\epsilon$ si ha
    \[\left \vert l_n - l \right \vert < \frac{\epsilon}{3}\]

    \item Inoltre, poiché
    \[\lim_{n \to +\infty} f_n = f\]
    uniforme, esiste $n^2_\epsilon \in \mathbb{N}$ tale che $\forall n \geq n^2_\epsilon$, si ha
    \[\left \vert f_n(x) - f(x) \right \vert < \frac{\epsilon}{3}, \hspace{1em} \forall x \in E\]

    \item Fissato, quindi, $\hat n \geq \max \{n^1_\epsilon,n^2_\epsilon\}$. Per questo $\hat n$ fissato, siccome
    \[\lim_{x \to x_0} f_{\hat n}(x) = l_{\hat n}\]
    si ha che
    \[\exists \delta_{\hat n} > 0 \hspace{1em} \text{tale che} \hspace{1em}\forall x \in E, x \neq x_0, \left \vert x - x_0 \right \vert < \delta_{\hat n} \hspace{1em} \text{si ha che} \hspace{1em} \left \vert f_{\hat n}(x) - l_{\hat n} \right \vert < \frac{\epsilon}{3}\]
    ma ciò vale solo per questo $\hat n$ fissato. Ovviamente, $\delta_{\hat n}$ dipende da $\hat n$, il quale, a sua volta, dipende da $n^1_\epsilon$ e $n^2_\epsilon$, per cui, in definitiva, dipende da $\epsilon$.
\end{enumerate}

\vspace{2em}
\noindent
\textbf{Ricapitolando}: Bisogna dimostrare il limite di cui sopra esiste, ovvero che se $\left \vert x - x_0 \right \vert < \delta$ si ha che
\[\left \vert f(x) - l \right \vert < \epsilon\]
Per fare ciò si maggiore l'ultima disuguaglianza, ottenendo
\[\left \vert f(x) - l \right \vert \leq \left \vert f(x) - f_n(x) \right \vert + \left \vert f_n(x) - l_n \right \vert + \left \vert l_n - l \right \vert\]
In particolare
\begin{itemize}
    \item $\left \vert f(x) - f_n(x) \right \vert < \dfrac{\epsilon}{3}$, se $n \geq n_1$, per la convergenza uniforme della $f$;
    \item $\left \vert l_n - l \right \vert < \dfrac{\epsilon}{3}$, se $n \geq n_2$, per il limite noto per ipotesi.
    \item Preso $n \geq \max\{n_1,n_2\}$, tale per cui ambedue le disuguaglianze di cui sopra sono verificate. Fissato tale $n$, dal momento che è noto per ipotesi che
    \[\exists \lim_{x \to x_0} f_n(x) = l_n\]
    si ha che $\exists \delta > 0$ tale che
    \[0 < \left \vert x - x_0 \right \vert < \delta \hspace{1em} \rightarrow \hspace{1em} \left \vert f_n(x) - l_n \right \vert < \dfrac{\epsilon}{3}\]
\end{itemize}

\vspace{1em}
\begin{corollary}
    Si osservi che se
    \[f_n : E \longmapsto \mathbb{R}\]
    è \textbf{continua} $\forall n$ e 
    \[\lim_{n\to +\infty} f_n = f\]
    \textbf{uniforme}. Allora $f$ è \textbf{continua}.
\end{corollary}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Naturalmente preso un punto isolato non ha senso parlare di limite, e la funzione è automaticamente continua. Pertanto, preso $x_0$ punto di accumulazione per $E$, è immediatamente evidente osservare che
\[\lim_{x \to x_0} f(x) = \lim_{x \to x_0} \left(\lim_{n \to +\infty} f_n(x)\right)\]
ovviamente è ora possibile applicare il teorema di inversione dei limiti essendo la convergenza di $f_n$ uniforme, per cui
\[\lim_{x \to x_0} \left(\lim_{n \to +\infty} f_n(x)\right) = \lim_{n \to +\infty} \left(\lim_{x \to x_0} f_n(x)\right)\]
ma essendo $f_n$ continua $\forall n$ è immediato evincere che
\[\lim_{x \to x_0} \left(\lim_{n \to +\infty} f_n(x)\right)= \lim_{n \to +\infty} f_n(x_0) = f(x_0)\]
in cui si è potuto affermare che
\[\lim_{n \to +\infty} f_n(x_0) = f(x_0)\]
in quanto la convergenza uniforme implica la convergenza puntuale.

\vspace{2em}
\noindent
\textbf{Osservazione}: Si osservi che
\[\lim_{n \to +\infty} f_n(x) = f(x)\]
uniforme, lo è anche puntuale, ma non viceversa. Infatti richiedere la convergenza uniforme è molto di più che richiedere la convergenza puntuale, in quanto nel primo caso $n_\epsilon$ non dipende da $x$, mentre nel secondo sì. Per cui è immediato evincere che la prima implica la seconda.

\vspace{2em}
\noindent
\subsection{Teorema di integrabilità}
Si espone di seguito il \textbf{teorema di integrabilità}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{TEOREMA DI INTEGRABILITÀ}}\\
    \parbox{\linewidth}{
        Sia $I \subseteq \mathbb{R}$ un \textbf{intervallo compatto} (ovvero limitato, con misura finita, denotata con $m(I)$) e sia 
        \[f_n : I \longmapsto \mathbb{R}\]
        \textbf{integrabile}, $\forall n$; sia, inoltre
        \[\lim_{n \to +\infty} f_n = f\]
        \textbf{uniforme}, con
        \[f : I \longmapsto \mathbb{R}\]
        allora $f$ è \textbf{integrabile} e si ha che
        \[\int_I \lim_{n \to +\infty} f_n(x) \dif x = \lim_{n \to +\infty} \int_I f_n(x) \dif x\]
        \vspace{-1mm}}\\
    \hline
\end{tabularx}


% Formattazione per la dimostrazione, etc.
\newpage
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: La parte complicata della dimostrazione è verificare l'integrabilità della $f$. Tale parte viene tralasciata, preferendo concentrarsi sul verificare la correttezza della formula. Parlando di integrale di Riemann, è noto che il valore assoluto dell'integrale è minore o uguale dell'integrale del valore assoluto; ciò permette di affermare che, sfruttando la linearità dell'integrale:
\[\left \vert \int_I f_n(x) \dif x - \int_I f(x) \dif x \right \vert \leq \int_I \left \vert f_n(x) - f(x) \right \vert \dif x < \epsilon \cdot m(I)\]
in quanto $\left \vert f_n(x) - f(x) \right \vert < \epsilon, \forall x$ se $n \geq n_\epsilon$ per la convergenza uniforme della $f_n$.

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la seguente successione di funzioni
\[f_n : [0,1] \longmapsto \mathbb{R}\]
in cui
\[f_n(x) = \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{lll}
        0 & \text{ se } & x \in \{0\} \cup \left]\dfrac{1}{n},1\right]\\
        n & \text{ se } & x \in \left]0,\dfrac{1}{n}\right]
    \end{array}
\right.\]
in cui appare evidente come
\[\int_{[0,1]} f_n(x) \dif x = 1, \forall n\]
e, ovviamente,
\[\lim_{n \to +\infty} \int_{[0,1]} f_n(x) = 1\]
mentre
\[\int_{[0,1]} \left(\lim_{n \to +\infty} f_n(x) \right) \dif x = \int_{[0,1]} 0 \dif x = 0\]
per cui la formula vista in precedenza non vale essendo la convergenza puntuale. Ovvero
\[\lim_{n \to +\infty} f_n(x) = 0, \hspace{1em} \forall x\]
puntualmente.

\newpage
\noindent
\begin{center}
    12 Ottobre 2022
\end{center}
È molto importante tenere in considerazione la differenza di definizione di convergenza puntuale e uniforme di una successione di funzioni. Infatti
\begin{itemize}
    \item se $f_n \to f$ \textbf{puntualmente}, allora
    \[\forall x, \forall \epsilon > 0, \exists n_{\epsilon,x} \in \mathbb{N} \hspace{1em} \text{tale per cui} \hspace{1em} \forall n \geq n_{\epsilon,x} \hspace{1em}\text{ si ottiene } \hspace{1em} \left \vert f_n(x) - f(x) \right \vert < \epsilon\]

    \item se $f_n \to f$ \textbf{uniformemente}, allora
    \[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{1em} \text{tale per cui} \hspace{1em} \forall n \geq n_\epsilon, \forall x \hspace{1em} \text{ si ottiene } \hspace{1em} \left \vert f_n(x) - f(x) \right \vert < \epsilon\]
\end{itemize}
In cui è fondamentale capire la differenza di collocamento di $\forall x$: se posto all'inizio, come nel caso della convergenza puntuale, l'$n_\epsilon$ dipende anche da $x$; se posto alla fine, come nel caso della convergenza uniforme, non si ha tale dipendenza, per cui $n_\epsilon$ è il medesimo $\forall x$.\\
Non solo, ma è noto che l'uniforme convergenza di una successione di funzioni implica la puntuale convergenza. Inoltre, se una successione di funzioni continue $\forall n$ converge uniformemente ad una funzione $f$, allora essa è continua. Se le funzioni $f_n$ sono integrabili $\forall n$, allora il limite uniforme $f$ è anch'esso integrabile (è facile che capire, però, che se le funzioni $f_n$ sono continue $\forall n$, allora sono integrabili, per cui anche il limite uniforme $f$ è continuo e quindi integrabile, ma in generale non è detto che le $f_n$ siano continue).

\vspace{1em}
\subsection{Teorema sulla derivata del limite}
Di seguito si espone l'enunciato del \textbf{teorema sulla derivata del limite}, in cui si puntualizza come \textbf{non sia vero} che il limite uniforme di una successione di funzioni derivabile sia derivabile. Tuttavia si ha che

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{TEOREMA SULLA DERIVATA DEL LIMITE}}\\
    \parbox{\linewidth}{Siano $f_n : E \to \mathbb{R}$ derivabili con derivata continua, ossia $f_n \in C^1(E)$. Sia
    \[\lim_{n \to +\infty} f_n(x) = f(x)\]
    \textbf{puntuale} (non è neanche sufficiente che la convergenza sia puntuale in ogni punto, ma basterebbe un punto solo). Sia, invece,
    \[\lim_{n \to +\infty} f'_n = g\]
    \textbf{uniforme} (ossia si richiede il limite uniforme sulla successione delle derivate, e non della funzione $f_n$). Se valgono tali condizioni, allora $f$ è derivabile e $f'=g$.\vspace{3mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Sia $x_0 \in E$ e sia $E$ un intervallo (o un'unione finita di intervalli). Dal teorema fondamentale del calcolo si ha che
\[f_n(x) = f_n(x_0) + \int_{x_0}^x f'_{n}(t) \dif t\]
Da notare che è stato possibile considerare $f'_{n}(t)$ come funzione integranda, in quanto per ipotesi $f_n(x) \in C^1(E)$, per cui presenta una derivata continua e quindi integrabile.\\
Si consideri, ora il limite puntuale della successione $f_n$, per $n \to +\infty$. Dal momento che $f'_n(x)$ è una successione di funzioni integrabile, in quanto continua, e convergente uniformemente a $g$, si può sfruttare il teorema di integrabilità, per cui il limite per $n \to +\infty$ dell'integrale è l'integrale del limite, ovvero
\[f(x) = f_{x_0} + \int_{x_0}^x g(t) \dif t\]
ma $g$ è continua perché limite uniforme di una successione di funzioni continue, per cui la funzione
\[f(x) = f_{x_0} + \int_{x_0}^x g(t) \dif t\]
è derivabile per il teorema fondamentale del calcolo e la sua derivata è
\[f'(x) = g(x)\]

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la successione di funzioni seguente
\[f_n(x) = \sqrt{x^2 + \frac{1}{n}}\]
allora è evidente che tale successione converge uniformemente a
\[\lim_{n \to +\infty} f_n = \vert x \vert\]
È immediato evincere che si tratti di una convergenza uniforme, in quanto
\[\left \vert \sqrt{x^2 + \frac{1}{n}} - \vert x \vert \right \vert = \left(\sqrt{x^2 + \frac{1}{n}} - \vert x \vert \right)\cdot \dfrac{\sqrt{x^2 + \dfrac{1}{n}}+ \vert x \vert }{\sqrt{x^2 + \dfrac{1}{n}} + \vert x \vert} = \dfrac{x^2 + \dfrac{1}{n} - x^2}{\sqrt{x^2 + \dfrac{1}{n}} + \vert x \vert}\]
Tramite una banale maggiorazione, riducendo il denominatore, si ottiene che
\[\dfrac{x^2 + \dfrac{1}{n} - x^2}{\sqrt{x^2 + \dfrac{1}{n}} + \vert x \vert} < \dfrac{x^2 + \dfrac{1}{n} - x^2}{\dfrac{1}{\sqrt{n}}} = \dfrac{1}{\sqrt{n}}\]
che ovviamente può essere reso piccolo quanto si vuole $\left(\text{basta scegliere }n>\dfrac{1}{\epsilon^2}\right)$, indipendentemente da $x$, da cui la convergenza uniforme.\\
Non solo, ma si ha che $f_n$ è derivabile $\forall n$ e, in particolare,
\[f'_n(x) = \dfrac{x}{\sqrt{x^2 + \dfrac{1}{n}}}\]
Tuttavia, il limite uniforme $f=\vert x\vert$ non è derivabile.

\vspace{2em}
\noindent
\textbf{Osservazione}: L'ipotesi del teorema sulla derivata richiede che
\[f_n(x) \to f(x)\]
puntualmente. Tuttavia, si richiede che la successione delle derivate converga uniformemente a $g$. Ha senso chiedersi se, a posteriori, effettivamente, la successione di funzioni di partenza converga anch'essa uniformemente a $f$. In altre parole, il teorema potrebbe suggerire che, siccome
\[f_n'(x) \to f'(x)\]
uniformemente, allora anche la successione di funzioni di partenza converge a $f$ uniformemente. Tuttavia, ciò è falso, e il controesempio è
\[f_n(x) = \frac{n}{n+x^2}\]
la quale è una successione che converge puntualmente a $1$, ossia
\[\lim_{n \to +\infty} f_n(x) = 1\]
ma non in modo uniforme (in quanto per $n$ piccolo fissato, se $x \to +\infty$ la funzione tende a $0$, per cui il suo grafico non può essere contenuto nella fascia $1-\epsilon,1+\epsilon$, $\forall \epsilon$). Tuttavia, si ha che
\[f'_n(x) = \frac{-2nx}{(n+x^2)^2}\]
in cui si ha che la successione delle derivate $f'_n$ converge
\[\lim_{n \to +\infty} f'_n = 0\]
uniformemente. Per dimostrarlo si ottiene che
\[\left \vert f'_n(x) - 0 \right \vert = \frac{2nx}{(n+x^2)^2} = g_n(x)\]
Per trovare una maggiorazione di $g_n(x)$, non si può fissare $x=\sqrt{n}$, in quanto questo legherebbe la dimostrazione all'$x$ scelto. Pertanto, si considera il massimo della funzione, tramite la derivata della $g_n$ stessa, ovvero
\[g'_n(x) = \frac{2n \cdot (n+x^2)^2 - 4n x \cdot (n+x^2) \cdot 2x}{(n+x^2)^3} = \frac{2n \cdot (n-3x^2)}{(n+x^2)^4}\]
È facile capire che per
\[x^2=\frac{n}{3} \hspace{1em} \rightarrow \hspace{1em} g'_n(x) = 0\]
Essendo la $g_n$ una funzione dispari, in cui $g_n(0)=0$ e per $n \to +\infty$ $g_n(x) \to 0$, esiste certamente un massimo: il punto di estremo relativo è $x=\sqrt{\dfrac{n}{3}}$, che è proprio è un punto di massimo per la funzione $f'_n$. Per ottenere il massimo cercato, si calcola
\[\left \vert g_n \left(\sqrt{\dfrac{n}{3}}\right) \right \vert = \left \vert f'_n \left(\sqrt{\dfrac{n}{3}}\right) \right \vert = K \cdot \dfrac{1}{\sqrt{n}}\]
per cui si è ottenuto che la la $\vert f'_n(x) - 0 \vert$ può essere maggiorata da una quantità che dipende solamente da $n$, ovvero
\[\left \vert f'_n(x) - 0 \right \vert = \frac{2nx}{(n+x^2)^2} = g_n(x) < K \cdot \dfrac{1}{\sqrt{n}} < \epsilon\]
Ciò dimostra che una successione di funzioni può presentare una convergenza puntuale, ma non uniforme, mentre la successione delle sue derivata presenta una convergenza uniforme.

\vspace{1em}
\noindent
\subsection{Serie di funzioni}
Di seguito si espone la definizione di \textbf{serie di funzioni}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SERIE DI FUNZIONI}}\\
    \parbox{\linewidth}{Data una successione di funzioni $(f_n)_n$ con valori nel campo reale $f_n : E \longmapsto \mathbb{R}$. Si chiama serie di funzioni l'oggetto matematico seguente
    \[\sum_{n=0}^{+\infty} f_n(x)\]
    in cui la somma parziale $s_n(x)$ è una funzione ed è definita come
    \[s_n(x) = \sum_{k=0}^n f_k(x)\]
    per cui serie di funzioni si dirà convergente alla somma $s(x)$, che è sempre una funzione, se la successione delle somme parziali $s_n(x)$ converge a lla funzione $s(x)$, ossia $s_n(x) \to s(x)$; dal momento che la successione di somme parziali può convergere alla funzione $s(x)$ in modo uniforme o solo puntuale, si parlerà di \textbf{limite uniforme} o \textbf{puntuale} della serie.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Osservazione 1}: Sia
\[\sum_{n=0}^{+\infty} f_n(x) = s(x)\]
una serie convergente in modo uniforme. Naturalmente, per la condizione necessaria della convergenza, si sa che
\[\lim_{n \to +\infty} f_n(x) = 0\]
ovverosia la successione dei termini generali è infinitesima. Siccome serie di partenza converge uniformemente a $s(x)$, è immediato anche evincere che la successione dei termini generali converge a $0$ uniformemente, in quanto è ovvio che
\[f_n(x) = s_n(x) - s_{n-1}(x)\]
e siccome entrambe le somme parziali convergono a $s(x)$ in modo uniforme (per ipotesi), allora anche $f_(x)$ converge a $0$ uniformemente, in quanto differenza di funzioni uniformemente convergenti è anch'essa uniformemente convergente.\\
Pertanto, se la successione dei termini generali, non converge a $0$ in modo uniforme, automaticamente la serie di partenza non può essere uniformemente convergente. Tuttavia, se la successione converge a $0$ in modo uniforme, ciò non è sufficiente ad affermare che la convergenza della serie di partenza sia anch'essa uniforme.

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la seguente serie
\[\sum_{n=0}^{+\infty} x^n = \frac{1}{1-x}\]
in quanto si è richiesto che $\vert x \vert < 1$. Tuttavia, tale convergenza non è uniforme in quanto, ovviamente
\[\lim_{n \to +\infty} x^n = 0\]
in modo puntuale.

\vspace{1em}
\subsection{Criterio di convergenza uniforme per serie di funzioni - M-test di Weierstrass}
Di seguito si espone il fondamentale \textbf{criterio per la convergenza uniforme di una serie di funzioni}, noto come \textbf{M-test di Weierstrass}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{M-TEST DI WEIERSTRASS}}\\
    \parbox{\linewidth}{Sia data
    \[f_n : A \subseteq \mathbb{C} \longmapsto \mathbb{C}\]
    ed esista una successione numerica $(a_n)_n$, tale per cui $a_n \in \mathbb{R}$, con $a_n \geq 0, \forall n$ tale per cui
    \begin{enumerate}
        \item $\forall n \in \mathbb{N}$, anche se sarebbe sufficiente richiederlo definitivamente, si ha che
        \[\vert f_n(z) \vert \leq a_n \hspace{1em} \forall z \in A\]
        \item $\displaystyle{\sum_{n=0}^{+\infty} a_n}$ è convergente
    \end{enumerate}
    allora la serie
    \[\sum_{n=0}^{+\infty} f_n\]
    converge assolutamente (in quanto si lavora sui moduli) e uniformemente.\vspace{3mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\newpage
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Usando il criterio di Cauchy, si dimostri che
\[\forall \epsilon > 0, \exists n_\epsilon \hspace{1em} \text{tale che} \hspace{1em} \forall n \geq n_\epsilon \hspace{1em} \text{e} \hspace{1em} \forall p \in \mathbb{N} \hspace{1em} \text{si ha che} \hspace{1em} \left \vert \sum_{k=n+1}^{n+p} f_k(z) \right \vert < \epsilon\]
Ciò è vero, in quanto
\[\left \vert \sum_{k=n+1}^{n+p} f_k(z) \right \vert \leq \sum_{k=n+1}^{n+p} \left \vert f_k(z) \right \vert \leq \sum_{k=n+1}^{n+p} f_k(z) a_n < \epsilon\]
in quanto la serie
\[\displaystyle{\sum_{n=0}^{+\infty} a_n}\]
è convergente per ipotesi, quindi è di Cauchy, per cui è $< \epsilon$.

\vspace{2em}
\noindent
\textbf{Osservazione}: Si consideri il seguente \textit{vademecum} per la determinazione della convergenza uniforme di una serie di funzioni:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{VADEMECUM PER LA CONVERGENZA UNIFORME DI UNA SERIE}}\\
    \parbox{\linewidth}{Data una serie di funzioni
    \[\sum_{n=0}^{+\infty} f_n(x)\]
    per provare la \textbf{convergenza uniforme}
        \begin{itemize}
            \item si adotta il \textbf{test di Weierstrass};
            \item si adotta la definizione, cercando la maggiorazione con qualcosa;
            \item si impiega il criterio di Cauchy;
        \end{itemize}
    per provare la \textbf{non convergenza uniforme}
        \begin{itemize}
            \item si verifica se $f_n(x)$ converge a $0$ in modo uniforme, perché se non converge uniformemente, automaticamente la serie non converge uniformemente;
            \item si usa la definizione per assurdo, scegliendo oculatamente $x$ in funzione di $n$;
            \item si impiega il criterio di Cauchy, scrivendo
            \[\left \vert \sum_{k=n}^{n+p} f_n(x) \right \vert < \frac{1}{1000}\]
            scegliendo opportunamente $p$ e $x$.
        \end{itemize}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

\vspace{2Em}
\noindent
\textbf{Esercizio 1}: La serie
\[\sum_{n=0}^{+\infty} x^n\]
è uniformemente convergente sull'intervallo $\left] - \dfrac{1}{2}, \dfrac{1}{2}\right[$. Infatti, è sufficiente considerare il test di Weierstrass e osservare che
\[\left \vert x^n \right \vert \leq \left(\frac{1}{2}\right)^n\]

\vspace{1em}
\noindent
\textbf{Esercizio 2}: La serie
\[\sum_{n=0}^{+\infty} \frac{\sin(nx)}{2^n}\]
è uniformemente convergente su $\mathbb{R}$, in quanto, per il test di Weierstrass, si ha che
\[\left \vert \frac{\sin(nx)}{2^n} \right \vert \leq \left(\frac{1}{2}\right)^n\]
per cui è uniformemente convergente.

\vspace{1em}
\noindent
\textbf{Esercizio 3}: La serie
\[\sum_{n=1}^{+\infty} \frac{\sqrt{1-x^{2n}}}{3^n}\]
è uniformemente convergente sull'intervallo $[-1,1]$. Infatti, è sufficiente applicare il test di Weierstrass e osservare che
\[\frac{\sqrt{1-x^{2n}}}{3^n} < \left(\frac{1}{3}\right)^n\]
per cui la serie di partenza converge uniformemente sull'intervallo $[-1,1]$.

\vspace{1em}
\noindent
\textbf{Esercizio 4}: Si consideri la serie seguente
\[\sum_{n=1}^{+\infty} (-1)^n \frac{\arctan(nx)}{n}\]
Si capisce immediatamente che il comportamento della serie dei moduli è identico a quello della serie armonica, che diverge. Tuttavia, la serie presenta un segno alternato, il che implica la possibilità di una convergenza semplice, come nel caso della serie di Leibniz. Ovviamente, se una serie di funzioni non converge assolutamente, non può funzionare il test di Weierstrass.\\
Studiandola per $x>0$ (ma essendo dispari, per $x<0$ la cosa è analoga), si evince che la serie diventa di tipo Leibniz. Il termine generale soddisfa le condizioni di Leibniz, infatti
\begin{itemize}
    \item il termine $a_n$ è positivo, ovvero
    \[\frac{\arctan(nx)}{n} > 0, \hspace{1em} \forall n\]
    \item il termine $a_n$ è infinitesima, in quanto
    \[\lim_{{n \to +\infty}} \frac{\arctan(nx)}{n} = 0\]
    \item il termine $a_n$ è decrescente, in quanto
    \[\dfrac{\dif}{\dif n} \frac{\arctan(nx)}{n} = \dfrac{\dfrac{nx}{1+(nx)^2} - \arctan(nx)}{n^2}\]
    che quindi permette di ottenere
    \[\arctan(nx) > \dfrac{nx}{1+(nx)^2}\]
    per cui per $n$ sufficientemente grande tale disuguaglianza è verificata.
\end{itemize}
ciò permette di affermare che la serie di partenza converge semplicemente e, quindi, puntualmente. Tuttavia Leibniz permette anche di stimare la somma, tramite la formula seguente
\[s_n(x)-s(x) \leq a_n\]
ma se si sostituisce ad $a_n$ il termine $n$-esimo e si esegue una banale maggiorazione, si ottiene
\[s_n(x)-s(x) \leq a_n = \frac{\arctan(nx)}{n} \leq \dfrac{\pi}{2} \cdot \dfrac{1}{n}\]
e siccome
\[\dfrac{\pi}{2} \cdot \dfrac{1}{n}\]
tende a $0$ uniformemente, in quanto non compare più $x$, si conclude che anche la serie di partenza deve convergere uniformemente.

\vspace{2em}
\noindent
\textbf{Osservazione}: Pertanto, se si ha una serie di funzioni di tipo Leibniz che soddisfa il criterio di Leibniz, si ha che la serie converge semplicemente e anche puntualmente. Se, però, tramite la stima della somma, si riesce a maggiorare il termine generale $a_n$ con qualcosa che non dipende da $x$, ma che è infinitesimo, si ottiene la convergenza uniforme cercata.

\vspace{2em}
\noindent
\textbf{Esercizio 5}: Si consideri la serie seguente
\[\sum_{n=1}^{+\infty} \frac{x}{x^2+n^2}\]
e si dimostri che tale serie non può convergere uniformemente su $\mathbb{R}$. Dapprincipio, si può verificare che il termine generale converge uniformemente a $0$. Per farlo si studia la successione di funzioni
\[f_n = \frac{x}{n^2+x^2}\]
e se ne calcola la derivata, ossia
\[f'_n(x) = \frac{n^2 + x^2 - 2x^2}{(n^2+x^2)^2}\]
e si evince facilmente che tale derivata si annulla in $\vert x \vert = n$, per cui $x_0=n$ è punto di massimo. Quindi il massimo della funzione è $f_n$ calcolato in $x=n$, da cui
\[f_n(n)=\frac{n}{n^2+n^2} = \frac{1}{2n} < \epsilon \hspace{1em} \forall \epsilon\]
Quindi il termine generale è infinitesimo uniformemente. Tuttavia, il fatto che la successione dei termini generali converga uniformemente a $0$ non significa che la serie converga uniformemente. Evidentemente, la serie converge puntualmente, in quanto
\[\left \vert \frac{x}{n^2+x^2} \right \vert \leq \left \vert x \right \vert \frac{1}{n}\]
ma tale maggiorazione dipende da $x$. Se la serie convergesse uniformemente, infatti, dovrebbe essere vero che
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \hspace{1em} \text{tale che} \hspace{1em} \forall n \geq n_\epsilon \hspace{1em} \text{e} \hspace{1em} \forall p \in \mathbb{N} \hspace{1em} \text{si ha che} \hspace{1em} \left \vert \sum_{k=n}^{n+p} f_n(x) \right \vert < \epsilon \hspace{1em} \forall x\]
Se fosse vero, ponendo $\epsilon=\dfrac{1}{1000}$, fissano $n \geq n_\epsilon$ e ponendo $p=n$ si dovrebbe avere che
\[\frac{1}{1000} > \sum_{k=n}^{2n} \frac{x}{n^2+x^2} = \frac{x}{n^2+x^2}+\frac{x}{(n+1)^2+x^2}+\frac{x}{(n+2)^2+x^2}+\dots+\frac{x}{(2n)^2+x^2}\]
Ma ovviamente, avendo $p=n$ termini a somma, per ottenere una minorazione, si somma $p=n$ volte il termine più piccolo, ossia l'ultimo, per cui
\[\frac{x}{n^2+x^2}+\frac{x}{(n+1)^2+x^2}+\frac{x}{(n+2)^2+x^2}+\dots+\frac{x}{(2n)^2+x^2} \geq n \cdot \dfrac{x}{(2n)^2+x^2}\]
Non solo, se la convergenza fosse uniforme, tale disuguaglianza dovrebbe essere verificata $\forall x$, per cui anche da $x=n$, ma ciò porta al seguente risultato
\[n \cdot \dfrac{x}{(2n)^2+x^2} = \frac{n^2}{(2n)^2 + n^2} = \frac{n^2}{5n^2} = \frac{1}{5}\]
ma ovviamente tale quantità non è minore dell'$\epsilon$ scelto in partenza, per cui si è ottenuto l'assurdo cercato.

\newpage
\noindent
\begin{center}
    14 Ottobre 2022
\end{center}
Uno dei test più importanti da adottare per dimostrare che una serie di funzioni è convergente uniformemente (e, quindi, assolutamente) è l'\textbf{M-Test di Weierstrass}. Se ciò non funziona, al fine di dimostrare l'uniforme convergenza, si può provare con il criterio di Cauchy. In alternativa si cerca di avere una stima del valore assoluto della differenza $\left \vert f_n - f \right \vert$, tramite la determinazione del massimo oppure con la stima della somma di Leibniz.\\
Similmente, per dimostrare che una serie non è uniformemente convergente si prova a vedere se il termine generale è infinitesimo in modo non uniforme: se così è, la serie di partenza non può convergere uniformemente.\\
Se lo è, la dimostrazione diviene più complessa ed è necessario, a questo stadio, utilizzare ancora Cauchy, andando a dimostrare che non può essere
\[\left \vert \sum_{k=n+1}^{n+p} f_k \right \vert < \epsilon\]
ma scegliendo oculatamente il valore di $p$ e di $x$ in funzione di $n$ opportunamente.

\vspace{1em}
\noindent
\textbf{Osservazione}: Le serie di funzioni sono fondamentali per costruire funzioni più complesse. Infatti, dato uno spazio vettoriale come $\mathbb{R}^n$, è possibile costruire ogni elemento dello spazio come combinazione lineare dei vettori della base. Ciò che è di interesse sono spazi vettoriali di dimensione infinita, i cui elementi sono funzioni, come $C^k(I)$, ossia lo spazio di funzioni derivabili con derivata $k$-esima continua.\\
Si consideri, per esempio, lo spazio vettoriale $C([-\pi,\pi])$ delle funzioni continue sull'intervallo $[-\pi,\pi]$. Allora, tale spazio presenta una dimensione infinita e determinarne una base non è semplice. Pertanto, se si considera
\[X = \{f \in C([-\pi,\pi]), \hspace{0.5em} \text{con} \hspace{0.5em} f \hspace{0.5em} \text{pari}\}\]
come spazio vettoriale. Un esempio di funzione che appartiene a tale spazio è la funzione $\cos(x)$, ma anche $\cos(kx)$, o ancora $\alpha_k \cdot \cos(k x)$. Allora, se si considera la combinazione lineare seguente, del tutto identica ad una somma, parziale
\[\sum_{k=0}^{n} \alpha_k \cdot \cos(kx)\]
in cui
\[\alpha_k = \frac{1}{(2n+1)^2} \hspace{1em} \text{e} \hspace{1em} k=2n+1\]
si ottiene una funzione che presenta come grafico un triangolo isoscele, tanto meglio approssimato quanto più il numero delle ripetizioni $n$ diventa grande. Tuttavia, il grafico di tale somma parziale è un triangolo non derivabile in $x=0$; per ottenere un triangolo perfetto (derivabile in $x=0$) si deve fare tendere $n \to +\infty$, ovverosia si deve considerare una serie.

\newpage
\subsection{Funzione sviluppabile in serie}
Di seguito si esprime il concetto di \textbf{funzione sviluppabile in serie}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{FUNZIONE SVILUPPABILE IN SERIE}}\\
    \parbox{\linewidth}{Preso in considerazione, in generale, l'insieme
    \[\Phi_n = \{\phi_n : n \in \mathbb{N}\}\]
    in cui $(\phi_n)_n$ è una generica successione di funzioni, con
    \[\phi_n : E \subset \mathbb{C} \longmapsto \mathbb{C}\]
    Allora, data una funzione
    \[\phi : E \subset \mathbb{C} \longmapsto \mathbb{C}\]
    si dice che essa è sviluppabile in serie di elementi di $\Phi$ se esiste una successione di funzioni $(\alpha_n)_n$ in $\mathbb{C}$ tale che
    \[\phi(x) = \sum_{n=0}^{+\infty} \alpha_n \cdot \phi_n(x)\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\subsubsection{Funzione sviluppabile in serie di potenze}
La definizione di cui sopra, naturalmente, viene fornita a livello generale, ma ciò che è di interesse è considerare $\Phi$ come un insieme di potenze.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{FUNZIONE SVILUPPABILE IN SERIE DI POTENZE}}\\
    \parbox{\linewidth}{Dato $z_0 \in \mathbb{C}$ e
    \[\Phi = \left\{1,(z-z_0)^n,\hspace{0.5em}\text{con}\hspace{0.5em}n\in \mathbb{N}^+\right\}\]
    Allora, data una funzione
    \[\phi : E \subset \mathbb{C} \longmapsto \mathbb{C}\]
    si dice sviluppabile in serie di potenze di centro $z_0$, se esiste una successione di funzioni $(\alpha_n)_n$ in $\mathbb{C}$ tale che
    \[\phi(x) = \alpha_0 + \sum_{n=1}^{+\infty} \alpha_n \cdot (z-z_0)^n = \sum_{n=0}^{+\infty} \alpha_n \cdot (z-z_0)^n\]
    in cui la prima forma è preferibile in quanto quando $z=z_0$ e $n=0$ si incorre nell'indeterminatezza della notazione $0^0$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio}: Posto $z_0=0$ e $\alpha_n=1, \hspace{0.5em} \forall n$, si consideri la funzione seguente
\[f=1+\sum_{n=1}^{+\infty} z^n\]
Allora appare evidente che la funzione espressa in questo modo, con $\vert z \vert < 1$ è proprio la somma della serie geometria di ragione $z$, per cui
\[f=1+\sum_{n=1}^{+\infty} \alpha_n \cdot z^n = \frac{1}{1-z}\]
ovviamente, quindi, la funzione $f$ è definita solamente quando $\vert z \vert < 1$.\\
Pertanto, la funzione $\dfrac{1}{1-z}$ è sviluppabile in serie di potenze con centro $z_0=0$ solamente nella palla
\[\mathcal{B}(0,1) = \{z \in \mathbb{C} : \vert z \vert < 1\}\]
ossia l'intervallo $]-1,1[$ in $\mathbb{R}$.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri, in $\mathbb{R}$, la serie di potenze
\[\sum_{n=1}^{+\infty} \frac{1}{n} x^n\]
deve essere definita sull'insieme di convergenza $[-1,1[$, chiusa a sinistra, ma aperto a destra, in quanto per $x=1$ è la serie armonica, mentre per $x=-1$ è la serie di Leibniz e quindi converge.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri, in $\mathbb{R}$, la serie di potenze
\[\sum_{n=1}^{+\infty} (-1)^n \frac{1}{n} x^n\]
deve essere definita sull'insieme di convergenza $]-1,1]$, aperto a sinistra, ma chiuso a destra, in quanto per $x=1$ è la serie di Leibniz, mentre per $x=-1$ è la serie armonica.

\vspace{1em}
\noindent
\subsection{Insieme di convergenza}
Di seguito si fornisce la definizione di \textbf{insieme di convergenza}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{INSIEME DI CONVERGENZA}}\\
    \parbox{\linewidth}{Si chiama insieme di convergenzadi una serie l'insieme dei punti dove la serie converge.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri, in $\mathbb{R}$, la serie di potenze
\[\sum_{n=1}^{+\infty} \frac{1}{n^2} x^n\]
deve essere definita sull'insieme di convergenza $[-1,1]$, chiuso sia a sinistra che a destra, in quanto la serie armonica generalizzata di ragione $2$ è convergente.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la serie
\[\sum_{n=1}^{+\infty} n^n \cdot x^n\]
È chiaro che la serie non è convergente; l'unica possibilità è che $x=0$, per cui l'insieme di convergenza è $E=\{0\}$, ossia un intervallo, anche se degenere.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la serie
\[\sum_{n=1}^{+\infty} \dfrac{1}{n^n} \cdot x^n\]
È chiaro che la serie è sempre convergente; pertanto l'insieme di convergenza è $E=\mathbb{R}$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che la serie di partenza
\[\alpha_0 + \sum_{n=1}^{+\infty} \alpha_n \cdot (z-z_0)^n\]
è sempre definita in $z=z_0$ e il valore di convergenza è $\alpha_0$.

\vspace{1em}
\subsection{Teorema di proprietà dell'insieme di convergenza di una serie di potenze}
Negli esempi di cui sopra, \textbf{l'insieme di convergenza} che si andava determinando era \textbf{sempre un intervallo}, in quanto si lavorava in $\mathbb{R}$. Invece, in $\mathbb{C}$, l'insieme di convergenza è sempre un disco. In ogni caso si parla sempre di una palla di centro $z_0$.\\
Di seguito si espone il \textbf{teorema di proprietà dell'insieme di convergenza di una serie di potenze}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{TEOREMA DI PROPRIETÀ DELL'INSIEME DI CONVERGENZA DI UNA SERIE DI POTENZE}}\\
    \parbox{\linewidth}{Si consideri la serie di potenze seguente
    \[\sum_{n=0}^{+\infty} a_n \cdot (z-z_0)^n\]
    \begin{enumerate}
        \item Si supponga che $\exists z_1 \in \mathbb{C}$, tale che
        \[\sum_{n=0}^{+\infty} a_n \cdot (z_1-z_0)^n\]
        converge, allora $\forall z \in \mathbb{C}$, se si verifica che $\left \vert z - z_0 \right \vert < \left \vert z_1 - z_0 \right \vert$, allora la serie di potenze di partenza converge assolutamente al numero $z$, ovvero
        \[\sum_{n=0}^{+\infty} \left \vert a_n \cdot (z-z_0)^n \right \vert\]
        converge.
    
        \item Su ogni palla chiusa compatta $\overline{\mathcal{B}(z_0,r)} \subset \mathcal{B}(z_0,\left \vert z_1-z_0 \right \vert)$ la convergenza è uniforme. Nella palla aperta, invece, si ha convergenza puntuale.
    \end{enumerate}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Per ipotesi è noto che
\[\sum_{n=0}^{+\infty} a_n \cdot (z_1-z_0)^n\]
è convergente. Per la condizione necessaria per la convergenza di una serie, la successione dei termini generale è infinitesima, ossia
\[\lim_{n \to +\infty} a_n \cdot (z_1-z_0)^n = 0\]
Pertanto, siccome la successione 
\[\left(a_n \cdot (z_1 - z_0)^n \right)_n\]
ammette limite, essa è limitata, ovverosia
\[\exists M \in \mathbb{R} \hspace{1em} \text{ tale che } \hspace{1em} \forall n \in \mathbb{N} \hspace{1em} \text{ si ha che } \hspace{1em} \left \vert a_n \cdot (z_1-z_0)^n \right \vert \leq M\]
Considerando, ora, la successione di interesse, ovverosia $(a_n \cdot (z_1-z_0)^n)_n$, moltiplicando e dividendo per la medesima quantità $\vert z_1-z_0 \vert^n$ si ottiene che
\[\left \vert a_n \cdot (z-z_0)^n \right \vert = \vert a_n \vert \cdot \vert z-z_0 \vert^n = \vert a_n \vert \cdot \vert z_1-z_0 \vert^n \cdot \dfrac{\vert z-z_0 \vert^n}{\vert z_1-z_0 \vert^n} \leq M \cdot \left \vert \dfrac{z-z_0}{z_1-z_0} \right \vert^n\]
Ma siccome è noto per ipotesi che $\left \vert z-z_0 \right \vert < \left \vert z_1-z_0 \right \vert$, allora la serie
\[\sum_{n=0}^{+\infty} M \cdot \left \vert \dfrac{z-z_0}{z_1-z_0} \right \vert^n\]
converge in quanto serie geometrica con ragione $k<1$. Per il criterio del confronto, convergerà assolutamente anche la serie di interesse
\[\sum_{n=0}^{+\infty} a_n \cdot (z-z_0)^n\]
pertanto l'insieme di convergenza è sempre un disco in $\mathbb{C}$, o un intervallo in $\mathbb{R}$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Si consideri la palla chiusa e limitata $\overline{\mathcal{B}(z_0,r)} \subset \mathcal{B}(z_0, \vert z_1-z_0 \vert)$.\\
Allora
\[\forall z \in \overline{\mathcal{B}(z_0,r)} \hspace{1em} \text{si ha che} \hspace{1em} \left \vert z - z_0 \right \vert \leq r < \left \vert z_1 -z_0 \right \vert\]
Dividendo la disuguaglianza di cui sopra per $\left \vert z_1 - z_0 \right \vert$ si ottiene che
\[\forall z \in \overline{\mathcal{B}(z_0,r)} \hspace{1em} \text{si ha che} \hspace{1em} \left \vert \frac{\left \vert z - z_0 \right \vert}{\left \vert z_1 - z_0 \right \vert} \right \vert \leq \frac{r}{\left \vert z_1 - z_0 \right \vert} < 1\]
in cui è fondamentale osservare come
\[\frac{r}{\left \vert z_1 - z_0 \right \vert}\]
non dipende più da $z$. Per quanto già esposto nella dimostrazione precedente, vale la seguente catena di disuguaglianze
\[\left \vert a_n \cdot (z-z_0)^n \right \vert \leq M \cdot \left \vert \frac{z-z_0}{z_1-z_0}\right \vert^n \leq M \cdot \left \vert \frac{r}{z_1 - z_0} \right \vert ^n\]
Se si pone
\[K = \left \vert \frac{r}{z_1-z_0} \right \vert < 1\]
si ottiene che
\[\left \vert a_n \cdot (z-z_0)^n \right \vert \leq M \cdot \left \vert \frac{z-z_0}{z_1-z_0} \right \vert \leq M \cdot K^n\]
Se ora si considera la successione $(b_n)_n$ dove $b_n=M K^n$, si è già dimostrato in precedenza che
\[\left \vert a_n \cdot (z-z_0)^n \right \vert \leq b_n \hspace{1em} \forall z \in \mathbb{C} \hspace{1em} \text{e} \hspace{1em} \forall n \in \mathbb{N}\]
Data, quindi, la serie numerica seguente
\[\sum_{n=0}^{+\infty} M K^n\]
convergente in quanto serie geometrica di ragione $K<1$, ne segue, per l'M-Test di Weierstrass che la serie
\[\sum_{n=0}^{+\infty} a_n \cdot (z-z_0)^n\]
converge uniformemente.

\newpage
\noindent
\subsection{Raggio di convergenza}
Si consideri la definizione di \textbf{raggio di convergenza} seguente:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{RAGGIO DI CONVERGENZA}}\\
    \parbox{\linewidth}{Sia 
    \[a_0 + \sum_{n=1}^{+\infty} a_n \cdot (z-z_0)^n\]
    una serie di potenze. Sia $E$ l'insieme di convergenza della serie. Se $E = \{z_0\}$ si pone $\rho=0$; se $E = \mathbb{C}$ si pone $\rho = +\infty$; altrimenti si pone
    \[\rho = \sup \{\left \vert z-z_0 \right \vert \in \mathbb{R} : z \in E\}\]
    in cui $\rho$ si dice \textbf{raggio di convergenza}.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\subsubsection{Proprietà caratteristiche del raggio di convergenza}
Si espongono di seguito le \textbf{proprietà caratteristiche del raggio di convergenza}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{RAGGIO DI CONVERGENZA}}\\
    \parbox{\linewidth}{Sia 
    \[a_0 + \sum_{n=1}^{+\infty} a_n \cdot (z-z_0)^n\]
    una serie di potenze. Sia $E$ l'insieme di convergenza della serie, posto $E \neq \{z_0\}$ e $E \neq \mathbb{C}$.\\
    Allora $\rho \in \mathbb{R}^+$ è il raggio di convergenza della serie \textbf{se e solo se}
    \begin{enumerate}
        \item $\forall z \in \mathbb{C}$, se $\left \vert z-z_0 \right \vert < \rho$, allora $z \in E$, ossia la serie converge in $z$;
        \item $\forall z \in \mathbb{C}$, se $\left \vert z-z_0 \right \vert > \rho$, allora $z \notin E$, ossia la serie non converge.
    \end{enumerate}
    \vspace{3mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Sia $r$ il raggio di convergenza, ossia
\[r = \sup \{\left \vert z - z_0 \right \vert : z \in E\}\]
per cui vale la $2$ proprietà; infatti, se $\left \vert z - z_0 \right \vert > r$ allora $z \notin E$: se $r$ è l'estremo superiore di tutte le distanze per cui si ha convergenza, presa una distanza maggiore di $r$ non si può avere convergenza.\\
Non solo, ma vale anche la $1$ proprietà: infatti, sia $\left \vert z - z_0 \right \vert < r$, allora esiste $z_1 \in E$ tale che
\[\left \vert z - z_0 \right \vert < \left \vert z_1 - z_0 \right \vert < r\]
essendo $r$ l'estremo superiore. Ma allora, per il teorema di proprietà dell'insieme di convergenza precedentemente esposto si ha che la serie converge in $z$ e, quindi, $z \in E$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Sia $\rho$ che verifica le proprietà $1$ e la proprietà $2$.\\
Allora la prima proprietà assicura che $\rho$ sia un maggiorante dell'insieme
\[\{\left \vert z - z_0 \right \vert : z \in E\}\]
Per dimostrare che è il minimo fra i maggioranti, si ponga $\rho_1 < p$, per cui esiste $z_1$ tale che
\[\rho_1 < \left \vert z_1 - z_0 \right \vert < \rho\]
tale per cui $z_1 \in E$, in quanto $\left \vert z_1 - z_0 \right \vert < \rho$. Quindi $\rho_1$ non può essere un maggiorante di $\{\left \vert z - z_0 \right \vert : z \in E\}$, in quanto $\left \vert z_1 - z_0 \right \vert$ è maggiore di $\rho_1$. Tale considerazione significa che
\[\rho = \sup \{\left \vert z - z_0 \right \vert : z \in E\}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Lavorando in $\mathbb{R}$, si osservi che se $E$ è un insieme di convergenza e si pone
\[I = \left ] x_0 - \rho, x_0 + \rho \right[\]
come \textbf{intervallo di convergenza}. Allora, in generale è vero che
\[I \subseteq E \subseteq \overline{I}\]
dove $\overline{I}$ è la chiusura di $I$. Ciò perché sul bordo dell'intervallo di convergenza non è sempre garantita la convergenza. Per esempio, se $E=]-1,1]$, allora $I=]-1,1[$, mentre $\overline{I}=[-1,1]$, per cui è immediata la disuguaglianza insiemistica di cui sopra.

\vspace{1em}
\subsection{Proprietà delle serie di potenze}
Di seguito si espongono le \textbf{proprietà delle serie di potenze}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PROPRIETÀ DELLE SERIE DI POTENZE}}\\
    \parbox{\linewidth}{Sia $f : I \longmapsto \mathbb{R}$ in cui $I$ è l'intervallo di convergenza, tale che
    \[I = \left]x_0-\rho,x_0+\rho\right[ \hspace{1em} \text{oppure} \hspace{1em} I = \mathbb{R}\] 
    escludendo il caso in cui $\rho=0$. Posto
    \[f(x) = a_0 + \sum_{n=1}^{+\infty} a_n \cdot (x-x_0)^n\]
    ovverosia
    \[f(x) = a_0 + a_1 (x-x_0) + a_2 (x-x_0)^2 + \dots\]
    si ha che
    \begin{enumerate}
        \item $\forall x \in I$ si ha che
        \[\int_{x_0}^x f(t) \dif t = \sum_{n=0}^{+\infty} \frac{a_n}{n+1} \cdot (x-x_0)^{n+1}\]
        
        \item Posto $f \in C^{\infty}(I)$, ossia è derivabile infinite volte e, in particolare, $f$ è continua, allora si ha che
        \begin{itemize}
            \item $\displaystyle{f'(x) = a_1 + \sum_{n=2}^{+\infty} n \cdot a_n (x-x_0)^{n-1}}$
            \item $\displaystyle{f''(x) = 2a_2 + \sum_{n=3}^{+\infty} n (n-1) \cdot a_n (x-x_0)^{n-2}}$
            \item $\dots$
            \item $\displaystyle{f^{(k)}(x) = k! \cdot a_k + \sum_{n=k+1}^{+\infty} n (n-1) \dots (n-k+1) \cdot a_n (x-x_0)^{n-k}}$
        \end{itemize}
    \end{enumerate}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: La dimostrazione sfrutta il fatto che si ha convergenza uniforme non su $I$, ma sugli intervalli compatti contenuti nell'intervallo $I$, come esposto dal teorema sulle proprietà dell'insieme di convergenza.\\
Sia, allora, $x \in I$ e si consideri $[a,b] \subset I$ tale che $x,x_0 \in [a,b]$: ciò si può sempre fare perché $x \in I$ che è un intervallo aperto, della forma $I = \left]x_0-\rho,x_0+\rho\right[$, per cui non è mai un estremo. Allora, per il teorema sulle proprietà dell'insieme di convergenza, si ha che la serie
\[a_0+ \sum a_n (x-x_0)^n\]
converge uniformemente su $[a,b]$, ovvero
\[s_n(x) \to s(x)\]
uniformemente. Ma allora, per il teorema sull'integrale del limite uniforme di una successione, si ha che $s(x)$ è integrabile e, in particolare,
\[\int_{x_0}^x s(t) \dif t = \lim_{n \to +\infty} \int_{x_0}^x s_n(t) \dif t\]
ma quindi, siccome la somma della serie di potenze è proprio la funzione $f$ cercata, è immediato evincere che
\[\int_{x_0}^x f(t) \dif t = \lim_{n \to +\infty} \int_{x_0}^x \left(a_0 + \sum_{k=1}^n a_k \cdot (x-x_0)^k\right) \dif t\]
ovvero
\[\lim_{n \to +\infty} \left[a_0 \cdot (x-x_0) + \sum_{k=1}^n a_k \cdot \frac{1}{k+1} (x-x_0)^{k+1}\right]\]
ma tale limite è proprio
\[\sum_{n=0}^{+\infty} \dfrac{a_n}{n+1} \cdot (x-x_0)^{n+1}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: La continuità di $f$ è immediata con un ragionamento analogo: lavorando sempre sui compatti contenuti in $I$, la convergenza uniforme implica la continuità del limite.\\
Pertanto, è fondamentale capire che bisogna ragionare sui compatti contenuti nell'intervallo di convergenza. Ciò, però, non garantisce che la funzione sia continua sul bordo, anche se questo risulta comunque vero per il \textbf{lemma di Abel}.

\vspace{1em}
\subsection{Derivabilità delle serie di potenze}
Si consideri la ridotta $s_n$
\[s_n(x) = a_0 + \sum_{k=1}^n a_k \cdot (x-x_0)^k\]
che è la ridotta di una serie di potenze. Calcolandone la derivata si ottiene
\[s_n'(x) = a_1 + \sum_{k=2}^{n} k \cdot a_k \cdot (x-x_0)^{k-1}\]
in questo modo si ottiene una nuova serie di potenze, nella forma
\[a_1 + \sum_{k=2}^{+\infty} k \cdot a_k \cdot (x-x_0)^{k-1}\]
Tuttavia, non è possibile affermare a priori che $s'_n(x)$ è la derivata di $s_n$. Nonostante ciò, il teorema sulla derivata del limite dice che se la successione delle derivate è convergente in modo uniforme a una funzione, allora tale funzione è la derivata della funzione a cui converge puntualmente la funzione di partenza.\\
È noto, però, che la successioni delle derivate è anch'essa una serie di potenze, che è noto convergere uniformemente su ogni sotto-intervallo compatto del suo intervallo di convergenza che, a priori, potrebbe essere diverso dall'intervallo di convergenza della serie originaria.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{DERIVABILITÀ DELLE SERIE DI POTENZE}}\\
    \parbox{\linewidth}{Quindi, detto $I'$ l'intervallo di convergenza della serie
    \[a_1 + \sum_{k=2}^{+\infty} k \cdot a_k \cdot (x-x_0)^{k-1}\]
    si ha che in ogni compatto $[a,b] \subset I' \cap I$ (in cui è fondamentale chiedere $I' \cap I$ in quanto devono essere definite entrambe le serie), la serie delle derivate è la derivata della serie.\vspace{3mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Per concludere la dimostrazione, si deve provare che $I'=I$, ossia che $\rho'=\rho$, ossia i due raggi di convergenza coincidono.\\
Siano, allora
\begin{itemize}
    \item $\rho$ il raggio di convergenza della serie $f(x)$
    \item $\rho'$ il raggio di convergenza della serie delle derivate
    \[a_1 + \sum_{n=2}^{+\infty} a_n \cdot n \cdot (x-x_0)^{n-1}\]
\end{itemize}
Si provi che $\rho=\rho'$. Sia $x$ tale che $\left \vert x - x_0 \right \vert < \rho$ e si provi che la serie delle derivate
\[\sum_{n=2}^{+\infty} a_n \cdot n \cdot (x-x_0)^{n-1}\]
converge. Se si riesce a dimostrare ciò, significa che se la serie di partenza converge (in quanto $\left \vert x - x_0 \right \vert < \rho$) allora converge anche la serie delle derivate.\\
Se, invece, si sceglie $x$ tale che $\left \vert x - x_0 \right \vert > \rho$ e si prova che la serie delle derivate
\[\sum_{n=2}^{+\infty} a_n \cdot n \cdot (x-x_0)^{n-1}\]
non converge si ha concluso, in quanto, per le proprietà caratteristiche del raggio di convergenza, $\rho$ che soddisfa le duw condizioni precedenti deve essere uguale a $\rho'$, in quanto raggio di convergenza anche della serie delle derivate.\\
Volendo confrontare la serie di partenza con la serie delle derivate, si osserva dapprincipio che
\[\sum_{n=2}^{+\infty} a_n (x-x_0)^n = (x-x_0) \sum_{n=2}^{+\infty} a_n (x-x_0)^{n-1}\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Sia $\left \vert x - x_0 \right \vert < \rho$. Esiste, allora, $x_1$ tale che
\[\left \vert x - x_0 \right \vert < \left \vert x_1 - x_0 \right \vert < \rho\]
Allora la serie
\[\sum_{n=2}^{+\infty} \left \vert a_n \cdot (x_1 - x_0)^{n-1} \right \vert\]
converge, in quanto $x_1$ sta dentro all'intervallo di convergenza della serie di partenza (che sarebbe quella con $x-x_0$ portato fuori dalla sommatoria, ma essendo costante si trascura). Quindi la successione $\left \vert a_n \cdot (x-x_0)^n \right \vert$ è limitata, per cui $\exists M$ tale che
\[\left \vert a_n \cdot (x_1-x_0)^n \right \vert < M, \forall n\]
Pertanto, preso ora il termine generale della serie delle derivate $\left \vert n \cdot a_n \cdot (x-x_0)^{n-1} \right \vert$, moltiplicando e dividendo per $\vert x_1-x_0 \vert^n$, si ottiene 
\[\left \vert n \cdot a_n \cdot (x-x_0)^{n-1} \right \vert = \left \vert a_n \cdot (x_1-x_0)^n \right \vert \cdot n \cdot \left \vert \frac{x-x_0}{x_1-x_0} \right \vert^{n-1} < M \cdot n \cdot K^{n-1}\]
posto
\[K=\left \vert \frac{x-x_0}{x_1-x_0} \right \vert < 1\]
che è minore di $1$ in quanto si è scelto $\left \vert x - x_0 \right \vert < \left \vert x_1 - x_0 \right \vert < \rho$. È facile capire, ora, che la serie
\[\sum{n \cdot K^{n-1}}\]
converge, per il criterio del rapporto
\[\frac{(n+1) \cdot K^n}{n \cdot K^{n-1}} \rightarrow K < 1\]
per cui, per il criterio del confronto, la serie
\[\sum n \cdot a_n \cdot (x-x_0)^{n-1}\]
converge. Quindi, dove converge la serie di partenza, anche la serie delle derivate è convergente.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Sia, allora, $x$ tale che $\left \vert x - x_0 \right \vert > \rho$. Ciò significa che la serie
\[\sum \left \vert a_n \cdot (x-x_0)^{n-1}\right \vert\]
non converge. Ma allora è facile osservare che
\[\left \vert n \cdot a_n \cdot (x-x_0)^{n-1} \right \vert \geq \left \vert a_n \cdot (x-x_0)^{n-1} \right \vert\]
quindi la serie delle derivate
\[\sum \left \vert n \cdot a_n \cdot (x-x_0)^{n-1} \right \vert\]
non converge per il criterio del confronto.

\vspace{2em}
\noindent
\textbf{Osservazione 1}: Si conclude, quindi, che
\[\frac{\dif}{\dif x} \left(a_0 + \sum_{n=1}^{+\infty} a_n \cdot (x-x_0)^n\right) = a_1 + \sum_{n=2}^{+\infty} n \cdot a_n \cdot (x-x_0)^{n-1}\]

\vspace{1em}
\noindent
\textbf{Osservazione 2}: Si consideri la funzione seguente, sviluppata in serie di potenze
\[f(x) = a_0 + \sum_{n=1}^{+\infty} a_n \cdot (x-x_0)^n\]
allora
\begin{itemize}
    \item $f(x_0) = a_0$
    \item $f'(x_0) = a_1$
    \item $f''(x_0)  = 2a_2$
    \item $\dots$
    \item $f^{(k)}(x_0) = k! \cdot a_k $
\end{itemize}
Pertanto sono stati ottenuti tutti i coefficienti delle derivate di ogni ordine, per cui
\[a_n =\frac{f^{(n)}(x_0)}{n!}\]
Ciò permette di riscrivere la serie di potenze di partenza come segue
\[f(x) = a_0+\sum_{n=1}^{+\infty} a_n \cdot (x-x_0)^n = f(x_0) + \sum_{n=1}^{+\infty} \frac{f^{(n)}(x_0)}{n!} (x-x_0)^n\]
che prende il nome di \textbf{serie di Taylor}.

\newpage
\noindent
\textbf{Osservazione 3}: Se la $f$ è sviluppabile in serie di potenze, allora
\[f(x) = \lim_{n \to +\infty} P_n(x)\]
in ui $P_n(x)$ è il polinomio di Taylor di grado $n$ di centro $x_0$:
\[P_n(x) = f(x_0) + \sum_{k=1}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k\]
cioè
\[\lim_{n \to +\infty} \left(f(x) - P_n(x)\right) = 0\]
in cui $E_n(x) = f(x)-P_n(x)$ è proprio l'errore di approssimazione. Pertanto, una funzione è sviluppabile in serie di potenze \textbf{se e solo se}
\[\lim_{n \to +\infty} \left(f(x) - P_n(x)\right) = 0\]

\vspace{2em}
\noindent
\textbf{Osservazione 4}: Una funzione si dice \textbf{analitica} se tale funzione è sviluppabile in serie di potenze, con
\[f : I \longmapsto \mathbb{R}\]
Naturalmente, se $f$ è analitica, allora $f$ è $C^\infty(I)$, ovvero deve essere derivabile infinite volte sull'intervallo. Tuttavia, in $\mathbb{R}$, se una funzione è $C^\infty$, non è detto che sia analitica (mentre nel campo $\mathbb{C}$ ciò è vero). Un esempio chiave è la funzione seguente:
\[f(x) = \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{lll}
        0 & \text{se} & x=0\\
        e^{-\frac{1}{x^2}} & \text{se} & x \neq 0
    \end{array}
\right.\]
Calcolandone la derivata si ottiene
\[f'(x) = \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{lll}
        0 & \text{se} & x=0\\
        e^{-\frac{1}{x^2}} \cdot (2x^{-3}) & \text{se} & x \neq 0
    \end{array}
\right.\]
che permette di evincere che la funzione è $C^\infty$, ovvero tutte le derivate esistono e sono $0$, cioè $f^{(k)}(0)=0$. Tuttavia la serie di Taylor di $f$ è
\[f(0) + \sum_{n=1}^{+\infty} \frac{f^{(n)}(0)}{n!} x^n = 0\]
e quindi non coincide con la funzione stessa, ovvero
\[f(x) \neq \sum_{n=1}^{+\infty} \frac{f^{(n)}(0)}{n!} x^n\]
Questo è un esempio in cui la serie di Taylor esiste, ma la funzione non è sviluppabile in serie di Taylor, in quanto essa non coincide con la funzione stessa. Pertanto $f$ non è analitica.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che un modo per decretare l'analiticità di una funzione è calcolare
\[\lim_{n \to +\infty} f(x) - P_n(x) = \lim_{n \to +\infty} E_n(x) = 0\]
Per descrivere il resto della formula di Taylor si può utilizzare il resto nella formula di Lagrange:
\[E_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \cdot (x-x_0)^{n+1}\]
con $\left \vert \xi - x_0 \right \vert < \left \vert x - x_0 \right \vert$.

\vspace{1em}
\noindent
\subsection{Primo criterio di sviluppabilità}
Di seguito si espone il \textbf{primo criterio di sviluppabilità} di una funzione in serie di potenze (\textbf{un'altro criterio è negli esercizi}):

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PRIMO CRITERIO DI SVILUPPABILITÀ}}\\
    \parbox{\linewidth}{Sia $f \in C^\infty(I)$ ed esista $M \in \mathbb{R}$ tale che $\left \vert f^{(n)}(x) \right \vert \leq M^n$, $\forall n \in \mathbb{N}$ e $\forall x \in I$. Allora $f$ è analitica, ossia sviluppabile in serie di potenze.\vspace{3mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: È sufficiente dimostrare che il resto nella forma di Lagrange tende a $0$. Pertanto si ha che, fissato $x \in I$
\[\left \vert E_n(x) \right \vert = \left \vert \frac{f^{(n+1)(\xi)}}{(n+1)!} (x-x_0)^{n+1} \right \vert \leq \frac{M^{n+1}}{(n+1)!} \cdot \left \vert x - x_0 \right \vert^{n+1} = \dfrac{K^{n+1}}{(n+1)!}\]
in cui si è posto
\[K = M \cdot \left \vert x-x_0 \right \vert\]
È facile capire che
\[\lim_{n \to +\infty} \dfrac{K^n}{n!} = 0\]
in quanto è sufficiente considerare tale rapporto come termine generale di una serie convergente, usando il criterio del rapporto:
\[\lim_{n \to +\infty} \dfrac{K^{n+1}}{(n+1)!} \cdot \dfrac{n!}{K^n} = \lim_{n \to +\infty} \dfrac{K \cdot K^{n}}{(n+1) \cdot n!} \cdot \dfrac{n!}{K^n} = \lim_{n\to +\infty} \dfrac{K}{n+1} = 0\]
essendo $K$ costante. Convergendo la serie, la successione dei termini generali è infinitesima.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la funzione $\cos(x)$, allora la serie di Taylor corrispondente è
\[\cos(x) = \sum_{n=0}^{+\infty} (-1)^n \cdot \frac{x^{2n}}{(2n)!}\]
e analogamente per il $\sin(x)$ si ottiene
\[\sin(x) = \sum_{n=0}^{+\infty} (-1)^n \cdot \frac{x^{2n+1}}{(2n+1)!}\]
Per verificare, tuttavia, che tali funzioni sono sviluppabili come serie di potenze, è sufficiente dimostrare che
\[\vert f^{(n)}(x) \vert \leq M^n\]
con $f=\cos(x)$ e $f=\sin(x)$. Ma è chiaro che seno e coseno sono funzioni limitate, posto $M=1$.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la serie di Taylor di $e^x$ seguente
\[e^x=1+\sum_{n=1}^{+\infty} \frac{x^n}{n!}\]
Tuttavia, in questo caso, non è vero che
\[\left \vert e^x \right \vert \leq M^n, \hspace{1em} \forall n \hspace{1em} \text{e} \hspace{1em} \forall x\]
Tuttavia, è possibile restringere $e^x$ nell'intervallo $[-b,b]$ in cui
\[\left \vert f^{(n)}(x) \right \vert = e^x \leq e^b \leq (e^b)^n = M^n\]
e preso tale intervallo arbitrariamente grande, è possibile considerare tutto $\mathbb{R}$, per cui $e^x$ è analitica, ossia sviluppabile come serie di potenze.

\newpage
\begin{center}
    17 Ottobre 2022
\end{center}
Ovviamente, non è vero che ogni funzione può essere sviluppata come serie di potenze. In particolare, non è sufficiente che una funzione sia $C^{\infty}$ (tuttavia ogni funzione sviluppabile come serie di potenze è $C^\infty$); una funzione sviluppabile come serie di potenze presenta un insieme di convergenza che è sempre un intervallo in $\mathbb{R}$ (o un disco in $\mathbb{C}$). Il primo criterio fondamentale da tenere in considerazione si basa sul controllo della derivata della funzione tramite una potenza $M^n$.

\vspace{1em}
\noindent
\textbf{Esempio 1}: Per quanto già esposto, è noto che la funzione $e^x$ è sviluppabile in serie di potenze come segue
\[e^x=\sum_{n=0}^{+\infty} \dfrac{x^n}{n!}\]
Similmente, la funzione $e^{-x}$ può essere sviluppata come segue
\[e^{-x} = \sum_{n=0}^{+\infty} \dfrac{(-x)^n}{n!} = \sum_{n=0}^{+\infty} \frac{(-1)^n}{n!} x^n\]
Se, ovviamente, si considera il seno iperbolico e il coseno iperbolico, ossia delle combinazioni lineari di $e^x$ e $e^-x$, è facile capire che essi sono sviluppabili come serie di potenze.
\[\sinh(x) = \dfrac{e^x-e^{-x}}{2} \hspace{1em} \text{e} \hspace{1em} \cosh(x) = \dfrac{e^x+e^{-x}}{2}\]

\vspace{1em}
\noindent
\textbf{Esempio 2}: Se si considera la funzione $f(x)=\log(1+x)$, si può considerare la sua derivata prima
\[f'(x)=\frac{1}{1+x}\]
Tale derivata può essere riscritta in modo furbo in modo tale da ottenere la somma della serie geometrica (nell'ipotesi in cui $\vert x \vert < 1$):
\[f'(x)=\frac{1}{1-(-x)}\]
Da ciò si evince che
\[f'(x)=\frac{1}{1-(-x)}=\sum_{n=0}^{+\infty} (-x)^n=\sum_{n=0}^{+\infty} (-1)^n \cdot x^n\]
In questo modo si è ottenuto lo sviluppo come serie di potenze della derivata della funzione di partenza. Basterà, ora, integrare per ottenere lo sviluppo funzione di partenza
\[\int_0^x f'(t) \dif t = \int_0^x \sum_{n=0}^{+\infty} (-1)^n \cdot t^n \dif t\]
per le proprietà delle serie di potenze è possibile portare l'integrale all'interno della somma, ottenendo
\[\int_0^x \sum_{n=0}^{+\infty} (-1)^n \cdot t^n \dif t = \sum_{n=0}^{+\infty} (-1)^n \cdot \int_0^x t^n \dif t = \sum_{n=0}^{+\infty} (-1)^n \cdot \frac{1}{n+1} \cdot x^{n+1}\]
ottenendo così lo sviluppo di $\log(1+x)$, che è valido solamente quando $\vert x \vert < 1$, per cui l'insieme di convergenza della serie di potenze è proprio $E=]-1,1[$. Ovviamente, lo sviluppo appena ottenuto può essere scritto in forma analoga come segue:
\[\log(1+x)=\sum_{n=0}^{+\infty} (-1)^n \cdot \frac{1}{n+1} \cdot x^{n+1}=\sum_{n=1}^{+\infty} (-1)^{n+1} \frac{1}{n} x^n\]
Analogamente per $\log(1-x)$ si ottiene
\[\log(1-x) = \sum_{n=1}^{+\infty} (-1)^{n+1} \frac{1}{n} (-1)^n x^n = - \sum_{n=1}^{+\infty} \dfrac{1}{n} x^n\]
rispettando sempre la condizione $\left \vert x \right \vert < 1$.

\vspace{2em}
\noindent
\textbf{Esempio 3}: Si consideri il caso seguente, molto importante per le applicazione pratiche. Sia data la serie seguente:
\[\sum_{n=1}^{+\infty} n x^n\]
Per calcolare il raggio di convergenza di tale serie è molto utile impiegare il criterio del rapporto. Pertanto si scrive
\[\lim_{n \to +\infty} \dfrac{\left \vert f_{n+1}(x) \right \vert}{\left \vert f_n(x) \right \vert} = \lim_{n \to +\infty} \dfrac{\left \vert (n+1) x^{n+1} \right \vert}{\left \vert n x^n \right \vert} = \vert x \vert\]
Per il criterio del rapporto, tale limite deve essere minore di $1$ al fine di avere convergenza; se è maggiore a $1$ diverge. Ma in questo modo si è implicitamente descritta la caratterizzazione del raggio di convergenza. Pertanto l'insieme di convergenza cercato è $\vert x \vert <1$. Fondamentale considerare il \textbf{valore assoluto}.

\vspace{2em}
\noindent
\textbf{Esempio 4}: Si consideri un altro esempio simile al precedente. Sia data la serie seguente:
\[\sum_{n=1}^{+\infty} 2^{n} \cdot \dfrac{(x-1)^{2n}}{n^3}\]
Ancora una volta, per determinare l'insieme di convergenza, si impiega il criterio del rapporto, ottenendo
\[\lim_{n \to +\infty} \dfrac{\left \vert f_{n+1}(x) \right \vert}{\left \vert f_n(x) \right \vert} = \lim_{n \to +\infty} 2^{n+1} \cdot \left \vert \dfrac{(x-1)^{2n+2}}{(n+1)^3} \right \vert \cdot \left \vert \dfrac{n^3}{(x-1)^{2n}} \right \vert \cdot \dfrac{1}{2^n} = 2 \cdot (x-1)^2\]
Per il criterio del rapporto, tale limite deve essere minore di $1$ al fine di avere convergenza; se è maggiore a $1$ diverge. Ma in questo modo si è implicitamente descritta la caratterizzazione del raggio di convergenza. Pertanto l'insieme di convergenza cercato è
\[(x-1)^2 < \dfrac{1}{2} \hspace{1em} \rightarrow \hspace{1em} \vert x-1 \vert < \sqrt{\dfrac{1}{2}}\]
L'intervallo di convergenza è, dunque
\[I=\left]1-\dfrac{1}{\sqrt{2}},1+\dfrac{1}{\sqrt{2}}\right[\]
e bisogna, ovviamente, controllare singolarmente ogni estremo per verificare la convergenza o meno (si vede facilmente che l'insieme di convergenza è l'intervallo chiuso, in quanto sostituendo a $x$ il valore degli estremi si ottiene una armonica generalizzata di ragione $3$), per cui
\[E=\left[1-\dfrac{1}{\sqrt{2}},1+\dfrac{1}{\sqrt{2}}\right]\]

\vspace{1em}
\noindent
\textbf{Esempio 4}: Si consideri la serie seguente
\[\sum_{n=0}^{+\infty} n x^n\]
Allora appare evidente che per avere la convergenza si deve imporre $\vert x \vert < 1$, con estremi esclusi. Per determinare la somma è fondamentale ricondursi ad una somma nota, ossia quella della serie geometrica. Infatti
\[\frac{1}{1-x} = 1 + \sum_{n=1}^{+\infty} x^n \hspace{1em} \text{se} \hspace{1em} \vert x \vert < 1\]
allora, derivando tale funzione si ottiene
\[D \left(\frac{1}{1-x}\right) = D \left(1 + \sum_{n=1}^{+\infty} x^n\right) \hspace{1em} \rightarrow \hspace{1em} \frac{1}{(1-x)^2}= \sum_{n=1}^{+\infty} n \cdot x^{n-1} \]
Da notare che è stato necessario estrapolare l'$1$, in quanto derivando esso diviene $0$.\\
Si può, ora, moltiplicare per la medesima quantità (ovviamente non $n$ che è l'indice di sommatoria, in costante incremento, ma un qualcosa che dipende da $x$), ottenendo
\[\frac{x}{(1-x)^2} = \sum_{n=1}^{+\infty} n x^n\]
ed ecco che, quindi, si è ottenuta la somma cercata.

\vspace{2em}
\noindent
\textbf{Esempio 5}: Si calcoli la somma delle seguente serie
\[\sum_{n=1}^{+\infty} \frac{n^2}{2^n}\]
Allora, sfruttando la teoria della serie di potenze, si può considerare la serie
\[\sum_{n=1}^{+\infty} n^2 x^n\]
in cui basta imporre $x=\dfrac{1}{2}$. Ora è molto semplice procedere al calcolo della somma della serie. Dall'esercizio precedente, è nota la uguaglianza seguente:
\[\sum_{n=1}^{+\infty} n x^n = \frac{x}{(1-x)^2}\]
Per ottenere, però, $n^2$, si procede a derivare nuovamente ambo i membri:
\[D \left(\frac{x}{(1-x)^2}\right) = D \left(\sum_{n=1}^{+\infty} n x^n \right) \hspace{1em} \rightarrow \hspace{1em} \dfrac{(1-x) \cdot (1+x)}{(1-x)^4} = \sum_{n=1}^{+\infty} n^2 x^{n-1}\]
per cui, per ottenere $n$ come esponente del termine generale della serie, si moltiplica per $x$ ambo i membri, da cui
\[x \cdot \dfrac{(1-x) \cdot (1+x)}{(1-x)^4} = \sum_{n=1}^{+\infty} n^2 x^n\]
ponendo $x=\dfrac{1}{2}$ si ottiene come somma $6$.

\newpage
\noindent
\subsection{Serie binomiale}
Posto $\alpha \in \mathbb{R}$ si ottiene che
\[(1+x)^\alpha = \sum_{n=0}^{+\infty} \binom{\alpha}{n} \cdot x^n\]
in cui il \textbf{coefficiente binomiale generalizzato} si definisce come segue:
\[\binom{\alpha}{n} = \frac{\alpha \cdot (\alpha-1) \cdot \dots \cdot (\alpha-n+1)}{n!}\]
Tale risultato è tale, in quanto
\begin{align*}
    & f'(x) = \alpha \cdot (1+x)^{\alpha-1}\\
    & f''(x) = \alpha \cdot (\alpha-1) \cdot (1+x)^{\alpha-2}\\
    & \dots\\
    & f^{(k)}(x) = \alpha \cdot (\alpha - 1) \cdot \dots \cdot (\alpha - k + 1) \cdot (1+x)^{\alpha-k}
\end{align*}
Tale serie risulta importante in quanto è possibile assegnare ad $\alpha$ un qualsiasi valore reale. Ovviamente, se $\alpha = m \in \mathbb{N}$ si ottiene il binomio di Newton, da cui
\[\binom{m}{n} = \frac{m \cdot (m-1) \cdot (m-n+1)}{n!}\]
in cui se $m=n$ si ottiene come risultato $1$, da cui
\[(1+x)^m = \sum_{n=0}^m \binom{m}{n} \cdot x^n\]
Se, invece, si pone $\alpha=\dfrac{1}{2}$ si ottiene lo sviluppo in serie della radice quadrata di $1+x$, ovvero
\[\sqrt{1+x}=\sum_{n=0}^{+\infty} \binom{\dfrac{1}{2}}{n} \cdot x^n\]
e per $\alpha=-\dfrac{1}{2}$ si ottiene lo sviluppo del reciproco della radice quadrata di $1+x$, ovvero
\[\frac{1}{\sqrt{1+x}} = \sum_{n=0}^{+\infty} \binom{-\dfrac{1}{2}}{n} \cdot x^n\]

\vspace{1em}
\noindent
\textbf{Osservazione}: L'ultimo risultato è particolarmente importante in quanto permette di ottenere lo sviluppo in serie di potenze dell'$\arcsin(x)$. In particolare, infatti, si ha che
\[\frac{1}{\sqrt{1+x}} = \sum_{n=0}^{+\infty} \binom{-\dfrac{1}{2}}{n} \cdot x^n\]
per cui considerando $x^2$ al posto di $x$, si ottiene la derivata dell'$\arcsin(x)$
\[\dfrac{\dif}{\dif x} \arcsin(x) = \frac{1}{\sqrt{1-x^2}} = \sum_{n=0}^{+\infty} \binom{-\dfrac{1}{2}}{n} (-1)^n x^{2n}\]
Per ricostruire l'$\arcsin(x)$ è sufficiente calcolare l'integrale seguente
\[\arcsin(x)=\int_0^x \frac{1}{\sqrt{1-t^2}} \dif t = \int_0^x \left[ \sum_{n=0}^{+\infty} \binom{-\dfrac{1}{2}}{n} (-1)^n t^{2n}\right] \dif t\]
Per le proprietà delle serie di potenze, è possibile portare l'integrale all'interno del segno di sommatoria, da cui
\[\int_0^x \left[ \sum_{n=0}^{+\infty} \binom{-\dfrac{1}{2}}{n} (-1)^n t^{2n}\right] \dif t = \sum_{n=0}^{+\infty} \binom{-\dfrac{1}{2}}{n} (-1)^n \int_0^x t^{2n} \dif t\]
pertanto si ottiene che
\[\arcsin(x)=\sum_{n=0}^{+\infty} \binom{-\dfrac{1}{2}}{n} (-1)^n \cdot \frac{1}{2n+1} x^{2n+1}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: In modo simile è possibile ottenere anche lo sviluppo in serie dell'$\arccos(x)$, prestando attenzione, però, al fatto che $\arccos(0)=\dfrac{\pi}{2}$, per cui
\[\int_0^x - \dfrac{1}{\sqrt{1-t^2}} = [\arccos(t)]_0^x = \arccos{x}-\dfrac{\pi}{2}\]
Per cui, in realtà, si ottiene che
\[\arccos(x) = \int_0^x - \dfrac{1}{\sqrt{1-t^2}} + \dfrac{\pi}{2} = \dfrac{\pi}{2}-\sum_{n=0}^{+\infty} \binom{-\dfrac{1}{2}}{n} (-1)^n \cdot \frac{1}{2n+1} x^{2n+1}\]

\vspace{1em}
\noindent
\subsection{Funzioni in $\mathbb{C}$}
Si consideri l'esponenziale sul campo complesso, come mostrato di seguito:
\[e^z \hspace{1em} \text{con} \hspace{1em} z \in \mathbb{C}\]
per cui, se $z=x+i \cdot y$, per la \textbf{formula di Eulero} si ottiene
\[e^{x+i\cdot y} = e^x \cdot \left[\cos(y) + i \cdot \sin(y)\right]\]
Tuttavia tal'una non è la definizione dell'esponenziale di un numero complesso. Per definire l'esponenziale nel campo complesso, in modo naturale, si utilizzano le serie, come mostrato di seguito:
\[e^z = \sum_{n=0}^{+\infty} \frac{z^n}{n!}\]
Per il criterio del rapporto si ha che la serie è convergente, in quanto
\[\lim_{n \to +\infty} \left \vert \frac{z^{n+1}}{(n+1)!} \right \vert \cdot \left \vert \frac{n!}{z^n} \right \vert = \lim_{n \to +\infty} \vert z \vert \cdot \frac{1}{n+1} = 0 < 1\]
la serie converge per $z$ (essendo $z$ fissato e costante): ciò implica che il raggio di convergenza è infinito, e la convergenza è uniforme su ogni compatto contenuto in $\mathbb{C}$.

\vspace{1em}
\noindent
\textbf{Esempio 1}: La definizione del $\sin(z)$ in campo complesso avviene sempre tramite le serie:
\[\sin(z) = \sum_{n=0}^{+\infty} (-1)^n \cdot \frac{z^{2n+1}}{(2n+1)!}\]
ancora una volta la serie converge. Trattandosi di serie di potenze, se essa converge, converge anche assolutamente, per cui è sufficiente osservare che la serie
\[\sum_{n=0}^{+\infty} \frac{z^{2n+1}}{(2n+1)!}\]
è convergente per il criterio del rapporto:
\[\lim_{n \to + \infty} \left \vert \dfrac{z^{2n+3}}{(2n+3)!} \right \vert \cdot \left \vert \dfrac{(2n+1)!}{z^{n+1}} \right \vert = \lim_{n \to + \infty}\frac{z^2}{(2n+3) \cdot (2n+2)} = 0 < 1\]
In particolare, essa converge per ogni $z$ (essendo $z$ fissato e costante). Il raggio di convergenza è infinito, e la convergenza è uniforme su ogni compatto contenuto in $\mathbb{C}$.

\vspace{1em}
\noindent
\textbf{Esempio 2}: La definizione del $\cos(z)$ in campo complesso avviene in modo analogo, sempre tramite le serie:
\[\cos(z) = \sum_{n=0}^{+\infty} (-1)^n \cdot \frac{z^{2n}}{(2n)!}\]
ancora una volta la serie converge. Trattandosi di serie di potenze, se essa converge, converge anche assolutamente, per cui è sufficiente osservare che la serie
\[\sum_{n=0}^{+\infty} \frac{z^{2n}}{(2n)!}\]
è convergente per il criterio del rapporto:
\[\lim_{n \to + \infty} \left \vert \dfrac{z^{2n+2}}{(2n+2)!} \right \vert \cdot \left \vert \dfrac{(2n)!}{z^{2n}} \right \vert = \lim_{n \to + \infty}\frac{z^2}{(2n+2) \cdot (2n+1)} = 0 < 1\]
In particolare, essa converge per ogni $z$ (essendo $z$ fissato e costante). Il raggio di convergenza è infinito, e la convergenza è uniforme su ogni compatto contenuto in $\mathbb{C}$.

\vspace{2em}
\noindent
\textbf{Esercizio}: Per dimostrare la formula di Eulero, si considera lo sviluppo seguente
\[e^{i \cdot y} = \sum_{n=0}^{+\infty} \frac{1}{n!} \cdot (i \cdot y)^n\]
in cui è evidente capire come, dal momento che
\[i^n = \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{lll}
        1 & \text{se} & n=4k\\
        i & \text{se} & n=4k+1\\
        -1 & \text{se} & n=4k+2\\
        -1 & \text{se} & n=4k+3\\
    \end{array}
\right.\]
è possibile spezzare la sommatoria di cui sopra in $4$ sommatorie a seconda del valore di $i^n$, ottenendo
\[\sum_{n=0}^{+\infty} \frac{1}{n!} \cdot (i \cdot y)^n = \sum_{n=4k}^{+\infty} \frac{y^{4k}}{(4k)!} + \sum_{n=4k+1}^{+\infty} \frac{i \cdot y^{4k+1}}{(4k+1)!} + \sum_{n=4k+2}^{+\infty} -\frac{y^{4k+2}}{(4k+2)!} + \sum_{n=4k+3}^{+\infty} -\frac{i \cdot y^{4k}}{(4k+3)!}\]
Allora posto $m=2n$, le sommatorie con $n=4k=2 \cdot (2k)$ e $n=4k+2=2 \cdot (2k+1)$, per cui si studia quando $m$ è pari o dispari, possono essere riscritte nel seguente modo:
\[\sum_{n=4k}^{+\infty} \frac{y^{4k}}{(4k)!} + \sum_{n=4k+2}^{+\infty} -\frac{y^{4k+2}}{(4k+2)!} = \sum_{m=0}^{+\infty} (-1)^m \frac{1}{(2m)!} \cdot y^{2m} = \cos(y)\]
similmente, le sommatorie con $n=4k+1=2 \cdot (2k)+1$ e $n=4k+3=2 \cdot (2k+1) + 1$, sempre studiando quando $m$ è pari o dispari, possono essere riscritte nel seguente modo:
\[\sum_{n=4k+1}^{+\infty} \frac{i \cdot y^{4k+1}}{(4k+1)!} +\sum_{n=4k+3}^{+\infty} -\frac{i \cdot y^{4k}}{(4k+3)!} = i \cdot \sum_{m=0}^{+\infty} (-1)^m \frac{1}{(2m+1)!} \cdot y^{2m+1} = i \cdot \sin(y)\]

\newpage
\section{Topologia di $\mathbb{R}^n$}
Lo spazio $\mathbb{R}^n$, con $n>1$, \textbf{non eredita la relazione d'ordine} del campo $\mathbb{R}$. In particolare, però, $\mathbb{R}^n$ è uno spazio
\begin{itemize}
    \item vettoriale, dal punto di vista algebrico;
    \item metrico (ovvero vi si può operare ad una certa distanza).
\end{itemize}
Lo spazio $\mathbb{R}^n$ è un insieme di punti, denotati tramite dei vettori. Per esempio, un vettore $x \in \mathbb{R}^n$ è sempre un \textbf{vettore colonna} della forma
\[
    \left(
        \rowcolors{1}{white}{white}
        \begin{array}{c}
            x_1\\
            x_2\\
            \dots\\
            x_n
        \end{array}
    \right)
    \in \mathbb{R}^n \hspace{1em} \text{con} \hspace{1em} x_i \in \mathbb{R}
\]
È fondamentale distinguere tra vettore colonna e matrice di riga: infatti la seconda rappresenta una forma lineare definita in $\mathbb{R}^n$ e non punti di $\mathbb{R}^n$. Quando si parla di $\mathbb{R}^n$ come spazio in generale si adotta la notazione di matrice di riga, mentre per $\mathbb{R}^n$ come spazio vettoriale si parla sempre di vettori colonna. Pertanto, per convenzione, si parlerà sempre di spazio vettoriale e non si distinguerà un punto da un vettore; sarà, quindi, sempre verificata l'equivalenza seguente:
\[
    \left(
        \rowcolors{1}{white}{white}
        \begin{array}{c}
            x_1\\
            x_2\\
            \dots\\
            x_n
        \end{array}
    \right)
    = \left(x_1, x_2, \dots, x_n\right){^T}
\]
Per le proprietà che lo caratterizzano, specialmente dal punto di vista geometrico, $\mathbb{R}^n$ è uno \textbf{spazio di Hilbert}: uno spazio di Hilbert ${\bf{H}} = (H,\left<\cdot,\cdot\right>)$ è uno spazio vettoriale $H$ reale o complesso sul quale è definito un prodotto scalare interno $\langle \cdot,\cdot \rangle$ tale che, detta $\dif$ la distanza indotta da $\langle \cdot ,\cdot \rangle$ $H$, lo spazio metrico $(H,\dif)$ è completo, ossia uno spazio in cui tutte le successioni di Cauchy sono convergenti ad un elemento dello spazio.

\newpage
\noindent
\subsection{Prodotto scalare}
Di seguito si espone la definizione di \textbf{prodotto scalare}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PRODOTTO SCALARE}}\\
    \parbox{\linewidth}{Sia $X$ uno spazio vettoriale su $\mathbb{R}$; allora un \textbf{prodotto scalare} in $X$ è una \textbf{funzione bilineare}
    \[\left< \cdot, \cdot \right> : X \times X \longmapsto \mathbb{R}\]
    che presenta le seguenti proprietà
    \begin{itemize}
        \item La \textbf{simmetria}
        \[\left<x,y\right>=\left<y,x\right> \hspace{1em} \forall x,y \in X\]
        \item La \textbf{positività} e \textbf{non degeneratezza}
        \[\left<x,x\right>\geq 0 \hspace{1em} \forall x \in X\]in particolare
        \[\left<x,x\right>=0 \hspace{1em} \textbf{se e solo se} \hspace{1em} x=0\]
        \item La \textbf{bilinearità}
        \[
            \begin{array}{ll}
                \left<\alpha \cdot x, y \right> = \alpha \cdot \left<x,y\right>&\\
                \left< x, \alpha \cdot y \right> = \alpha \cdot \left<x,y\right>&\\
            \end{array}
            \forall \alpha \in \mathbb{R} \hspace{1em} \text{e} \hspace{1em} \forall x, y \in X
        \]
        \[
            \hspace{-2em}
            \begin{array}{ll}
                \left<x+y,z\right>=\left<x,z\right>+\left<y,z\right>&\\
                \left<x,y+z\right>=\left<x,y\right>+\left<x,z\right>&\\
            \end{array}
            \forall x, y, z \in X
        \]
    \end{itemize}
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione 1}: In particolare, in $\mathbb{R}^n$, si definisce il prodotto scalare come un banale prodotto righe per colonne, ovvero
\[\left<x,y\right> = x{^T} \cdot y = \left(x_1,x_2,\dots,x_n\right) \cdot \left(
    \rowcolors{1}{white}{white}
    \begin{array}{c}
        y_1\\
        y_2\\
        \dots\\
        y_n
    \end{array}
\right) = \sum_{k=1}^n x_k \cdot y_k\]

\vspace{1em}
\noindent
\textbf{Osservazione 2}: Un prodotto scalare non definito su $\mathbb{R}^n$, ma definito sullo spazio vettoriale seguente
\[C^0 \left(\left[a,b\right]\right) = \{\phi : [a,b] \longmapsto \mathbb{R}, \text{ continua}\}\]
allora il prodotto interno seguente
\[\left<\phi,\psi\right> = \int_a^b \phi(x) \cdot \psi(x) \dif x\]
è a tutti gli effetti un prodotto scalare, in quanto soddisfa tutte le proprietà di cui sopra, che derivano dalla linearità dell'integrale.

\vspace{2em}
\subsection{Norma di un vettore}
Si espone di seguito la definizione di \textbf{norma di un vettore}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{NORMA DI UN VETTORE}}\\
    \parbox{\linewidth}{Sia $X$ uno spazio vettoriale su cui è definito un prodotto scalare. Allora si definisce norma di un vettore $x \in X$ una funzione
    \[\vert \vert \cdot \vert \vert : X \longmapsto \mathbb{R}\]
    definita come segue
    \[\left \vert \left \vert x \right \vert \right \vert = \sqrt{\left<x,x\right>}\]
    La norma gode delle seguenti proprietà
    \begin{itemize}
        \item $\left \vert \left \vert x \right \vert \right \vert \geq 0, \hspace{1em} \forall x \in X$\\\\
        In particolare $\left \vert \left \vert x \right \vert \right \vert = 0$ \textbf{se e solo se} $x=0$
        \item $\left \vert \left \vert \alpha x \right \vert \right \vert = \left \vert \alpha \right \vert \cdot \left \vert \left \vert x \right \vert \right \vert$ con $\alpha \in \mathbb{R}$ e $x \in X$
        \item $\left \vert \left \vert x+y \right \vert \right \vert \leq \left \vert \left \vert x \right \vert \right \vert + \left \vert \left \vert y \right \vert \right \vert$ chiamata disuguaglianza triangolare.
    \end{itemize}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Tale dimostrazione è immediata dalla definizione di norma
\[\left \vert \left \vert x \right \vert \right \vert = \sqrt{\left<x,x\right>}\]
per cui, essendo il prodotto scalare sempre non negativo, automaticamente lo è anche la sua radice. Non solo, ma siccome il prodotto scalare è non degenere, per cui $\left<x,x\right>=0$ se e solo $x=0$, tale proprietà viene automaticamente riflessa sulla norma.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Per la proprietà di bilinearità del prodotto scalare è immediato evincere che
\[\vert \vert \alpha \cdot x \vert \vert = \sqrt{\left< \alpha \cdot x, \alpha \cdot x\right>} = \sqrt{ \alpha \cdot \left<x, \alpha \cdot x\right>} = \sqrt{\alpha^2 \cdot \left<x,x\right>} = \left \vert \alpha \right \vert \cdot \sqrt{\left<x,x\right>} = \left \vert \alpha \right \vert \cdot \left \vert \left \vert x \right \vert \right \vert\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 3}: La disuguaglianza triangolare si può dimostrare partendo dalla definizione di norma, ovvero
\[\vert \vert x+y \vert \vert = \sqrt{\left<x+y,x+y\right>}\]
Per la proprietà di bilinearità e simmetria del prodotto scalare, si ottiene che
\[\sqrt{\left<x+y,x+y\right>} = \sqrt{\left<x,x\right> + 2 \cdot \left<x,y\right> + \left<y,y\right>} = \sqrt{\vert \vert x \vert \vert^2 + 2 \cdot \left<x,y\right> + \vert \vert y \vert \vert^2}\]
ma ora è possibile impiegare la disuguaglianza di Bunyakovsky-Cauchy-Schwarz, per cui
\[2 \cdot \left<x,y\right>  \leq 2 \cdot \vert\left<x,y\right> \vert \leq 2 \cdot \vert \vert x \vert \vert \cdot \vert \vert y \vert \vert\]
per cui
\[\sqrt{\vert \vert x \vert \vert^2 + 2 \cdot \left<x,y\right> + \vert \vert y \vert \vert^2} \leq \sqrt{\vert \vert x \vert \vert^2 + 2 \cdot \vert \vert x \vert \vert \cdot \vert \vert y \vert \vert + \vert \vert y \vert \vert^2} = \sqrt{(\vert \vert x \vert \vert + \vert \vert y \vert \vert)^2} = \vert \vert x \vert \vert + \vert \vert y \vert \vert\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che la norma potrebbe anche essere definita in modo molto più astratto: basterebbe trovare una funzione che soddisfi le proprietà della norma e quella, automaticamente, diventerebbe una norma. Se in uno spazio vettoriale, però, è definito un prodotto scalare, allora si adotta come norma la cosiddetta \quotes{norma canonica}:
\[\left \vert \left \vert x \right \vert \right \vert = \sqrt{\left<x,x\right>}\]
In generale, in $\mathbb{R}^n$ la norma che si considera è la cosiddetta \textbf{norma euclidea}:
\[\vert \vert x \vert \vert = \sqrt{\sum_{k=1}^n x_k^2}\]
Ci sono, tuttavia, spazi vettoriali in cui non è definito un prodotto scalare, ma presentano comunque una norma. Pertanto, se in uno spazio vettoriale è definito un prodotto scalare, automaticamente vi è una norma indotta, ma non vale il viceversa.\\
Si osservi che se $X$ è uno spazio vettoriale dotato di una norma $\left \vert \left \vert \cdot \right \vert \right \vert$ (che potrebbe essere anche indotta da un prodotto scalare), si può definire la distanza come la funzione seguente
\[d(x,y) = \left \vert \left \vert x-y \right \vert \right \vert\]
che consente di parlare di palle, intorni, intervalli aperti e chiusi e, di conseguenza, anche di limiti.\\
Ancora una volta, il concetto di distanza potrebbe essere introdotto anche in spazi molto più generali, che non è nemmeno necessario siano spazi vettoriali.

\vspace{1em}
\subsection{Disuguaglianza di Bunyakovsky-Cauchy-Schwarz}
Si espone di seguito la disuguaglianza di Bunyakovsky-Cauchy-Schwarz:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{DISUGUAGLIANZA DI BUNYAKOVSKY-CAUCHY-SCHWARZ}}\\
    \parbox{\linewidth}{Sia $X$ uno spazio vettoriale con prodotto scalare, allora $\forall x,y \in X$ si ha che
    \[\left \vert \left<x,y\right> \right \vert \leq \left \vert \left \vert x \right \vert \right \vert \cdot \left \vert \left \vert y \right \vert \right \vert\]
    Inoltre, vale l'uguaglianza \textbf{se e solo se} $x$ e $y$ sono \textbf{linearmente dipendenti}, ossia esiste una combinazione lineare
    \[\alpha \cdot x + \beta \cdot y = 0\]
    in cui gli scalari $\alpha,\beta$ non sono tutti nulli, ossia $\alpha^2+\beta^2\neq 0$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si dimostri che $\forall x, y \in X$ si ha che
\[\left \vert \left<x,y\right> \right \vert^2 \leq \left \vert \left \vert x \right \vert \right \vert^2 \cdot \left \vert \left \vert y \right \vert \right \vert^2 \hspace{1em} \rightarrow \hspace{1em} \left<x,y\right>^2 - \left<x,x\right> \cdot \left<y,y\right> \leq 0\]
che equivale a dimostrare l'enunciato della disuguaglianza di Bunyakovsky-Cauchy-Schwarz. Più specificatamente, si distinguono i casi seguenti
\begin{itemize}
    \item Nel caso in cui $y=0$, si ha che
    \[\left<x,y\right>=0 \hspace{0.5em} \forall x \hspace{1em} \hspace{1em} \text{e} \hspace{1em} \left \vert \left \vert y \right \vert \right \vert = 0\]
    per cui la disuguaglianza è immediata.
    \item Nel caso in cui $y \neq 0$ e $x \neq 0$, si introduce, $\forall t \in \mathbb{R}$, il polinomio seguente
    \[0 \leq \left \vert \left \vert x + ty \right \vert \right \vert^2 = \left \vert \left \vert x \right \vert \right \vert^2 + 2t \cdot \left<x,y\right> + t^2 \cdot \left \vert \left \vert y \right \vert \right \vert^2\]
    Tale sviluppo deriva dalla definizione di norma e dall'applicazione delle proprietà di bilinearità del prodotto scalare, infatti:
    \[\vert \vert x+ty \vert \vert^2 = \left<x+ty,x+ty\right>=\left<x,x+ty\right>+\left<ty,x+ty\right>=\left<x,x\right>+\left<x,ty\right>+\left<ty,x\right>+\left<ty,ty\right>\]
    ma per la definizione di norma e per la simmetria e linearità del prodotto scalare:
    \[\left<x,x\right>+2 \cdot \left<x,ty\right>+\left<ty,ty\right> = \vert \vert x \vert \vert^2 + 2t \cdot \left<x,y\right> + t^2 \cdot \vert \vert y \vert \vert^2\]
    Avendo fissato $x$ e $y$, è possibile ora considerare tale espressione come un polinomio in funzione di $t$, ovvero
    \[f(t) = \underbrace{\vert \vert x \vert \vert^2}_c + t \cdot \underbrace{2\left<x,y\right>}_b + t^2 \cdot \underbrace{\vert \vert y \vert \vert^2}_a\]
    È immediato osservare che
    \[f(t) \geq 0 \hspace{0.5em} \forall t \hspace{1em} \textbf{se e solo se} \hspace{1em} \Delta f(t) \leq 0\]
    Ma siccome il discriminante $\Delta$ del polinomio si ottiene come segue
    \[\Delta f(t) = b^2 - 4ac = 4 \cdot \left(\left<x,y\right>\right)^2 - 4 \cdot \left \vert \left \vert x \right \vert \right \vert^2 \cdot \left \vert \left \vert y \right \vert \right \vert^2\]
    si deve dimostrare che $\Delta f(t) \leq 0$ per veder verificata la disuguaglianza di Bunyakovsky-Cauchy-Schwarz, ossia che
    \[4 \cdot \left(\left<x,y\right>\right)^2 - 4 \cdot \left \vert \left \vert x \right \vert \right \vert^2 \cdot \left \vert \left \vert y \right \vert \right \vert^2 \leq 0\]
    che può essere riscritto come
    \[\left(\left<x,y\right>\right)^2 \leq \left \vert \left \vert x \right \vert \right \vert^2 \cdot \left \vert \left \vert y \right \vert \right \vert^2\]
    che è esattamente la disuguaglianza che si stava cercando.
\end{itemize}
Per dimostrare che la disuguaglianza di Bunyakovsky-Cauchy-Schwarz diviene un'uguaglianza \textbf{se e solo se} i vettori $x$ e $y$ considerati sono linearmente dipendenti, si deve dimostrare che ciò è vero se e solo se $\Delta f(t) = 0$, dal momento che il polinomio, per quanto già dimostrato, è $f(t) \geq 0$, per cui può annullarsi solamente con $\Delta=0$.\\
Ma chiedere che il discriminante del polinomio sia nullo significa chiedere che esiste uno $0$ del polinomio, ossia $\exists \tilde{t} \in \mathbb{R}$ tale che $f(\tilde{t})=0$, ovvero che
\[\left \vert \left \vert x+ \tilde{t}y \right \vert \right \vert^2 = 0\]
Ma ciò è vero se e soltanto se $x+ \tilde{t}y=0$ per la definizione di norma, ovvero $x=-\tilde{t}y$, che significa che i vettori $x$ e $y$ sono linearmente dipendenti. Da notare, infine, che $\tilde{t} \neq 0$, in quanto se fosse nullo, allora $x=0$, ma ciò contrasta con le ipotesi di partenza.

\newpage
\begin{center}
    18 Ottobre 2022
\end{center}
Lo spazio $\mathbb{R}^n$ è uno spazio vettoriale, in cui i suoi elementi sono delle matrici colonna, non proprio dei vettori. Non solo, ma per quanto già esposto, $\mathbb{R}^n$ è uno spazio di Hilbert; per capirne il significato, si consideri uno spazio in cui è è definito un \textbf{prodotto scalare} come segue
\[X \text{ spazio vettoriale con } \left<\cdot,\cdot\right> : X \times X \longmapsto \mathbb{R}\]
Allora, se in uno spazio vettoriale viene definito un prodotto scalare, automaticamente viene definita una \textbf{norma indotta}
\[\left \vert \left \vert \cdot \right \vert \right \vert : X \longmapsto \mathbb{R}\]
definita come
\[\left \vert \left \vert x \right \vert \right \vert = \sqrt{\left<x,x\right>}\]
Tuttavia, esistono anche spazi vettoriali su cui è definita una norma, senza che vi sia definito uno prodotto scalare, come quella esposta di seguito, definita come \textbf{norma $\bf{1}$}: 
\begin{itemize}
    \item La norma $1$ di un vettore è la somma dei valori assoluti delle sue componenti:
    \[\left \vert \left \vert x \right \vert \right \vert_1 = \vert x_1 \vert + \vert x_2 \vert + \dots + \vert x_n \vert\]
    posto
    \[x=(x_1,x_2,\dots,x_n){^T}\]
    \item Per quanto riguarda l'omogeneità, è noto che, nel caso di una norma indotta si ha che, $\left \vert \left \vert \alpha \cdot x \right \vert \right \vert = \vert \alpha \vert \cdot \left \vert \left \vert x \right \vert \right \vert$. Nel caso di un vettore, si ha la corrispondenza
    \[\left \vert \left \vert \alpha \cdot (x_1,x_2,\dots,x_n){^T} \right \vert \right \vert_1 = \left \vert \left \vert (\alpha \cdot x_1, \alpha \cdot x_2, \dots, \alpha \cdot x_n){^T} \right \vert \right \vert_1\]
    per quanto esposto sul significato della norma $1$, tale espressione diviene
    \[\vert \alpha \cdot x_1 \vert + \vert \alpha \cdot x_2 \vert + \dots + \vert \alpha \cdot x_n \vert = \vert \alpha \vert \cdot \left \vert \left \vert x \right \vert \right \vert_1\]
    \item La disuguaglianza triangolare è ugualmente verificata.Nel caso di una norma indotta è noto che
    \[\vert \vert x_1 + x_2 \vert \vert \leq \vert \vert x_1 \vert \vert + \vert \vert x_2 \vert \vert\]
    mentre nel caso della norma $1$ si ha
    \[\vert \vert (x_1,x_2,\dots,x_n){^T} + (y_1,y_2,\dots,y_n){^T} \vert \vert = \vert \vert (x_1+y_1,x_2+y_2,\dots,x_n+y_n){^T} \vert \vert\]
    e per quanto esposto sul significato della norma $1$, tale espressione diviene
    \[\vert x_1+y_1 \vert + \vert x_2 + y_2 \vert + \dots + \vert x_n + y_n \vert \leq \vert x_1 \vert + \vert y_1 \vert + \vert x_2 \vert + \vert y_2 \vert + \dots + \vert x_n \vert + \vert y_n \vert\]
\end{itemize}

\vspace{1em}
\noindent
\textbf{Osservazione}: L'unica norma che risulta essere indotta da un prodotto scalare è la norma $2$. È possibile, inoltre, andare a definire in generale la norma $p$ di un vettore (avendo definito la norma $1$ come la somma del valore assoluto delle sue componenti). In particolare si ha che
\[\vert \vert x \vert \vert_p = \left(\sum_{i=1}^n \vert \vert x_i \vert \vert ^p\right)^{\frac{1}{p}}\]
In questo modo è automaticamente definita la norma $2$, per cui
\[\vert \vert x \vert \vert_2 = \sqrt{\sum_{i=1}^n x_i^2}\]
o anche la \textbf{norma infinito}, calcolata come
\[\vert \vert x \vert \vert_\infty = \max \left\{\vert x_i \vert, \hspace{1em} \text{con} \hspace{1em} i=1,2,\dots,n\right\}\]

\vspace{1em}
\noindent
Ciò che risulta essere fondamentale è lo spazio di funzioni del tipo
\[X = C \left([a,b]\right)\]
in cui viene considerato il prodotto scalare
\[\left<\phi,\psi\right> = \int_{[a,b]} \phi(t) \cdot \psi(t)\]
naturalmente, la norma $2$ indotta da tale prodotto scalare viene definita come
\[\vert \vert \phi \vert \vert_2 = \sqrt{\int_a^b \phi^2(t) \dif t}\]
e più in generale la norma $p$ (con $p \geq 1$) indotta da tale prodotto scalare verrà definita come
\[\vert \vert \phi \vert \vert_p = \left(\int_a^b \phi^p(t) \dif t \right)^{\frac{1}{p}}\]
e quindi la norma infinito è data semplicemente dal massimo della funzione, ossia
\[\vert \vert \phi \vert \vert_\infty = \max \left\{ \vert \phi(t) \vert, \hspace{1em} \text{con} \hspace{1em} t \in [a,b] \right\}\]

\vspace{1em}
\noindent
Le norme finora esposte sono definite \textbf{norme nello spazio $L^p$}, ossia lo spazio delle funzioni a $p$-esima potenza sommabile.

\vspace{1em}
\noindent
\subsection{Distanza (metrica)}
Ogni qualvolta si ha a disposizione una norma, automaticamente è possibile definire una distanza: in particolare si può considerare come distanza tra due punti $x$ e $y$ la norma della differenza, ossia, $\vert \vert x-y \vert \vert$.\\
Di seguito si espone la definizione di \textbf{distanza (metrica)}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{DISTANZA (METRICA)}}\\
    \parbox{\linewidth}{Dato $X$ un \textbf{insieme non vuoto}, si chiama \textbf{distanza (metrica)} su $X$ una funzione $d$ definita come
    \[d : X \times X \longmapsto \mathbb{R}\]
    che soddisfa le proprietà seguenti:
    \begin{itemize}
        \item la distanza tra due punti è sempre non negativa, per cui
        \[d(x,y) \geq 0, \hspace{0.5em} \forall x, y \in \mathbb{R}\]
        inoltre si ha che
        \[d(x,y) = 0 \hspace{0.5em} \textbf{se e solo se} \hspace{0.5em} x=y\]
        tuttavia esistono anche \textbf{semi-metriche} dove tale proprietà non è verificata;

        \item la proprietà simmetrica, ovvero
        \[d(x,y)=d(y,x)\]
        che, come in precedenza, non è sempre verificata per distanze semi-metriche;

        \item la disuguaglianza triangolare, che afferma che 
        \[d(x,z) \leq d(x,y) + d(y,z)\]
    \end{itemize}
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazioni}: Si osservi che un prodotto scalare induce una norma, una norma induce una distanza e una distanza induce una topologia. Il concetto più astratto, ovviamente, è quello di spazio topologico che può essere definito in modo molto astratto, senza parlare di distanza.

\vspace{1em}
\noindent
\subsection{Spazio metrico}
Di seguito si espone la definizione di \textbf{spazio metrico}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SPAZIO METRICO}}\\
    \parbox{\linewidth}{Si dice spazio metrico un insieme in cui è definita una distanza. \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio 1}: Si consideri uno spazio $X \neq \varnothing$ (non necessariamente uno spazio vettoriale). È sempre possibile definire la \textbf{distanza banale} seguente:
\[
    d(x,y) = \left\{
        \rowcolors{1}{white}{white}
        \begin{array}{lll}
            0 & \text{se} & x=y\\
            1 & \text{se} & x \neq y\\
        \end{array}
    \right.
\]
che è una distanza a tutti gli effetti, in quanto soddisfa tutte le proprietà precedentemente esposte.

\vspace{1em}
\noindent
\textbf{Esempio 2}: Si consideri uno spazio $X=\mathbb{R}^n$, allora si definisce \textbf{distanza euclidea} come
\[d(x,y) = \vert \vert x-y \vert \vert_2 = \sqrt{\sum_{i=1}^n (x_i-y_i)^2}\]

\vspace{1em}
\noindent
\textbf{Esempio 3}: Si consideri uno spazio $X=C([0,2 \pi])$, ossia lo spazio delle funzioni continue nell'intervallo $[0,2\pi]$. Allora si definisce la distanza come
\[d(\phi,\psi) = \vert \vert \phi-\psi \vert \vert_2 = \sqrt{\int_0^{2\pi} (\phi(t) - \psi(t))^2}\]
Allora si ha che
\[d(\sin(t),\cos(t)) = \sqrt{2\pi}\]
in quanto
\[\sqrt{\int_0^{2\pi} (\sin(t) - \cos(t))^2} = \sqrt{\int_0^{2\pi} \sin^2(t) - 2 \sin(t) \cos(t) + \cos^2(t))}\]
da ciò si evince che 
\[\sqrt{\int_0^{2\pi} \sin^2(t) - 2 \sin(t) \cos(t) + \cos^2(t))} = \sqrt{\int_0^{2\pi} \dfrac{1-\cos(2t)}{2} - \sin(2t) + \dfrac{1+\cos(2t)}{2}}\]
da cui
\[= \sqrt{\left[\dfrac{1}{2}t - \dfrac{\sin(2t)}{4} + \dfrac{\cos(2t)}{2} + \dfrac{1}{2}t + \dfrac{\sin(2t)}{4}\right]_0^{2\pi}} = \sqrt{2\pi}\]

\vspace{1em}
\subsection{Vettori ortogonali}
Di seguito si espone la definizione di \textbf{vettori ortogonali}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{VETTORI ORTOGONALI}}\\
    \parbox{\linewidth}{Dato uno spazio metrico $X, \left<\cdot,\cdot\right>$, due vettori $x,y$ si dicono \textbf{ortogonali} se
    \[\left<x,y\right>=0\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
Nello spazio dell'esempio precedente, $\sin(t)$ e $\cos(t)$ sono ortogonali, in quanto
\[\int_0^{2\pi} \sin(t) \cot(t) = 0\]

\vspace{1em}
\subsection{Palla-aperta e palla-chiusa}
Si espone di seguito la definizione di \textbf{palla} (sia \textbf{aperta} che \textbf{chiusa}):

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PALLA-APERTA E PALLA-CHIUSA}}\\
    \parbox{\linewidth}{Sia $X,d$ uno spazio metrico, con $X \neq \varnothing$ e $d$ la distanza in esso definita.\\
    Si chiama \textbf{palla-aperta} di centro $x_0 \in X$ e raggio $r \in \mathbb{R}^+$ l'insieme così definito
    \[\mathcal{B}(x_0,r) = \{x \in X : d(x,x_0) < r\}\]
    mentre si chiama \textbf{palla-chiusa} di centro $x_0 \in X$ e raggio $r \in \mathbb{R}^+$ l'insieme così definito
    \[\mathcal{B}_\text{chiusa}(x_0,r) = \overline{\mathcal{B}(x_0,r)} = \{x \in X : d(x,x_0) \leq r\}\]
    in cui si è indicata la palla-chiusa come $\overline{\mathcal{B}(x_0,r)}$ in quanto in $\mathbb{R}^n$ la palla-chiusa è proprio la chiusura della palla-aperta, ma ciò non è sempre vero.
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Esempio 1}: Si consideri lo spazio vettoriale $\mathbb{R}^2$ con la norma $\vert \vert \cdot \vert \vert_2$; allora la $\mathcal{B}(0,1)$ è semplicemente il cerchio di centro $0$ e raggio $1$.\\
Tuttavia, se al posto di considerare $\vert \vert \cdot \vert \vert_2$ si considera $\vert \vert \cdot \vert \vert_1$, allora si ha che
\[d((x_1,x_2){^T},(0,0){^T}) = \vert x_1 \vert + \vert x_2 \vert\]
Allora la palla $\mathcal{B}(0,1)$ è un quadrato ruotato con i vertici sugli assi (in quanto basta considerare la retta $x_1+x_2=1$ e lavorare per simmetria).\\
Se si considera, invece, la norma indotta
\[\vert \vert x \vert \vert_\infty = \max\{\vert x_1 \vert, \vert x_2 \vert\}\]
è esattamente un quadrato di lato $2$ e centrato nell'origine.\\
Se si considerano, in generale, delle norme $\vert \vert \cdot \vert \vert_p$ con $p \geq 2$, allora si ottengono delle \quotes{circonferenze} sempre più grandi e schiacciate ai lati, avvicinandosi sempre di più al quadrato ottenibile con $p=\infty$. 

\newpage
\subsection{Intorno}
Di seguito si espone la definizione di \textbf{intorno}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{INTORNO}}\\
    \parbox{\linewidth}{Sia $X,d$ uno spazio metrico, con $x_0 \in X$. Si dice intorno di $x_0$ qualunque insieme $U \subseteq X$ tale che esiste $r>0$ affinché
    \[\mathcal{B}(x_0,r) \subseteq U\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\subsubsection{Proprietà di un intorno}
Si espongono di seguito le \textbf{proprietà degli intorni}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PROPRIETÀ DEGLI INTORNI}}\\
    \parbox{\linewidth}{Sia $\mathcal{N}_x$ (con $\mathcal{N}$ ad indicare \quotes{neighbourhood}) una famiglia di intorni di $x$, allora
    \begin{enumerate}
        \item Sia $U \in \mathcal{N}_x$, allora $x \in U$;
        \item \textbf{Proprietà di filtro}
        \begin{itemize}
            \item Siano $U,V \in \mathcal{N}_x$, allora $U \cap V \in \mathcal{N}_x$;
            \item Siano $U \in \mathcal{N}_x$ e $V \subseteq X$ (con $X$ spazio metrico generale); se $U \subset V$, allora $V \in \mathcal{N}_x$, ossia ogni soprainsieme di un intorno è ancora è un intorno;
        \end{itemize}
        \item \textbf{Proprietà di separazione di Hausdorff}: siano $x \neq y$; allora esiste $U \in \mathcal{N}_x$ e $V \in \mathcal{N}_y$ tale che $U \cap V = \varnothing$.
    \end{enumerate}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Dal momento che $U \in \mathcal{N}_x$, allora per definizione di intorno $\exists \mathcal{B}(x,r) \subset U$; per definizione di palla, allora, deve essere che $x \in U$, in quanto il centro della palla deve appartenere all'insieme.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Dal momento che $U \in \mathcal{N}_x$, allora $\exists \mathcal{B}(x,r_1) \subset U$. Analogamente, dal momento che $V \in \mathcal{N}_x$, allora $\exists \mathcal{B}(x,r_2) \subset V$.\\
Si considera il minimo raggio tra $r_1$ e $r_2$, ovvero $r=\min\{r_1,r_2\}$. Allora, ovviamente, si ha che
\[\mathcal{B}(x,r) \subseteq U \cap V\]
Ciò significa che $U \cap V \in \mathcal{N}_x$ per definizione di intorno.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 3}: Sia $U \in \mathcal{N}_x$, allora $\exists \mathcal{B}(x,r) \subset U$; ma siccome $U \subset V$ quindi $\mathcal{B}(x,r) \subset V$; per definizione di intorno, ciò significa che $V \in \mathcal{N}_x$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 4}: Si considerino $x$ e $y$ due numeri reali distinti. Allora, per la proprietà della distanza, essendo $x \neq y$, si ha che la loro distanza è strettamente positiva
\[r = d(x,y) > 0\]
Al fine di avere due palle disgiunte, si considerano due raggi minori della metà della distanza tra $x$ e $y$; per esempio
\[r_1=\frac{r}{4} \hspace{1em} \text{e} \hspace{1em} r_2=\frac{r}{4}\]
Ciò comporta che
\[\mathcal{B}(x,r_1) \cap \mathcal{B}(y,r_2) = \varnothing\]
Per dimostrare ciò, si proceda per assurdo, assumendo che $\exists p \in \mathcal{B}(x,r_1) \cap \mathcal{B}(y,r_2)$. Per la disuguaglianza triangolare, allora si ha che
\[r=d(x,y) \leq d(x,p) + d(p,y) < 2 \cdot \frac{r}{4}\]
che è assurdo.\\
Tale proprietà è alla base della dimostrazione del teorema di unicità del limite.

\vspace{1em}
\noindent
\subsection{Punto interno ad un insieme}
Si fornisce di seguito la definizione di \textbf{punto interno ad un insieme}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PUNTO INTERNO AD UN INSIEME}}\\
    \parbox{\linewidth}{Un punto $x$ si dice \textbf{punto interno} di un insieme $U$ se $U$ è un intorno di $x$, ovvero esiste $\mathcal{B}(x,r) \subseteq U$. \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che chiedere che un punto sia interno ad un insieme è più forte che chiedere che il punto appartenga ad un insieme. Infatti, se $U = [a,b]$, allora $a \in U$, ma $a$ non è interno ad $U$.\\
Un insieme $A \subseteq X$ è \textbf{aperto} se per ogni $x \in A$, $x$ è interno ad $A$, ossia $A$ è un intorno di ogni suo punto.

\vspace{1em}
\noindent
\subsection{Punto isolato}
Si fornisce di seguito la definizione di \textbf{punto isolato}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PUNTO ISOLATO}}\\
    \parbox{\linewidth}{Un punto $x \in X$ si dice isolato in $E \subseteq X$ se $x \in E$ ed esiste $\mathcal{B}(x,r)$ tale che $\mathcal{B}(x,r) \cap E = \{x\}$. \vspace{3mm}}\\
    \hline
\end{tabularx}


% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PUNTO DI ACCUMULAZIONE}}\\
    \parbox{\linewidth}{Sia $E \subseteq X$. Un punto $x \in X$ si dice \textbf{punto di accumulazione} di $E$ se per ogni intorno $U$ di $x$ esiste $y \in U \cap E$, $y \neq x$. \vspace{3mm}}\\
    \hline
\end{tabularx}

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{CHIUSURA DI UN INSIEME}}\\
    \parbox{\linewidth}{Sia $E \subseteq X$, si chiama chiusura di $E$ l'insieme
    \[\overline{E} = E \cup \{\text{punto di accumulazione di } E\}\]
    \vspace{-2mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione}: Sia $X$ un insieme, allora $X$ e $\varnothing$ sono sia aperti che chiusi. Tuttavia, in $\mathbb{R}$, per esempio, esistono insiemi che non sono né aperti né chiusi, come $]a,b]$.

\vspace{1em}
\noindent
\textbf{Esercizio}: Si consideri la serie seguente
\[\sum_{n=0}^{+\infty} \frac{2n+i}{3^n-n \cdot i}\]
Tale serie può essere studiata andando a calcolare la corrispondente serie dei moduli, ottenendo
\[\left \vert \frac{2n+i}{3^n-n \cdot i} \right \vert = \frac{\vert 2n+i \vert}{\vert 3^n-n \cdot i \vert} = \frac{\sqrt{4n^2+1}}{\sqrt{9^n+n^2}}\]
Appare immediatamente evidente che tale termine abbia ordine di infinitesimo soprareale, per cui è necessariamente convergente.\\
Oppure può essere considerato anche il criterio del rapporto, per cui
%\[\dfrac{\dfrac{\sqrt{4(n+1)^2+1}}{\sqrt{9^(n+1)+(n+1)^2}}}{}\] 

\vspace{1em}
\noindent
\subsection{Corrispondenza tra palla aperta e insieme aperto}
Una palla aperta è un insieme aperto.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si consideri uno spazio metrico generale $X,d$. Sia, allora, una palla aperta $\mathcal{B}(x_0,r)$ di centro $x_0$ e raggio $r \in \mathbb{R}^+$.\\
Si vuole dimostrare che che $\forall x_1 \in \mathcal{B}(x_0,r) \hspace{1em} \exists \mathcal{B}_1(x_1,\rho) \subseteq \mathcal{B}(x_0,r)$.\\
Dal momento che $r-d(x_1,x_0) > 0$, si definisce $\rho > 0$ che presenta la seguente proprietà $\rho < r - d(x_1,x_2)$. Si verifichi, ora, che $\mathcal{B}_1 \subseteq \mathcal{B}$, ovvero dato $x \in \mathcal{B}$ deve essere che $x \in \mathcal{B}$.\\
Per quanto assunto in precedenza, si ha che $d(x,x_1) < \rho < r-d(x_1,x_0)$. Allora si ha che
\[d(x,x_0) \leq d(x,x_1) + d(x_1,x_0) < r - d(x_1,x_0)+d(x_1,x_0) = r\]

\vspace{1em}
\subsection{Caratterizzazione di un insieme chiuso}
$E \subseteq X$ è un insieme chiuso \textbf{se e solo se}l'insieme complementare $X-E$ è aperto in $X$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Sia $E$ un insieme chiuso e si consideri $x_0 \in X-E$. Ovviamente $x_0 \notin E$, per cui non può essere un punto di accumulazione per $E$, in quanto esso è un insieme chiuso e, quindi, contiene tutti i suoi punti di accumulazione.\\
Se $x_0$ fosse un punto di accumulazione, per definizione, $\forall U$ intorno di $x_0$, dovrebbe $\exists y \neq x_0$ tale che $y \in U \cap E$.\\
Pertanto, non essendo $x_0$ punto di accumulazione per $E$, significa che $\exists \mathcal{B}(x_0,\epsilon)$, con $\epsilon>0$ tale che $\mathcal{B} \subset X - E$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Sia $X - E$ un insieme aperto, si dimostri che $E$ è chiuso.\\
Siccome $X-E$ è aperto, si ha che
\[\forall x_1 \in X - E, \hspace{0.5em} \exists \mathcal{B}(x,\epsilon) \text{ con } \epsilon>0 \text{ tale che } \mathcal{B} \subset X - E\]
Siccome $\mathcal{B} \cap E = \varnothing$, $x$ non è punto di accumulazione per $E$ e, siccome ciò significa che $E$ contiene tutti i suoi punti di accumulazione, $E$ è chiuso.

\vspace{2em}
\noindent
\textbf{Esercizio}: Si provi che ogni insieme del tipo $]a,b[ \times ]c,d[$ con $a<b$ è aperto in $\mathbb{R}^n$.\\

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si consideri $A=]a,b[$ e $B=]c,d[$, per cui
\[C=A \times B = \{(x,y) \in \mathbb{R}^2 \text{ tale che } x \in A, y \in B\}\]
Si vuole dimostrare che $\forall (x,y) \in C, \exists r > 0$ tale che
\[\mathcal{B} \left((x,y), r\right) \subset C\]
Allora si considera un punto d coordinate $(x,y)$, con $a<x<b$ e $c<y<d$, allora si può prendere $r_1>0$ e $r_2>0$ tale che
\[a < x-r_1 < x < x+r_1 < b \hspace{1em} \text{e} \hspace{1em} c < y-r_2 < y < y+r_2 < d\]
Si considera, poi $r = \min(r_1,r_2)$, affinché si possa costruire un quadrato di lato $l=2r$.\\
... continua ...

\vspace{1em}
\noindent
\subsection{Punto di frontiera}
Si espone di seguito la definizione di \textbf{punto di frontiera}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PUNTO DI FRONTIERA}}\\
    \parbox{\linewidth}{Sia $E \subseteq X$; allora, preso $x \in X$, si dice punto di frontiera di $E$ se per ogni intorno $U$ di $x$ esiste
    \[y_1 \in U \cap E \hspace{1em} \text{e} \hspace{1em} y_2 \in U - E\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\subsection{Frontiera di un insieme}
Si espone di seguito la definizione di \textbf{frontiera di un insieme}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{FRONTIERA DI UN INSIEME}}\\
    \parbox{\linewidth}{Si chiama frontiera di $E$ l'insieme dei punti di frontiera di $E$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}


\vspace{1em}
\noindent
\subsection{Insieme denso}
Si espone di seguito la definizione di \textbf{insieme denso}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{INSIEME DENSO}}\\
    \parbox{\linewidth}{
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\subsection{Insieme limitato}
Si espone di seguito la definizione di \textbf{insieme limitato}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{INSIEME LIMITATO}}\\
    \parbox{\linewidth}{Sia $E \subseteq X$. Allora $E$ si dice \textbf{limitato} se esiste $\mathcal{B}(x_0,r) \subseteq X$ tale che $E \subseteq \mathcal{B}(x_0,r)$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\subsection{Diametro di un insieme}
Si espone di seguito la definizione di \textbf{diametro di un insieme}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{DIAMETRO DI UN INSIEME}}\\
    \parbox{\linewidth}{Sia $E \subseteq X$. Allora si chiama diametro di $E$
    \[\text{dim } E = \sup \{d(x,y), x,y \in E\}\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione}: Un insieme $E$ è limitato \textbf{se e solo se} dim $E$ è finito.

\newpage
\begin{center}
    19 Ottobre 2022
\end{center}
Dopo aver introdotto la topologia in $\mathbb{R}^n$, sono stati introdotti dei concetti che sono validi per qualsiasi spazio metrico, quali il concetto di \textbf{palla} e di \textbf{intorno} che consente di introdurre la definizione di limite, in qualsiasi spazio vettoriale.

\vspace{1em}
\noindent
\subsection{Proprietà di insiemi aperti e chiusi}
Si espongono di seguito le proprietà di insiemi aperti e chiusi:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{PROPRIETÀ DI INSIEMI APERTI E CHIUSI}}\\
    \parbox{\linewidth}{Gli insiemi aperti e chiusi soddisfano le proprietà seguenti:
    \begin{enumerate}
        \item L'unione di insiemi aperti è aperta
        \item L'intersezione di insiemi aperti è aperta
        \item L'unione di insiemi chiusi è chiusa
        \item L'intersezione di insiemi chiusi è chiusa
    \end{enumerate}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Sia
\[A=\bigcup_{i \in I} A_i\]
con $I$ insiemi di indici arbitrari tale che $A$ è aperto. Allora si osserva che
\[\forall x \in A, \exists i \in I \text{ tale che } x \in A_i\]
quindi
\[\exists r > 0 \text{ tale che } \mathcal{B}(x,r) \subset A_i \subset A\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Sia
\[A=A_1 \cap A_2\]
con $A_1$ e $A_2$ aperti. Allora $\forall x \in A$ si ha che $x \in A_1$ e $x \in A_2$.\\
Visto che $x \in A_1$
\[\exists r_1 > 0 \text{ tale che } \mathcal{B}(x,r_1) \subset A_1\]
Similmente, visto che $x \in A_2$
\[\exists r_2 > 0 \text{ tale che } \mathcal{B}(x,r_2) \subset A_2\]
Sarà ora sufficiente considerare $r=\min\{r_1,r_2\}$ per evincere che
\[\mathcal{B}(x,r) \subset A\]
per cui $A$ è aperto.

\vspace{1em}
\noindent
\textbf{Osservazione}: Nel caso di un numero infinito di insiemi, non può essere utilizzata tale dimostrazione, in quanto non è detto che esista il minimo dei raggi.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 3}: Sia
\[C = \bigcap_{i \in I} C_i\]
con $C_i$ aperto. Allora è ovvio che
\[X-C=\bigcup_{i \in I} X - C_i\]
che è ovviamente chiuso.
... continua ...

\vspace{2em}
\noindent
\textbf{Esercizio 1}: Si dimostri che esiste l'intersezione di aperti che non è aperto. È infatti possibile considerare
\[\left(\mathcal{B} \left(x,\frac{1}{n}\right)\right)_n\]
che corrisponde a
\[\bigcap_{n \in \mathcal{N}} \mathcal{B} \left(x,\frac{1}{n}\right) = \{x\}\]

\vspace{2em}
\noindent
\textbf{Esercizio 2}: È noto che $\mathbb{Q}$ è denso in $\mathbb{R}$, in quanto $\overline{\mathbb{Q}}$. Si deve provare che
\[\overline{\mathbb{Q} \times \mathbb{Q}} = \mathbb{R} \times \mathbb{R}\]
ovvero che ogni insieme aperto di $\mathbb{R}^2$ contiene un punto $\left(p,q\right){^T}$ con $p,q \in \mathbb{Q}$

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Sia dato un insieme $A$ è aperto. Allora si ha che
\[\forall (x,y){^T} \in A, \exists r > 0 \text{ tale che } \mathcal{B}((x,y){^T}, r) \subset A\]
per definizione stessa di aperto. Si consideri allora il quadrato inscritto nella palla presa in considerazione, i cui vertici sono $a$ e $b$, con $a < b$ e $c$ e $d$, con $c < d$. Ma per il teorema di densità di $\mathbb{Q}$ in $\mathbb{R}$ è noto che $\exists q \in ]a,b[ \cap \mathbb{Q}$ e, similmente, $\exists p \in ]c,d[ \cap \mathbb{Q}$.\\
Ciò dimostra che se un insieme $D$ è denso in $\mathbb{R}$, ovvero $\overline{D} = \mathbb{R}$, allora anche il prodotto cartesiano $D \times D$ è denso in $\mathbb{R} \times \mathbb{R}$, ovvero $\overline{D} \times \overline{D} = \mathbb{R} \times \mathbb{R}$.

\vspace{1em}
\subsection{Geometria di $\mathbb{R}^n$}
Si considerino di seguito alcune osservazioni in merito alla geometria in $\mathbb{R}^n$.

\vspace{1em}
\subsubsection{Retta nel piano $\mathbb{R}^2$}
È noto che una forma per rappresentare una retta in $\mathbb{R}^2$ è la seguente
\[ax+by+c=0\]
Tuttavia, in forma generale si ottiene che
\[a \cdot (x-x_0) + b \cdot (y-y_0) = 0\]
che può essere interpretata come
\[\left<\left(a,b\right){^T},\left(x-x_0,y-y_0\right){^T}\right> = 0\]
che equivale ad affermare che
\[\left(a,b\right){^T} \perp \left(x-x_0,y-y_0\right){^T}\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri un campo scalare del tipo
\[F(x,y) \leq a \cdot (x-x_0) + b \cdot (y-y_0)\]
allora una retta è l'insieme degli zeri di tali campo scalare, ovvero
\[Z_r = \{(x,y){^T} \in \mathbb{R}^2 \text{ tale che } F(x,y) = 0\}\]
In generale, tuttavia, l'insieme degli zeri di un campo scalare è una curva. Infatti, dato un campo scalare
\[F : A \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}\]
in cui l'insieme degli zeri sono 
\[Z_r = \{(x,y){^T} \in \mathbb{R}^2 \text{ tale che } F(x,y) = 0\}\]
e se
\[F(x,y) = x^2+y^2-1\]
allora l'insieme degli $0$ è una circonferenza. Se il campo scalare è da $\mathbb{R}^3$ in $\mathbb{R}$ e si considera
\[F(x,y,z) = x^2+y^2+z^2+1\]
allora questo è l'insieme vuoto $\varnothing$.

\vspace{1em}
\noindent
Data una retta in forma esplicita:
\[ax + by + c = 0\]
se si chiede $b \neq 0$ si può ottenere
\[y=-\frac{a}{b} x - \frac{c}{b}\]
che rappresenta una funzione
\[f : \mathbb{R} \longmapsto \mathbb{R}\]
in cui l'insieme delle soluzioni è il grafico della funzione $f$:
\[\mathcal{G}_f = \{(x,f(x)){^T} : x \in \mathbb{R}\}\]

\vspace{1em}
\noindent
Se si considera, ora, un campo scalare su $\mathbb{R}^3$, quale è il seguente
\[F(x,y,z) = ax+by+cz+d\]
in cui l'insieme delle soluzioni è
\[Z_F = \{(x,y,z){^T} \in \mathbb{R}^3 : F(x,y,z) = 0\}\]
per cui se $c \neq 0$ si può considerare
\[f : \mathbb{R}^2 \longmapsto \mathbb{R}\]
definita come
\[f(x,y)=-\frac{a}{c} x - \frac{b}{c} y - \frac{d}{c}\]
che, ovviamente, è un piano 
... continua ...

\vspace{1em}
\noindent
\subsection{Curva piana}
Per quanto esposto in precedenza, è possibile considerare una curva piana
\begin{itemize}
    \item come insieme degli zeri di un campo scalare da $\mathbb{R}^2$ in $\mathbb{R}$, della forma
    \[F : A \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}\]
    \item come grafico di una funzione
    \[f : E \subseteq \mathbb{R} \longmapsto \mathbb{R}\]
    \item in forma parametrica, della forma
    \[\gamma : I \subseteq \mathbb{R} \longmapsto \mathbb{R}^2\]
\end{itemize}
L'ultima modalità permette di fornire la definizione di \textbf{curva}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{CURVA}}\\
    \parbox{\linewidth}{Si chiama \textbf{curva in $\bf{\mathbb{R}^n}$} una \textbf{funzione continua}
    \[\gamma : I \subseteq \mathbb{R} \longmapsto \mathbb{R}^n\]
    con $I$ un intervallo. \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio}: La classica curva
\[\gamma : [0,2\pi] \longmapsto \mathbb{R}^2\]
con
\[\gamma(t) = \left(\cos(t),\sin(t)\right){^T}\]
è la circonferenza.

\vspace{1em}
\noindent
\subsubsection{Sostegno di una curva}
L'insieme $\gamma(I) \subset \mathbb{R}^n$ si dice sostegno della curva, ovvero l'insieme immagine.

\vspace{1em}
\noindent
\textbf{Esempio}: La curva seguente
\[\gamma : [0,4\pi] \longmapsto \mathbb{R}^3\]
definita come
\[\gamma(t) = \left(\cos(t),\sin(t),t\right){^T}\]
è un'elica.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che curve diverse possono presentare il medesimo sostegno. Per esempio
\[f : \mathbb{R} - \{0\} \longmapsto \mathbb{R}^3 \hspace{1em} \text{ con } \hspace{1em} f(t)=\left(\frac{1}{t^2},1\right){^T}\]
e
\[g : \mathbb{R} \longmapsto \mathbb{R}^3 \hspace{1em} \text{ con } \hspace{1em} g(t)=\left(e^t,1\right){^T}\]
presentano il medesimo sostegno.

\vspace{1em}
\noindent
\subsection{Superficie parametrica in $\mathbb{R}^3$}
Per quanto esposto in precedenza, è possibile considerare una superficie piana
\begin{itemize}
    \item come insieme degli zeri di un campo scalare da $\mathbb{R}^3$ in $\mathbb{R}$, della forma
    \[F : A \subseteq \mathbb{R}^3 \longmapsto \mathbb{R}\]
    \item come grafico di una funzione
    \[f : E \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}\]
    \item in forma parametrica, della forma
    \[\gamma : I \subseteq \mathbb{R} \longmapsto \mathbb{R}^3\]
\end{itemize}
L'ultima modalità permette di fornire la definizione di \textbf{superficie parametrica}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{CURVA}}\\
    \parbox{\linewidth}{Si chiama \textbf{superficie parametrica in $\bf{\mathbb{R}^3}$} una \textbf{funzione}
    \[\gamma : \Omega \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}^3\]
    con
    \[\gamma(s,t) = \left(x(s,t),y(s,t),z(s,t)\right){^T}\]
    in cui il sostegno della superficie è l'insieme immagine $\gamma(\Omega) \subset \mathbb{R}^3$ \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\subsubsection{Retta in $\mathbb{R}^2$ in forma parametrica}
Una retta in $\mathbb{R}^2$ viene parametrizzata tramite la seguente funzione $\gamma$:
\[\gamma : I \subseteq \mathbb{R} \longmapsto \mathbb{R}^2 \hspace{1em} \text{con} \hspace{1em} y(t) (x_0,y_0){^T} + t \cdot (a,b){^T}\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si determini la retta passante per i punti $(0,2){^T}$ e $(1,0){^T}$. Un modo per determinarne l'equazione è quello di considerare il sistema di equazioni:
\[\left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
        a \cdot (x-0) + b \cdot (y-2) = 0 \rightarrow ax + by = 2b\\
        a \cdot (x-1) + b \cdot (y-0) = 0 \rightarrow ax + by = a\\
    \end{array}
\right.\]
per cui è immediato capire che $a=2b$, per cui l'equazione cercata è
\[2bx + by = 2b \rightarrow 2x + y = 2\]
Un altro metodo prevederebbe di considerare la funzione
\[y=f(x)=mx+q\]
ottenendo le due equazioni seguenti:
\[\left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
        0=m \cdot 1 + q\\
        2=m \cdot 0 + q
    \end{array}
\right.\]
per cui è immediato evincere che $q=2$ e $m=-2$. Pertanto si ottiene ancora l'equazione
\[y=-2x+2\]
Se, invece, si volesse impiegare la forma parametrica, si potrebbe considerare il coefficiente angolare
\[\vec v = P_2-P_1=(1,-2){^T}\]
per cui si ottiene 
\[\gamma(t) = (0,2){^T} + t \cdot (1,-2){^T} = (t,2-2t)^{T}\]
considerando il primo punto. Altrimenti si sarebbe potuto considerare il secondo punto, ottenendo:
\[\gamma(t) = (1,0){^T} + t \cdot (1,-2){^T} = (t+1,-2t)^{T}\]

\vspace{2em}
\noindent
\subsubsection{Piano in $\mathbb{R}^3$ in forma parametrica}
Un piano in $\mathbb{R}^3$ può essere descritto come
\[\gamma(s,t) = (x_0,y_0,z_0){^T} + s \cdot (a_1,a_2,a_3){^T} + t \cdot (b_1,b_2,b_3){^t}\]

\vspace{1em}
\noindent
\textbf{Esempio}: Dati i seguenti punti
\[(1,0,0){^T} \hspace{1em} (0,2,0){^T} \hspace{1em} (0,0,3){^T}\]
Al fine di scrivere la giacitura del piano, si considerano due coppie di vettori linearmente indipendenti e si ottiene
\[a=P_2-P_1=(-1,2,0){^T} \hspace{1em} \text{e} \hspace{1em} b=P_3-P_1=(-1,0,3){^T}\]
per cui l'equazione parametric del piano cercata è
\[\gamma(s,t)=(1,0,0){^T} + s \cdot (-1,2,0){^T} + t \cdot (-1,0,3){^T}\]

\vspace{1em}
\noindent
\subsubsection{Sfera}
Si consideri una sfera in $\mathbb{R}^3$, come quella considerata:
\[x^2+y^2+z^2=1\]
allora tale sfera non può essere rappresentata come grafico da $\mathbb{R}^2$ in $\mathbb{R}$, in quanto non è un grafico. Al limite si può considerare come l'unione di due grafici, quali
\[f_1=-\sqrt{1-x^2-y^2} \hspace{1em} \text{e} \hspace{1em} f_2=\sqrt{1-x^2-y^2}\]
per descrivere, invece, la sfera tramite equazioni parametriche, ossia con una funzione
\[\gamma : A \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}^3 \hspace{1em} \text{con} \hspace{1em} \gamma=\gamma(\phi,\theta)\]
ottenendo
\begin{itemize}
    \item $x(\phi,\theta)=\sin(\phi) \cdot \cos(\theta)$
    \item $y(\phi,\theta)=\sin(\phi) \cdot \sin(\theta)$
    \item $z(\phi,\theta)=\cos(\theta)$
\end{itemize}

\newpage
\noindent
\begin{center}
    21 Ottobre 2022
\end{center}

\vspace{1em}
\noindent
\textbf{Esercizio 1}: Si consideri la serie seguente
\[\sum_{n=1}^{+\infty} (-1)^n \cdot \log \left(1+\frac{1}{n}+\frac{1}{\sqrt{n}}\right)\]
Allora per il criterio di Leibniz si ha che
\begin{itemize}
    \item il termine $a_n$ è sempre positivo, ovvero $a_n>0$
    \item il termine $a_n$ è infinitesimo
    \item il termine $a_n$ è anche decrescente, in quanto composta di una funzione crescente con una decrescente.
\end{itemize}

\vspace{1em}
\noindent
\textbf{Esercizio 2}: Si consideri la serie seguente
\[\sum_{n=0}^{+\infty} \frac{i^n \cdot \left(\sqrt{n} - \sqrt{n-1}\right) + i^{2n} \cdot \sqrt{n+1}}{n}\]
Allora è possibile spezzare la serie, ottenendo
\[\sum_{n=0}^{+\infty} \frac{i^n \cdot \left(\sqrt{n} - \sqrt{n-1}\right)}{n} + (-1)^n \cdot \frac{\sqrt{n+1}}{n}\]
in cui è immediato evincere che
\[\sum_{n=1}^{+\infty} (-1)^n \cdot \frac{\sqrt{n+1}}{n}\]
è ovviamente convergente per Leibniz (ma non è assolutamente convergente).\\
Il primo termine, invece, può essere scomposto come segue:
\[\frac{\left(\sqrt{n} - \sqrt{n-1}\right)}{n} \cdot \frac{\sqrt{n} + \sqrt{n-1}}{\sqrt{n} + \sqrt{n-1}}\]
pertanto si ottiene che
\[\dfrac{1}{n \cdot \sqrt{n} \cdot \left(1+\sqrt{1-\dfrac{1}{n}} \right)}\]
che è un infinitesimo di ord $\dfrac{3}{2}>1$. Ciò implica il fatto che
\[\left \vert \frac{i^n \cdot \left(\sqrt{n} - \sqrt{n-1}\right)}{n} \right \vert =  \frac{\left(\sqrt{n} - \sqrt{n-1}\right)}{n}\]
che è assolutamente convergente, quindi è convergente. Se ne conclude che la funzione di partenza è convergente semplicemente, ma non assolutamente.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che una retta può essere interpretata come
\begin{itemize}
    \item insieme degli zeri di un campo scalare da $\\mathbb{R}^2$ a $\mathbb{R}$
    \item grafico di una funzione
    \item curva parametrica
\end{itemize}
In generale, se si considera una funzione
\[f : \mathbb{R}^n \longmapsto \mathbb{R}^m\]
si possono distinguere due casistiche
\begin{itemize}
    \item se $m=1$ allora $f$ è un campo scalare della forma
    \[f \left((x_1,\dots,x_n){^T}\right) = f(x_1,\dots,x_n)\]
    
    \item se $n=1$ e $m\geq 2$, allora $f$ è una curva
    \item se $n\geq 21$ e $m\geq 2$, allora $f$ viene definita \textbf{campo vettoriale}. In particolare, se $n=2$ e $m=3$, $f$ è una superficie. Se si considera
    \[f(x_1,\dots,x_n) = \left(f_1(x_1,\dots,x_n),\dots,f_m(x_1,\dots,x_n) \right){^T}\]
    che sono proprio le componenti del campo vettoriale.
\end{itemize}

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la funzione
\[f(x,y,z) = \left(\frac{\sin(x,z)}{x-y}, \log(x+z)\right){^T}\]
Allora il dominio di tale funzione deve essere l'intersezione del dominio di tutte e due le funzioni, ovvero
\[
    \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
        x-y \neq 0\\
        x+z > 0
    \end{array}
    \right.
\]

\vspace{1em}
\noindent
\subsection{Rappresentazione grafica in $\mathbb{R}^n$}
Si considerino i seguenti esempi di rappresentazione grafica di alcune curve nello spazio:
\begin{itemize}
    \item sia data la funzione $f=2x^2+y^2$, allora il suo grafico viene definito come
    \[\mathcal{G}_f = \{(x,y,f(x,y)){^T} : (x,y) \in \mathbb{R}^2\}\]
    che, per essere rappresentato può essere scomposto nelle sue proiezioni $xy$, $yz$ e $xy$.
    \begin{itemize}
        \item Sul piano $xz$ la funzione da considerare è $z=2x^2$, che è una parabola più ripida del normale;
        \item Sul piano $yz$ la funzione da considerare è $z=y^2$, che è una parabola normale;
        \item Sul piano $xy$ la funzione da considerare è $0=2x^2+y^2$, che è solamente il punto $(0,0){^T}$.
    \end{itemize}

    \item sia data la funzione $f=\dfrac{1}{x+y}$, allora il suo dominio non è più tutto $\mathbb{R}^2$, ma è l'insieme dei punti per cui $x \neq y$. Il grafico della funzione è
    \[\mathcal{G}_f = \{(x,y,f(x,y)){^T} : x \neq y\}\]
    Esso, per essere rappresentato può essere scomposto nelle sue proiezioni $xy$, $yz$ e $xy$.
    \begin{itemize}
        \item Sul piano $xz$ la funzione da considerare è $z=\dfrac{1}{x}$, che è un'iperbole;
        \item Sul piano $yz$ la funzione da considerare è $z=\dfrac{1}{y}$, che è un'iperbole.
        \item Sul piano $xy$ la funzione da considerare è $0=\dfrac{1}{x+y}$, che si ha solamente quando $x$ o $y$ sono infiniti.
    \end{itemize}
\end{itemize}

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che in tutti i casi precedenti, risulta molto difficile capire la rappresentazione grafica delle funzioni. Se, però, si considerasse come funzione $h(x,y)$ la quota sul livello del mare di un monte, si possono impiegare le \textbf{curve di livello}, chiamate anche \textbf{isoipse}.

\vspace{1em}
\noindent
\subsection{Insieme di livello}
Di seguito si fornisce la definizione di \textbf{insieme di livello}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{INSIEME DI LIVELLO}}\\
    \parbox{\linewidth}{Sia $f : E \subseteq \mathbb{R}^n \longmapsto \mathbb{R}$ e sia $\alpha \in \mathbb{R}$. Si chiama, allora, \textbf{insieme di livello} $\alpha$ di $f$ l'insieme
    \[\boxed{L_\alpha = \{x \in E : f(x)=\alpha\}}\]
    ovvero l'insieme degli zeri del campo scalare $Z_{f-\alpha}$.
    \begin{itemize}
        \item Con $n=2$, l'insieme $L_\alpha$ è una curva (appunto, la curva di livello)
        \item Con $n=3$, l'insieme $L_\alpha$ è una superficie.
    \end{itemize}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio 1}: Nel primo esempio, posto
\[f(x,y)=2x^2+y^2\]
allora l'insieme di livello è
\[L_\alpha = \{(x,y) \in \mathbb{R}^2 : 2x^2+y^2=\alpha\}\]
che può essere riscritto come
\[\frac{x^2}{\left(\sqrt{\frac{\alpha}{2}} \right)^2} + \frac{y^2}{(\sqrt{\alpha})^2}=1\]
che permette di capire come
\[a=\sqrt{\frac{\alpha}{2}} \hspace{1em} \text{e} \hspace{1em} b=\sqrt{\alpha}\]
che permette di capire come il semiasse orizzontale è minore di quello verticale.

\vspace{2em}
\noindent
\textbf{Esempio 2}: Nel secondo esempio, posto
\[f(x,y)=\frac{1}{x+y}\]
naturalmente si ha sempre $x \neq y$ come campo di esistenza. L'insieme di livello, invece, è
\[L_\alpha = \{(x,y) \in \mathbb{R}^2 : \frac{1}{x+y}=\alpha\}\]
In cui è immediato evincere che
\begin{itemize}
    \item se $\alpha$ diviene $>>1$, allora la retta sul piano $xy$ che si ottiene è sempre più distante dall'origine.
    \item se $\alpha$ diviene $<<1$, allora la retta sul piano $xy$ che si ottiene è sempre più vicina all'origine.
    \item se $\alpha<0$, allora si ottiene la medesima rappresentazione dei primi due punti, ma simmetrica rispetto all'asse $y=-x$ 
\end{itemize}

\newpage
\section{Funzioni tra spazi metrici}
Si consideri una funzione
\[f : E \subseteq X_1 \longmapsto X_2\]
avente delle metriche $(X_1,d_1)$ e $(X_2,d_2)$.

\vspace{1em}
\noindent
\subsection{Limite di una funzione}
Di seguito si espone la definizione di \textbf{limite di una funzione}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{LIMITE DI UNA FUNZIONE}}\\
    \parbox{\linewidth}{Siano $(X_1,d_1)$ e $(X_2,d_2)$ due spazi metrici; si consideri
    \[f : E \subseteq X_1 \longmapsto X_2\]
    Sia $x_1 \in X_1$ un punto di accumulazione per $E$, con $l \in X_2$. Allora si dira che
    \[\lim_{x \to x_1} f(x) = l\]
    se
    \[\forall \epsilon>0, \exists \delta>0 \text{ tale che } \forall x \in E, x \neq x_1, d_1(x,x_1) < \delta \text{ si ha che } d_2(f(x),l) < \epsilon\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Osservazione 1}: In particolare, sulla base della definizione di limite di cui sopra, $f$ è \textbf{continua} in $X_1$ se
\[\lim_{x \to x_1} f(x) = f(x_1)\]
ma solamente se $x_1 \in E$ e $x_1$ è punto di accumulazione. In generale, infatti, $x_1$ può anche essere un punto isolato, per cui la definizione generale richiederebbe che
\[\forall \epsilon>0, \exists \delta>0 \text{ tale che } \forall x \in E, d_1(x,x_1) < \delta \text{ si ha che } d_2(f(x),l) < \epsilon\]

\vspace{1em}
\noindent
\textbf{Osservazione 2}: In particolare, sulla base della definizione di limite di cui sopra, $f$ è \textbf{uniformemente continua} in $X_1$ se
\[\forall \epsilon>0, \exists \delta>0 \text{ tale che } \forall x_1,y_1 \in X_1, d_1(x_1,y_1) < \delta \text{ si ha che } d_2(f(x_1),f(x_2)) < \epsilon\]

\vspace{1em}
\noindent
\subsection{Continuità della norma}
Di seguito si espone il teorema sulla \textbf{contonuità della norma}:

\begin{theorem}
    Sia $(X,d)$ uno spazio metrico con $d$ indotta da una norma $\left \vert \left \vert \cdot \right \vert \right \vert$. Allora la funzione
    \[f : X \longmapsto \mathbb{R}\]
    definita da $f(x) = \left \vert \left \vert x \right \vert \right \vert$ è continua.
\end{theorem}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si provi che
\[\forall \epsilon > 0, \exists \delta > 0 \text{ tale che } \left \vert \left \vert x-x_0 \right \vert \right \vert < \delta \text{ allora } \left \vert f(x)-f(x_0)\right \vert < \epsilon\]
ovvero che
\[\left \vert \left \vert \left \vert x \right \vert \right \vert - \left \vert \left \vert x_0 \right \vert \right \vert \right \vert\]
Ciò è evidente in quanto
\begin{itemize}
    \item $\left \vert \left \vert x \right \vert \right \vert = \left \vert \left \vert x-x_0+x_0 \right \vert \right \vert \leq \left \vert \left \vert x-x_0 \right \vert \right \vert + \left \vert \left \vert x_0 \right \vert \right \vert$
    \item $\left \vert \left \vert x_0 \right \vert \right \vert = \left \vert \left \vert x_0-x+x \right \vert \right \vert \leq \left \vert \left \vert x_0-x \right \vert \right \vert + \left \vert \left \vert x \right \vert \right \vert$
\end{itemize}
Ma ciò implica quanto si voleva dimostrare.

\vspace{1em}
\noindent
\subsection{Limite delle componenti}
Di seguito si espone il teorema sul \textbf{limite delle componenti}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{LIMITE DELLE COMPONENTI}}\\
    \parbox{\linewidth}{Sia $(X_1,d_1)$ uno spazio metrico, con
    \[f=(f_1,\dots,f_m){^T} : E \subseteq X_1 \longmapsto \mathbb{R}^m\]
    con $x_1 \in X_1$ punto di accumulazione per $E$ e
    \[l = (l_1,\dots,l_m){^T} \in \mathbb{R}^m\]
    Si ha, allora, che
    \[\lim_{x \to x_0} f(x) = l\]
    \textbf{se e solo se}
    \[\lim_{x \to x_k} f_k(x) = l_k \hspace{1em} \text{per ogni } k =1,2,\dots,m\]
     \vspace{-1mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Si operi con $m=2$. Si supponga, per ipotesi che
\[\lim_{x \to x_k} f_k(x) = l_k \hspace{1em} \text{per ogni } k =1,2,\dots,m\]
e si dimostri che
\[\lim_{x \to x_0} f(x) = l\]
Allora, per definizione dei due limiti seguenti
\[\lim_{x \to x_0} f_1(x) = l_1 \hspace{1em} \text{e} \hspace{1em} \lim_{x \to x_0} f_2(x) = l_2\]
è noto che
\begin{align*}
    &\forall \epsilon>0, \exists \delta_1 > 0, x \neq x_0, d_1(x,x_0) < \delta_1 \text{ si ha che } \left \vert f(x) - l_1 \right \vert < \epsilon
    &\forall \epsilon>0, \exists \delta_2 > 0, x \neq x_0, d_2(x,x_0) < \delta_2 \text{ si ha che } \left \vert f(x) - l_2 \right \vert < \epsilon
\end{align*}
Allora si ha che, preso $\delta = \min\{\delta_1,\delta_2\}$ (che è un passo fondamentale, in quanto non sarebbe vera per $m$ infinito), si ha che
\[\left \vert \left \vert \cdot \right \vert \right \vert\]
... continua ...

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Si operi con $m=2$. Si supponga, per ipotesi che
\[\lim_{x \to x_0} f(x) = l \hspace{1em} \text{ovvero} \hspace{1em} \left \vert \left \vert (f_1,f_2){^T}(x) - (l_1,l_2){^T} \right \vert \right \vert < \epsilon\]
allora si ha che
... continua ...

\vspace{1em}
\noindent
\subsection{Successione in $\mathbb{R}^n$}
Di seguito si espone la definizione di \textbf{successione in $\mathbb{R}^n$}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SUCCESSIONE}}\\
    \parbox{\linewidth}{Una successione $(x_n)_n$ in uno spazio metrico $(X,d)$ è una funzione
    \[f : E \subseteq \mathbb{N} \longmapsto X\]
    con $E$ infinito.
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione}: Il limite di una successione
\[\lim_{n \to +\infty} x_n = l\]
con 

\vspace{2em}
\noindent
\subsection{Caratterizzazione del limite di una funzione usando le successioni}
Si espone di seguito il \textbf{teorema di caratterizzazione del limite di una funzione usando le successioni}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{CARATTERIZZAZIONE DEL LIMITE DI UNA FUNZIONE USANDO LE SUCCESSIONI}}\\
    \parbox{\linewidth}{Siano $(X_1,d_1)$ e $(X_2,d_2)$ due spazi metrici; si consideri
    \[f : E \subseteq X_1 \longmapsto X_2\]
    Sia $\alpha \in X_1$ un punto di accumulazione per $E$, con $l \in X_2$. Si ha, allora, che
    \[\lim_{x \to \alpha} f(x) = l\]
    \textbf{se e solo se, per ogni successione} $(x_n)_n$ in $X_1$ tale che
    \[\lim_{n \to +\infty} x_n = \alpha\]
    si ha che
    \[\lim_{n \to +\infty} f(x_n) = l\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Si supponga che
\[\lim_{x \to \alpha} f(x) = l\]
Si consideri, allora, una successione $(x_n)_n$ una successione tale per cui
\[\lim_{n \to +\infty} x_n = \alpha\]
e si dimostri che 
\[\lim_{n \to +\infty} f(x_n) = l\]
Fissato $\epsilon>0$, allora per la definizione di limite di cui sopra, si ha che
\[\exists \delta > 0 \text{ tale che } \forall x \in E, x \neq \alpha, d_1(x,x_0) < \delta \text{ tale che } d_2(f(x),l) < \epsilon\]
Dal momento che, per ipotesi, si ha che
\[\lim_{n \to +\infty} x_n = \alpha\]
per definizione di limite
\[\exists n_\epsilon \in \mathbb{N} \text{ tale che } \forall n \geq n_\epsilon \text{ si ha che } d_1(x_n,\alpha) < \delta\]
e, di conseguenza,
\[d_2(f(x),l) < \epsilon\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Si supponga, per ipotesi, che per ogni successione $(x_n)_n$ in $X_1$ tale che
\[\lim_{n \to +\infty} x_n = \alpha\]
si ha che
\[\lim_{n \to +\infty} f(x_n) = l\]
e si dimostri che
\[\lim_{x \to \alpha} f(x) = l\]
Dalla definizione dell'ultimo limite si ha che
\[\forall \epsilon > 0, \exists \delta > 0 \text{ tale che } \forall x \in E, x \neq \alpha, d_1(x,\alpha)<\delta, d_2(f(x),l) < \epsilon\]
Si proceda per assurdo, e si neghi tale affermazione
\[\exists \epsilon > 0, \forall \delta = \frac{1}{n} > 0, \exists x_n \in E, x_n \neq \alpha, \text{ tale che } d_1(x,\alpha)<\delta=\frac{1}{n} \text{ ma } d_2(f(x),l) \geq \epsilon\]
in cui si è assunto che $\delta=\frac{1}{n}$, per comodità. Ciò consente di generare, implicitamente, una successione che permetterà di ottenere l'assurdo. Infatti, avendo ottenuto che
\[d_1(x,\alpha)<\delta=\frac{1}{n} \hspace{1em} \text{ è immediato evincere che } \hspace{1em} \lim_{n \to +\infty} x_n = \alpha\]
Ma siccome per ipotesi è noto che ogni successione $(x_n)_n$ in $X_1$ tale che
\[\lim_{n \to +\infty} x_n = \alpha\]
si ha che
\[\lim_{n \to +\infty} f(x_n) = l\]
Pertanto si conclude l'assurdo, perché si era assunto che 
\[d_2(f(x),l) \geq \epsilon\]

\vspace{1em}
\noindent
\textbf{Esercizio}: Si consideri lo spazio delle funzioni continue su $[0,1]$ a valori reali: $X=C^0([0,1])$.\\
Per ogni $n \in \mathbb{N}^+$ si consideri la funzione $\phi_n \in X$ definita da
\[
    \phi_n(t) = \rowcolors{1}{white}{white}    
    \begin{array}{lll}
        1-nt & \text{se} & t \in \left[0,\dfrac{1}{n}\right] \geq 0\\
        0   & \text{se} & t \in \left[\dfrac{1}{n},1\right] \geq 0\\
    \end{array}
\]
Si discuta l'eventuale convergenza della successione $(\phi_n)_n$ nello spazio metrico $(X,d_1)$ e nello spazio metrico $(X,d_\infty)$, dove $d_1$ è la metrica indotta dalla norma $\left \vert \left \vert \cdot \right \vert \right \vert_1$ e $d_\infty$ è la metrica indotta dalla norma $\left \vert \left \vert \cdot \right \vert \right \vert_\infty$.\\
Essendo la norma $1$ seguente $\left \vert \left \vert \cdot \right \vert \right \vert_1$ denotata con
\[\left \vert \left \vert \phi \right \vert \right \vert_1 = \int_0^1 \left \vert \phi(t) \right \vert \dif t\]
Dalla rappresentazione grafica è facile intuire che il limite di tale successione sia $0$, ovvero
\[\forall \epsilon > 0, \exists n_\epsilon, \forall n \geq n_\epsilon, \int_0^1 \left \vert \phi(t) \right \vert \dif t < \epsilon\]
ma siccome la funzione $\phi(t)$ è nulla da $\dfrac{1}{n}$ a $1$, basta considerare
\[\int_0^1 \left \vert \phi(t) \right \vert \dif t = \int_0^\frac{1}{n} \left \vert \phi(t) \right \vert \dif t + 0 = \left[t-\frac{1}{2}nt^2\right]_0^\frac{1}{n} = \frac{1}{n} - \frac{1}{2n} = \frac{1}{2n} < \epsilon\]
... continua ...

\vspace{2em}
\noindent
\textbf{Esercizio}: Si consideri la funzione
\[f : C([0,1]) \longmapsto \mathbb{R}\]
con norma infinito. Si provi che la funzione
\[f : X \longmapsto \mathbb{R}\]
definita da
\[f(\phi) = \int_0^1 \phi(t) \dif t\]

\vspace{1em}
\noindent
Si dimostri la continuità in $\phi_0$, ovvero
\[\forall \epsilon > 0, \exists \delta > 0 \text{ tale che } \forall \phi \text{ con } \left \vert \left \vert \phi-\phi_0 \right \vert \right \vert_\infty < \delta, \left \vert f(\phi) - f(\phi_0) \right \vert < \epsilon\]
Ma in particolare, per la definizione stessa della $f$ si ha che
\[\left \vert \int_0^1 \phi(t) \dif t - \int_0^1 \phi_0(t) \dif t\right \vert = \left \vert \int_0^1 (\phi(t) - \phi_0(t)) \dif t \right \vert\]
ma siccome
\[\left \vert \phi(t) - \phi_0(t) \right \vert \leq \left \vert \left \vert \phi(t) - \phi_0(t) \right \vert \right \vert_{\infty}\]
in quanto, per definizione $\left \vert \left \vert \phi(t) - \phi_0(t) \right \vert \right \vert_{\infty} = \sup\{\left \vert \phi-\phi_0 \right \vert\}$
si conclude che 
\[\left \vert \int_0^1 \phi(t) \dif t - \int_0^1 \phi_0(t) \dif t\right \vert \leq \left \vert \left \vert \phi(t) - \phi_0(t) \right \vert \right \vert_{\infty} \cdot \int_0^1 \dif t = \delta < \epsilon\]

\newpage
\noindent
\begin{center}
    21 Ottobre 2022
\end{center}
\textbf{Esercizio}: Si considerino i due spazi metrici $X_1 = C^{-1}([0,1])$ e $X_2 = C^0([0,1])$ in cui si definisce in ambo i spazi la norma $\vert \vert \cdot \vert \vert_{\infty}$. Si considera, allora, la funzione
\[f : X_1 \longmapsto X_2 \hspace{1em} \text{e} \hspace{1em} f(\phi) = \phi'\]
Si vuole dimostrare che la funzione $f$ considerata non è continua. È noto che una funzione continua se
\[\lim_{\phi \to \phi_0} f(\phi) = f(\phi_0)\]
Per il teorema di caratterizzazione del limite di una funzione tramite le successioni, si ha che
\[f \text{ è in continua in } \alpha \textbf{ se e solo se } \forall (f_n)_n \in X_1, \text{ con } \lim_{n \to +\infty} \alpha_n = \alpha \text{ si ha che } \lim_{n \to +\infty} f(\alpha_n) = f(\alpha)\]
Se si considera la funzione
\[\phi_n=\frac{1}{n} \cdot \sin(n x) \text{ tale che } \lim_{n \to + \infty} \phi_n = 0\]
e, ovviamente, si ha che
\[f(\phi_n)=\cos(n x)\]
Tuttavia è ovvio che
\[\lim_{n \to +\infty} f(\phi_n) = \lim_{n \to +\infty} \cos(nx) = \nexists\]
che permette di concludere che tale funzione, con tale norma, non è continua.

\vspace{1em}
\noindent
\textbf{Osservazione}: L'ultima considerazione è evidente, in quanto la norma non è concorde con la tipologia di funzione e con gli spazi metrici analizzati. La norma ...

\vspace{1em}
\noindent
\textbf{Esempio}: Si considerino gli spazi metrici $X=C([0,1])$ con $\vert \vert \cdot \vert \vert_1$ e $Y=C([0,1])$ con $\vert \vert \cdot \vert \vert_{\infty}$.\\
Per verificare se la funzione $f : X \longmapsto Y$, con $f(\phi)=\phi$ è continua, è facile capire se si considera una successione che vale praticamente sempre $0$ e vale $1$ in un solo punto, allora per la norma infinito il valore è $1$, mentre per la norma $1$ il valore dell'integrale è $0$, per cui non può essere continua.\\
Nel caso, invece, della funzione inversa $f^{-1} : Y \longmapsto x$, con $f(\phi)=\phi$ è continua, in quanto data ...continua..

\vspace{1em}
\noindent
\subsection{Teorema sui limiti e sulle funzioni continue}
Di seguito si espongono alcuni fondamentali teoremi sui limiti e sulle funzioni continue.

\vspace{1em}
\noindent
\subsubsection{Teorema di unicità del limite}
Per la dimostrazione si impiega il principio di separazione di Hausdorff.

\vspace{1em}
\noindent
\subsubsection{Teorema sul limite delle restrizioni}
È noto che data una funzione
\[f : E \subseteq X \longmapsto X\]
in cui
\[\lim_{x \to x_0} f(x) = l\]
allora, posto $F \subseteq E$ una restrizione di $E$, con $x_0$ punto di accumulazione per $F$. Allora anche
\[\lim_{x \to x_0} f\vert_F(x)=l\]

\vspace{2em}
\noindent
\textbf{Esercizio}: Si consideri il limite seguente
\[\lim_{(x,y){^T} \to (0,0){^T}} \frac{x \cdot y}{x^2 + y^2}\]
Allora per risolvere tale limite, si può considerare come restrizione l'asse $x$, ponendo
\[F = \{(x,y){^T}, y=0\} - \{0,0\}\]
Allora se esiste il limite di partenza, esso deve essere uguale a
\[\lim_{(x,y){^T} \to (0,0){^T}, y=0} \frac{x \cdot 0}{x^2 + 0^2} = 0\]
Anche la restrizione sull'asse $y$ avrebbe fornito il medesimo risultato. Tuttavia, se si considera la restrizione sulla bisettrice si ottiene:
\[\lim_{(x,y){^T} \to (0,0){^T}, x=y} \frac{x^2}{2x^2} = \frac{1}{2}\]
per cui si sono trovate due restrizioni in cui il limite non coincide: il limite di partenza non esiste.

\vspace{1em}
\noindent
\subsubsection{Teorema sul limite della funzione composta}
Si considerino due funzioni
\[f : E \subseteq X_1 \longmapsto X_2 \hspace{1em} \text{e} \hspace{1em} g : F \subseteq X_2 \longmapsto X_3\]
Posto $\alpha$ punto di accumulazione per $E$, tale per cui
\[\lim_{x \to \alpha} f(x) = \beta\]
e sia $\beta$ punto di accumulazione per $F$. Allora se
\[\lim_{y \to \beta} g(y) = \beta\]
si evince che
\[\lim_{x \to \alpha} g(f(x)) = \gamma\]
ammesso che \textbf{esista un intorno } $U$ \textbf{ di } $\alpha$ \textbf{ tale che } $\forall x \in U, x \neq \alpha, f(x) \neq \beta$

\vspace{1em}
\noindent
\subsubsection{Teorema sul limite della combinazione lineare di funzioni}
Se $X$ è uno spazio metrico, con $Y$ \textbf{spazio vettoriale} con una norma $\vert \vert \cdot \vert \vert$ e la distanza indotta
\[d(y_1,y_2) = \left \vert \left \vert y_1-y_2 \right \vert \right \vert\]
si ha che
\[\lim_{x \to x_0} f(x) = l_1 \hspace{1em} \text{e} \hspace{1em} \lim_{x \to x_0} g(x) = l_1\]
posto $\alpha, \beta \in \mathbb{R}$ si ha che
\[\lim_{x \to x_0} \left(\alpha \cdot f + \beta \cdot g\right)(x) = \alpha \cdot l_1 + \beta \cdot l_2\]
Inoltre, se $Y=\mathbb{R}$, o uno spazio metrico su cui è definito un prodotto, si può anche affermare che
\[\lim_{x \to x_0} (f \cdot g)(x) = l_1 \cdot l_2\]
Ha anche significato affermare, in $\mathbb{R}$, che
\[\lim_{x \to x_0} f(x) = \pm \infty\]
che significa ...continua...

\vspace{1em}
\noindent
\textbf{Esercizio 1}: Si calcoli il seguente limite:
\[\lim_{(x,y){^T} \to (0,0){^T}} \left(\frac{\sin(xy)}{y}, \frac{3x^3y-xy^2+1}{xy+2x-y-2}\right){^T}\]
Siccome si tratta di un campo vettoriale, bisogna suddividere lo stesso nelle sue due componenti, andando a studiare separatamente i due limite. Se ciascuno esiste, il limite del campo vettoriale sarà il vettore con componenti il limite delle componenti.\\
Pertanto si andranno a studiare
\[\lim_{(x,y){^T} \to (0,0){^T}} \frac{\sin(xy)}{y} \hspace{1em} \text{e} \hspace{1em} \lim_{(x,y){^T} \to (0,0){^T}} \frac{3x^3y-xy^2+1}{xy+2x-y-2}\]
È immediato evincere che il secondo limite sia, ovviamente
\[\lim_{(x,y){^T} \to (0,0){^T}} \frac{3x^3y-xy^2+1}{xy+2x-y-2}=-\frac{1}{2}\]
Per quanto riguarda il primo limite, si devono considerare delle restrizioni opportune, per esempio
\begin{itemize}
    \item Considerando la restrizione all'asse $x$ si ottiene
    \[\lim_{(x,y){^T} \to (0,0){^T}, x=0} \frac{\sin(xy)}{y} = 0\]
    \item Considerando la restrizione sulla bisettrice si ottiene
    \[\lim_{(x,y){^T} \to (0,0){^T}, x=y} \frac{\sin(xy)}{y} = 0\]
\end{itemize}
Ciò suggerisce che il limite possa esistere; basterà andare a stimare che
\[\left \vert \frac{\sin(xy)}{y} \right \vert < \epsilon\]
ma è immediato osservare che 
\[\left \vert \frac{\sin(xy)}{y} \right \vert = \left \vert \frac{\sin(xy)}{xy} \right \vert \cdot \vert x \vert \leq \vert x \vert\]
e siccome $\vert x \vert \to 0$, si evince che tale funzione è proprio nullo. Pertanto il limite del campo è il campo con componenti il limite del componenti:
\[\lim_{(x,y){^T} \to (0,0){^T}} \left(\frac{\sin(xy)}{y}, \frac{3x^3y-xy^2+1}{xy+2x-y-2}\right){^T} = \left(0,-\frac{1}{2}\right){^T}\]

\vspace{1em}
\noindent
\textbf{Esercizio 2}: Si consideri il seguente limite:
\[\lim_{(x,y){^T} \to (0,0){^T}} \frac{x^2y}{x^4+y^2}\]
Se esiste il limite, esso deve essere $0$, in quanto basta lavorare sugli assi e si ottiene $0$. Anche se si lavora sulla bisettrice si ottiene $0$.\\
Tuttavia, se si considera la parabola $y=x^2$, è facile capire che
\[\lim_{(x,y){^T} \to (0,0){^T},y=x^2} \frac{x^4}{2x^4}=\frac{1}{2}\]
per cui si evince che tale limite non esiste.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che negli spazi $X=C([0,1])$ con norma $\vert \vert \cdot \vert \vert_\infty$ si lavora con successioni di funzioni $\phi_n$ che convergono ad una funzione $\phi$ con distanza indotta dalla norma infinito. Allora la convergenza per tali successioni viene definita come
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \text{ tale che } \forall n \in n_\epsilon, \underset{x \in [0,1]}{\max} \left \vert \phi_n(x) - \phi(x) \right \vert < \epsilon\]
mentre la convergenza uniforme richiede che
\[\forall \epsilon > 0, \exists n_\epsilon \in \mathbb{N} \text{ tale che } \forall n \geq n_\epsilon, \forall x \in [0,1] \text{ si ha che } \left \vert \phi_n(x) - \phi(x) \right \vert < \epsilon\]
che sono la stessa cosa, in quanto si sta lavorando con uno spazio topologico $C([0,1])$ di funzioni continue, per cui esiste sempre il massimo.

\vspace{1em}
\noindent
\subsection{Trasformazioni coordinate}
Si consideri la funzione
\[f : \mathbb{R}^2 \longmapsto \mathbb{R}^2 \hspace{1em} \text{e} \hspace{1em} f^{-1} : \mathbb{R}^2 \longmapsto \mathbb{R}^2\]
in cui se si considerano $x$ e $y$ coordinate nel dominio di $f$ e $u$ e $v$ coordinate nel codominio di $f$. Allora se la mappatura è la seguente:
\[
    \left\{
    \begin{array}{l}
        u=x-y\\
        v=x+y
    \end{array}  
    \right.
    \rightarrow
    \left\{
    \begin{array}{l}
        x=\dfrac{1}{2} (u-v)\\
        y=\dfrac{1}{2} (u+v)\\
    \end{array}  
    \right.
\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Nel caso di coordinate polari del tipo ...continua...
\[
    \left\{
    \begin{array}{l}
        u=x-y\\
        v=x+y
    \end{array}  
    \right.
\]
... continua ...\\
Allora si può considerare
\[f : ]0,+\infty[ \times [0,2\pi[ \longmapsto \mathbb{R}^2 - \{(0,0){^T}\}\]
in cui si pone
\[(\rho,\theta){^T} \longmapsto (x,y){^T}\]
con la mappatura 
\[
    \left\{
    \begin{array}{l}
        x=\rho \cdot \cos(\theta)\\
        y=\rho \cdot \sin(\theta)\\
    \end{array}  
    \right.
\]
Essendo un campo vettoriale, si considerano le componenti una ad una. Per verificare se $f$ è continua si studiano le componenti, che ovviamente sono il prodotto di due funzioni continue che, quindi, è continua.\\
Studiando la funzione inversa $f^{-1}$ definita come
\[f^{-1}(x,y) = (\rho,\theta){^T}\]
allora si ha che
\begin{itemize}
    \item $\rho=\sqrt{x^2+y^2}$
    \item $\theta$ è l'angolo tale che
    \[x=\rho \cos(\theta) \hspace{1em} \text{e} \hspace{1em} y=\rho \sin(\theta)\]
\end{itemize}
Pertanto non esiste una formula generale per esprimere l'angolo $\theta$. La funzione inversa non è continua, in quanto considerando il punto sulla circonferenza goniometrica, continua...

\vspace{1em}
\noindent
\textbf{Osservazione 1}: Lavorare con le coordinate polari non è semplice, in quanto non si ha un omeomorfismo per la descrizione delle coordinate nei due sistemi. L'omeomorfismo si ha quando si esclude la semiretta dei reali positivi e, quindi, si considera la funzione
\[f : \mathbb{R}^2 - \{(x,y){^T} \in \mathbb{R}^2 : x \geq 0\} \longmapsto ]0,+\infty[ \times ]0,2\pi[\]

\vspace{1em}
\noindent
\textbf{Osservazione 2}: È possibile anche considerare delle coordinate ellittiche centrate in $(x_0,y_0){^T}$, ovvero un sistema come quello seguente
\[
\left\{
    \begin{array}{l}
        x=x_0 + a \cdot \rho \cdot \cos(\theta)\\
        y=y_0 + b \cdot \rho \cdot \sin(\theta)\\
    \end{array}  
    \right.
\]

\vspace{1em}
\noindent
\textbf{Osservazione 3}: È possibile anche considerare delle coordinate ellissoidali, andando a considerare un sistema seguente
\[
\left\{
    \begin{array}{l}
        x=x_0 + a \cdot \rho \cdot \cos(\theta) \cdot \cos(\phi)\\
        y=y_0 + b \cdot \rho \cdot \sin(\theta) \cdot \sin(\phi)\\
        z=z_0 + c \cdot \rho \cdot \cos(\theta)
    \end{array}  
    \right.
\]

\vspace{1em}
\noindent
\subsection{Intervallo e insieme compatto}
Di seguito si espongono le definizioni generalizzate agli spazi metrici di intervallo e di insieme compatto.

\vspace{1em}
\noindent
\subsubsection{Insieme compatto}
Di seguito si espone la definizione di \textbf{insieme compatto} in uno spazio metrico:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{INSIEME COMPATTO}}\\
    \parbox{\linewidth}{Sia $X$ uno spazio metrico. Allora un insieme $K \subseteq X$ si dice compatto (per successioni) se per ogni successioni $(x_n)_n$ con $x_n \in K, \forall n$, esiste una sottosuccessione $(x_{n_k})_k$ convergente, ossia
    \[\lim_{k \to +\infty} x_{n_k} = l \in K\] \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\begin{theorem}
    Se $K$ è compatto, allora è chiuso e limitato.
\end{theorem}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Si dimostri che se $K$ è compatto, allora è chiuso, ovvero contiene tutti i suoi punti di accumulazione. Sia, allora, $\alpha$ un punto di accumulazione per $K$, allora si considera, $\forall n$, la palla
\[B \left(\alpha, \frac{1}{n}\right)\]
Si supponga esista $x_n \neq \alpha$, con $x_n \in K \cap B \left(\alpha, \dfrac{1}{n}\right)$. Ovviamente, per costruzione si ha che
\[\lim_{n \to +\infty} x_n = \alpha\]
Poiché $K$ è compatto, esiste una sottosuccessione $(x_{n_k})_k$ tale che
\[\lim_{k \to +\infty} x_{n_k} = l \in K\]
ma $l=\alpha$ e, quindi, $\alpha \in K$, per cui $K$ è chiuso.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Si dimostri che se $K$ è compatto, allora è limitato. Sia, allora, per assurdo $K$ non limitato. Si fissi $x_0 \in X$ tale per cui $\forall n$ esiste $x_n \in K$ con $x_n \notin B \left(x_0,n\right)$.\\
Ma siccome $K$ è compatto per ipotesi, esiste una sottosuccessione $(x_{n_k})_k$ tale per cui
\[\lim_{k \to +\infty} x_{n_k} = l \in K\]
ma tale successione non può convergere (ovvero non può essere che $d(x_{n_k},l)<\epsilon$), in quanto
\[d(x_{n_k},l) \geq d(x_{n_k},x_0) - d(x_0,l) \geq n_k - M\]
essendo $d(x_0,l)=M$ costante. Ma siccome $n_k \to +\infty$ per cui non si può avere convergenza.

\newpage
\noindent
\begin{center}
    25 Ottobre 2022
\end{center}
Le coordinate sferiche esposte in precedenza sono
\begin{align*}
    &x=\rho \sin(\phi) \cdot \cos(\theta)\\
    &y=\rho \sin(\phi) \cdot \sin(\theta)\\
    &z=\rho \cos(\phi)
\end{align*}
Negli spazi metrici viene privilegiata la definizione per gli insiemi compatti del teorema di Bolzano-Weierstrass, andando a considerare un insieme compatto per successioni, tale per cui un insieme è compatto se ogni successione in esso definita presenta una sottosuccessione convergente in tale insieme.\\
Ogni insieme compatto è chiuso e limitato. Tuttavia, non è vero il contrario. 

\vspace{1em}
\noindent
\textbf{Esempio 1}: Se, infatti, si considera lo spazio metrico $\mathbb{Q}$ e vi si considera l'insieme
\[[0,\pi[ \cap \mathbb{Q} = C\]
in cui $C$ è chiuso e limitato in $\mathbb{Q}$ non è compatto.

\vspace{1em}
\noindent
\textbf{Esempio 2}: Ancora, se si considera lo spazio metrico
\[C([0,1]), \vert \vert \cdot \vert \vert_{\infty}\]
allora data la palla chiusa
\[\overline{B(0,1)} = \{\phi \in [0,1] \longmapsto \mathbb{R} \hspace{1em} \text{tale che} \hspace{1em} \phi \text{ è continua e } \max \left \vert \phi(x) \right \vert \leq 1\}\]
allora tale insieme è chiuso e limitato. Tuttavia, non è compatto. Per dimostrarlo, è sufficiente considerare una successione che non ha sottosuccessioni convergenti.\\
Pertanto, se si considerano per $n \neq m$ le due successioni
\[\forall x, \hspace{1em} \phi_n(x) \neq 0 \hspace{1em} \text{si ha che} \hspace{1em} \phi_m(x) = 0\]
per cui è evidente che, $\forall x \in [0,1]$ si ha
\[\max \left \vert \phi_n(x) - \phi_m(x)\right \vert = 1, \hspace{1em} \forall n \neq m\]
che non è altro che
\[\left \vert \left \vert \phi_n - \phi_m \right \vert \right \vert_\infty\]
per cui non esistono sotto-successioni convergenti. Infatti, si supponga esista una sottosuccessione $\phi_{n_k}$ convergente; allora, fissato $\epsilon$ piccolo con $\epsilon=\dfrac{1}{100}$ deve esistere $n_\epsilon \in \mathbb{N}$ tale che
\[\forall n \geq n_\epsilon \hspace{1em} \text{e} \hspace{1em} \forall p \in \mathbb{N} \hspace{1em} \text{si ha che} \hspace{1em} \left \vert \left \vert \phi_{n+p} - \phi_n \right \vert \right \vert < \frac{1}{100}\]
ma ciò è impossibile, in quanto $\left \vert \left \vert \phi_{n+p} - \phi_n \right \vert \right \vert=1$ se $p \neq 0$.

\vspace{1em}
\noindent
\subsection{Teorema di caratterizzazione dei compatti in $\mathbb{R}^n$}
Se $K \subseteq \mathbb{R}^n$ è chiuso e limitato, allora $K$ è compatto (ma ciò vale solamente in $\mathbb{R}^n$).

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Sia $K \subset \mathbb{R}^2$ chiuso e limitato. Sia $\left((x_n,y_n){^T}\right)_n$ una successione in K. Si considera, in $\mathbb{R}$ la successione $(x_n)_n$, ossia la proiezione sull'asse $x$ della successione di partenza. Si osservi che tale successione è limitata in $\mathbb{R}$, in quanto
\[\left \vert x \right \vert \leq \left \vert \left \vert (x_0,y_0){^T} \right \vert \right \vert < M\]
essendo $K$ limitato. Per il Teorema di Bolzano-Weierstrass, esiste una sottosuccessione $(x_{n_k})_k$ convergente ad $\alpha \in \mathbb{R}$.\\
Ora è fondamentale considerare la sottosuccessione $(y_{n_k})_k$ che è ovviamente una sottosuccessione limitata, quindi esiste una sotto-sottosuccessione $(y_{n_{k_j}})_j$ convergente a $\beta \in \mathbb{R}$. Se, ora, si considera la sotto-sottosuccessione $(x_{n_{k_j}})_j$, è anch'essa convergente a $\alpha \in \mathbb{R}$, in quanto sottosuccessione di una successione convergente. Allora la sottosuccessione della successione di partenza
\[\left((x_{n_{k_k}}, y_{n_{k_j}}){^T}\right)_j\]
converge a $(\alpha,\beta){^T}$. Si noti, ovviamente, che $(\alpha,\beta){^T} \in K$ perché $K$ è chiuso e i limiti delle successioni sono punti di accumulazione per l'insieme $K$.

\vspace{1em}
\noindent
\subsection{Teorema di compattezza}
Sia $f : X \longmapsto Y$, con $X$ e $Y$ due spazi metrici e $f$ \textbf{continua}. Sia $K \subseteq X$ \textbf{compatto}, allora $f(K)$ è compatto.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Sia $(y_n)_n$ una successione, con $y_n \subset f(K)$. Allora, per definizione di insieme immagine, si ha che
\[\forall n, \exists x_n \in K \hspace{1em} \text{tale che} \hspace{1em} y_n=f(x_n)\]
La successione $(x_n)_n$ ha una sottosuccessione convergente $(x_{n_k})_k$ per il teorema di Bolzano-Weierstrass, ovvero
\[\lim_{k \to +\infty} x_{n_k} = \alpha \in K\]
Per la continuità di $f$ si ha che
\[\lim_{k \to +\infty} f(x_{n_k}) = f(\alpha)\]

\vspace{1em}
\noindent
\subsubsection{Teorema di Weierstrass}
Sia $f : X \longmapsto \mathbb{R}$ con $K \subseteq X$ compatto, e $f$ continua. Allora esistono
\[\underset{k}{\min} f \hspace{1em} \text{e} \hspace{1em} \underset{k}{\max} f\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Per il teorema di compattezza $f(K) \subset \mathbb{R}$ è compatto. Quindi esiste il massimo e il minimo.

\vspace{1em}
\noindent
\subsection{Teorema di Heine-Cantor sulla continuità uniforme}
Sia $f : K \subseteq X \longmapsto Y$ con $f$ continua e $K$ compatto. Allora $f$ è uniformemente continua.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Affermare che $f$ è uniformemente continua significa che
\[\forall \epsilon > 0, \exists \delta > 0 \hspace{1em} \text{tale che} \hspace{1em} \forall x_1,x_2 \in K \hspace{1em} \text{con} \hspace{1em} d_x(x_1,x_2) < \delta \hspace{1em} \text{allora} \hspace{1em} d_y(f(x_1),f(x_2)) < \epsilon\]
Per assurdo, si supponga che la funzione $f$ non sia uniformemente continua, ovvero
\[\exists \epsilon > 0 \hspace{1em} \text{tale che} \hspace{1em} \forall \delta = \frac{1}{n} \hspace{1em} \exists x^1_n,x^2_n \in K \hspace{1em} \text{con} \hspace{1em} d(x^1_n,x^2_n) < \frac{1}{n} \hspace{1em} \text{e} \hspace{1em} d_y(f(x^1_n),f(x^2_n)) \geq \epsilon\]
Avendo, ora, a disposizione due successioni $x^1_n$ e $x^2_n$ si può affermare, essendo $K$ compatto, che $(x^1_n)_n$ ha una sottosuccessione $(x^1_{n_k})_{k}$ convergente, ovvero
\[\lim_{k \to +\infty} x_{n_{k}} = \alpha \in K\]
Si considera, ora, la sottosuccessione $(x^2_{n_k})_k$, allora essa ha una sottosuccessione $(x^2_{n_{k_j}})_j$ convergente a $\beta$. Si noti che, ovviamente $\alpha = \beta$ poiché
\[d_x(x^1_{n_{k_k}},x^2_{n_{k_j}}) < \frac{1}{n_{k_j}} \to 0\]
Non solo, ma essendo $f$ continua, lo è in particolare in $\alpha$, quindi
\[\exists \delta > 0 \hspace{1em} \text{tale che} \hspace{1em} \forall x \in K, x \neq \alpha, d_x(x,\alpha) < \delta \hspace{1em} \text{allora} \hspace{1em} d_y(f(x_1),f(x_2)) < \frac{\epsilon}{100}\]
si ha allora che
\[\epsilon \leq d_y(f(x^1_{n_{k_j}})) + d_y\]
... continua ...

\vspace{1em}
\noindent
\subsection{Insieme connesso per archi}
Si fornisce di seguito la definizione di insieme connesso per archi:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{INSIEME CONNESSO PER ARCHI}}\\
    \parbox{\linewidth}{Sia $X$ uno spazio metrico. Allora $E \subset X$ si dice connesso per archi se per ogni $x_0,x_1 \in E$ esiste una curva continua
    \[\boxed{\gamma : [0,1] \longmapsto E}\]
    in cui è fondamentale che abbia valori in $E$, tale che
    \[\gamma(0)=x_0 \hspace{1em} \text{e} \hspace{1em} \gamma(1)=x_1\]
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\subsubsection{Teorema di connessione}
Sia una funzione $f : X \longmapsto Y$ una funzione definita tra $X$ e $Y$ due spazi metrici. Sia $E \subseteq X$ un insieme connesso (per archi), con $f$ continua. Allora $f(E)$ è connesso (per archi)

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Siano $y_0,y_1 \in f(E)$. Allora, per definizione di insieme immagine, si ha che
\[\exists x_0,x_1 \in E \hspace{1em} \text{tali che} \hspace{1em} f(x_0) = y_0 \hspace{1em} \text{e} \hspace{1em} f(x_1)=y_1\]
Ma siccome $E$ è connesso per ipotesi, si può affermare che
\[\exists \gamma : [0,1] \longmapsto E \hspace{1em} \text{continua con} \hspace{1em} \gamma(0)=x_0 \hspace{1em} \text{e} \hspace{1em} \gamma(1) =x_1\]
Allora la funzione composta
\[f \circ \gamma : [0,1] \longmapsto f(E)\]
è una curva continua con
\[f(\gamma(0)) = y_0 \hspace{1em} \text{e} \hspace{1em} f(\gamma(1)) = y_1\]

\vspace{1em}
\noindent
\textbf{Osservazione 1}: Un insieme connesso che non è connesso per archi (ma non può essere il viceversa) è il cosiddetto tendine, in cui si considera la successione $\dfrac{1}{n}$ e degli intervalli di ampiezza unitaria, in cui, in $0$ si considera solamente un punto.\\ Allora tale insieme non è connesso per archi, ma è connesso, in quanto la seconda è una definizione più debole della prima.

\vspace{1em}
\noindent
\textbf{Osservazione 2}: Si osservi che $E \subseteq \mathbb{R}$ è connesso \textbf{se e solo se} $E$ è un intervallo (o un intervallo degenere, approssimabile ad un punto, del tipo $[a,a]$).

\vspace{1em}
\noindent
\subsubsection{Teorema di esistenza degli zeri}
Sia $f : E \subseteq X \longmapsto \mathbb{R}$ con $f$ continua ed $E$ connesso. Allora, se esistono $x_1,x_2 \in E$ tali che
\[f(x_1) < 0 < f(x_1)\]
esiste $x_0 \in E$ tale che $f(x_0)=0$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Siccome $f(E)$ è un intervallo che contiene punti negativi e positivi.

\newpage
\noindent
\section{Calcolo differenziale}
La pendenza di un grafico, per esempio, di una funzione $f : \mathbb{R}^2 \longmapsto \mathbb{R}$ dipende dalla direzione dello spostamento.

\vspace{1em}
\noindent
\subsection{Derivata direzionale}
Di seguito si espone la definizione di \textbf{derivata direzionale}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{DERIVATA DIREZIONALE}}\\
    \parbox{\linewidth}{Sia data la funzione
    \[f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}^m\]
    Sia $x_0 \in A$ e sia $v \in \mathbb{R}^n$ un versore (ossia un vettore di $\vert \vert v \vert \vert = 1$).\\
    Si consideri la \textbf{funzione composta}
    \[f \circ \gamma : I \subseteq \mathbb{R} \longmapsto \mathbb{R}\]
    con $I$ un intervallo del tipo $]-\delta,+\delta[$ tale che $\forall t \in I, \gamma(t) \in A$.\\
    Allora si chiama \textbf{derivata direzionale} di $f$ in $x_0$ relativa alla direzione $v$, se esiste, la derivata in $t=0$ della funzione composta $f \circ g$, e si scriverà
    \[\frac{\partial f}{\partial v} (x_0) = \frac{d}{dt} (f \circ \gamma)(t) \vert_{t=0} = \lim_{t \to 0} \frac{f(\gamma(t)) - f(\gamma(t))}{t} = \lim_{t \to 0} \frac{f(x_0+tv) - f(x_0)}{t}\]
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la funzione
\[f(x,y)=xy+2x\]
e sia dato il punto $x_0=(1,0){^T}$ e la direzione $\displaystyle{v=\frac{1}{\sqrt{2}} \cdot (1,1){^T}}$. Allora la derivata in direzione $v$ di $f$ diviene
\[\frac{\partial f}{\partial v}(1,0) = \lim_{t \to 0} \frac{f\left(1+t \frac{1}{\sqrt[]{2}},t\frac{1}{\sqrt{2}}\right) - f(1,0)}{t}\]
che può essere scritto come
\[\lim_{t \to 0} \dfrac{(1+\frac{t}{\sqrt{2}}) \cdot \frac{t}{\sqrt{2}} + 2 \cdot \left(1+\frac{t}{\sqrt{2}} - 2\right)}{t} = \lim_{t \to 0} \dfrac{\frac{t}{\sqrt{2}} + \frac{t^2}{2} + 2 + 2 \frac{t}{\sqrt{2} - 2}}{t} = \frac{3}{\sqrt{2}}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: È chiaro che è possibile prendere in considerazione $v=e_k$ con $e_k$ versore di base della forma
\[e_k=(0,\dots,0,1,0,\dots,0){^T}\]
con $1$ solo nella posizione $k$-esima. Allora la parametrizzazione in funzione di $t$ rispetto a tale direzione è
\[\gamma(t)=(x_1^0,x_2^0,\dots,x_k^0,\dots,x_n^0){^T} + t \cdot (0,0,\dots,1,\dots,0){^T} = (x_1^0,\dots,x_{k-1}^0,t,x_{k+1}^0,dots,x_n^0){^T}\]
ciò permette, quindi, di definire la derivata direzionale come
\[\frac{\partial f}{\partial e_k}(x_0) = \lim_{t \to 0} \frac{f(x_0^0,\dots,x_k^0+t,)}{t}\]
... continua ...\\
Quindi, per calcolare le derivate di $f$ nella direzione $e_k$ si procede a \quotes{congelare} tutte le variabili diverse da $x_k$ e si considera la funzione nella sola variabile $x_k$.

\vspace{1em}
\noindent
\subsection{Derivata parziale}
Di seguito si espone la definizione di \textbf{derivata parziale}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{DERIVATA PARZIALE}}\\
    \parbox{\linewidth}{La derivata particolare rispetto alla direzione dell'asse $k$-esimo
    \[\frac{\partial f}{\partial e_k}(x_0)\]
    si indica con
    \[\frac{\partial f}{\partial x_k}(x_0)\]
    e si dice \textbf{derivata parziale} $k$-esima di $f$. Talvolta si indicano come
    \[\frac{\partial f}{\partial e_k} (x_0) = \frac{\partial f}{\partial x_k} (x_0) = D_{x_k} f(x_0) = f_{x_k}(x_0)\]
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la funzione $f:E \subseteq \mathbb{R}^3\longmapsto \mathbb{R}$ definita come
\[f(x,y,z) = \frac{\log(x+3z)}{y}\]
allora si ha che
\begin{align*}
    &\frac{\partial f}{\partial x} = \frac{1}{y} \cdot \frac{1}{x+3z}\\
    &\frac{\partial f}{\partial y} = -\frac{1}{y^2} \cdot \log(x+3z)\\
    &\frac{\partial f}{\partial z} = \frac{1}{y} \cdot \frac{3}{x+3z}\\
\end{align*}

\vspace{1em}
\noindent
\textbf{Osservazione}: Se si considera una funzione generale
\[f : \mathbb{R}^n \longmapsto \mathbb{R}^m\]
con
\[f=(f_1,\dots,f_m){^T}\]
allora si ha che
\[\frac{\partial f}{\partial v} = \left(\frac{\partial f_1}{\partial v}, \dots, \frac{\partial f_m}{\partial v}\right){^T}\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si consideri la funzione
\[f:\mathbb{R}^2 \longmapsto \mathbb{R}\]
definita come segue
\[f(x,y) = \left\{
    \begin{array}{lll}
        1 & \text{se} & xy=0\\
        0 & \text{se} & xy \neq 0\\
    \end{array}
    \right.    
\]
È facile capire che
\[\frac{\partial f}{\partial x}(0,0)=0 \hspace{1em} \text{e} \hspace{1em} \frac{\partial f}{\partial y}(0,0)=0\]
Tuttavia si ha che, posto $v=\dfrac{1}{\sqrt{2}} (1,1){^T}$
\[\frac{\partial f}{\partial v} = \nexists\]
per cui il fatto che esistano le derivate parziali non implica che esistano le derivate direzionali.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la funzione
\[f:\mathbb{R}^2 \longmapsto \mathbb{R}\]
definita come segue
\[f(x,y) = \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{lll}
        \dfrac{x^2y}{x^4+y^2} & \text{se} & (x,y){^T}\neq(0,0){^T}\\
        0 & \text{se} & (x,y){^T}=(0,0){^T}\\
    \end{array}
    \right.    
\]
Allora appare evidente come
\[\frac{\partial f}{\partial x}(0,0) = 0 \hspace{1em} \text{e} \hspace{1em} \frac{\partial f}{\partial y}(0,0) = 0\]
così come se si considera la direzione $v=\frac{1}{\sqrt{2}}(1,1){^T}$ si ha che
\[\frac{\partial f}{\partial v}(0,0) = \lim_{t \to 0} \frac{f\left(\dfrac{t}{\sqrt{2}},\dfrac{t}{\sqrt{2}}\right) - f(0,0)}{t} = \dfrac{\dfrac{t^3}{2\cdot\sqrt{2}} \cdot \dfrac{4}{t^4+2t^2}}{t} = \lim_{t \to 0} \frac{2}{\sqrt{2}} \cdot \frac{1}{t^2 + 2} = \frac{1}{\sqrt{2}}\]
per cui esistono le derivate parziale ed esiste la derivata direzionale lungo la bisettrice. Considerazione una qualunque altra direzione della forma $y=mx$, con $v=(a,b){^T}$, si ottiene
\[\frac{\partial f}{\partial v}(0,0) = \lim_{t \to 0} \frac{f(at,bt)-f(0,0)}{t} = \lim_{t \to 0} \dfrac{\dfrac{a^2bt^3}{a^4t^4+b^2t^2}-0}{t}=\lim_{t \to 0} \frac{a^2bt^2}{a^4t^4+b^2t^2} = \lim_{t \to 0} \frac{a^2b}{a^4t^2+b^2} = \frac{a^2}{b}\]
che, implica che la derivata direzionale in $(0,0){^T}$ esiste in qualunque direzione, ma la funzione non è continua in $0$, in quanto basta considerare la sua restrizione in $y=x^2$, per cui si ottiene 
\[f=\frac{x^2 \cdot x^2}{x^4 + x^4} = \frac{1}{2}\]

\newpage
\noindent
\begin{center}
    26 Ottobre 2022
\end{center}
Si riprenda in considerazione la funzione
\[f:\mathbb{R}^2 \longmapsto \mathbb{R}\]
definita come segue
\[f(x,y) = \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{lll}
        \dfrac{x^2y}{x^4+y^2} & \text{se} & (x,y){^T}\neq(0,0){^T}\\
        0 & \text{se} & (x,y){^T}=(0,0){^T}\\
    \end{array}
    \right.    
\]
Per quanto già visto in precedenza, tale funzione presenta derivate parziali e derivate direzionali in ogni direzione, ma tale funzione non è continua.\\
Con derivata direzionale si intende il calcolo della derivata della restrizione di una funzione in una specifica direzione descritta da un versore. Le derivate parziali sono delle particolari derivate direzionali, in cui la funzione viene ristretta su ogni specifico asse.\\
Se si vuole estendere il concetto di derivabilità allo spazio $\mathbb{R}^n$, non è sufficiente richiedere che esista la derivata in ogni direzione. È necessario richiedere che la funzione sia \textbf{differenziabile}.

\vspace{1em}
\noindent
\subsection{Funzione differenziabile}
Si espone di seguito la definizione di \textbf{funzione differenziabile}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{FUNZIONE DIFFERENZIABILE}}\\
    \parbox{\linewidth}{Sia data una funzione
    \[f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}^m\]
    con $x_0 \in A$. Allora $f$ si dice \textbf{differenziabile} in $x_0$ se esiste un'\textbf{applicazione lineare} (ovvero un approssimante lineare)
    \[L \in \mathcal{L}(\mathbb{R}^n, \mathbb{R}^m)\]
    appartenente all'insieme delle applicazioni lineare da $\mathbb{R}^n$ in $\mathbb{R}^m$ tale che
    \[\lim_{x \to x_0} \dfrac{f(x) - f(x) - L(x-x_0)}{x-x_0}=O_v\]
    e, in particolare, si ha
    \[\widetilde{f}(x) : f(x-x_0)\]
    si dice approssimante lineare di $f$ in $x_0$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}


\vspace{2em}
\noindent
\textbf{Osservazione}: Si osservi che $L$ se esiste è unica, per cui si indica con $\dif f(x)$ e si dice il differenziale di $f$ in $x_0$, con
\[\boxed{\dif f(x) : \mathbb{R}^n \longmapsto \mathbb{R}^m}\]
Si applica, quindi, la \textbf{formula di Taylor} del primo ordine: se $f$ è differenziabile in $x_0$ può essere scritta come
\[f(x) = f(x_0) + \dif f(x) \cdot (x-x_0) + o(x-x_0)\]
con
\[\lim_{x \to +\infty} \frac{o(x-x_0)}{x-x_0}=0\]
ovvero è un infinitesimo di ordine maggiore di $1$.\\
Non solo, data $\dif f(x)$ un'applicazione lineare, è possibile associarvi una matrice che rappresenta $\dif f(x)$ chiamata matrice Jacobiana che rappresenta $\dif f(x)$ in funzione della base canonica, ovvero
\[J f(x) = \left(df(x)(x-e_1),\dif f(x) (x-e_2), ..., \dif f(x) (x-e_n)\right) \in N \times N\]

\vspace{2em}
\noindent
\subsubsection{Teorema di continuità della funzione differenziabile}
Sia $f$ differenziabile in $x_0$, allora $f$ è continua in $x_0$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Sia dato il differenziale di $f(x)$, ovvero
\[f(x)-f(x_0) = \dif f(x)(x-x_0)+o(x-x_0)\]

\vspace{2em}
\noindent
\subsubsection{Teorema di esistenza della derivata direzionale}
Sia $f$ differenziabile in $x_0$, allora $\forall v$ versore
\[\exists \frac{\partial f}{\partial v}(x)\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Secondo la definizione di derivata direzionale, si ha che
\[\lim_{t \to 0} \frac{f(x_0+tv) - f(x_0)}{t} = \lim_{t \to 0} \left[ \frac{\dif f(x) \cdot (x-x_0)}{t} - \frac{o(t v)}{tv} - \frac{tv}{t}\right]\]
ma essendo $\dif f(x)$ un'applicazione lineare, si può chiaramente osservare che
\[\lim_{t \to 0} t \cdot \frac{\dif f(x)(v)}{t} = \dif f(x)\]

\vspace{1em}
\noindent
In particolare
\[\frac{\partial f}{\partial x}(x_0) = \dif f(x) (x_0)\]
e, quindi, la matrice Jacobiana raccoglie tutte le derivate della funzione.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la funzione
\[f : A \subseteq \mathbb{R^2} \longmapsto \mathbb{R}^3\]
data come segue
\[f(x,y,z)=\left[\sin(xy),x \log(y),\frac{x}{y}\right]{^T}\]
si ottengono le derivate parziali seguenti
\begin{align*}
    &\frac{\partial f}{\partial x} = \left(y\cos(xy),\log(y),\frac{1}{y}\right){^T}\\
    &\frac{\partial f}{\partial y} = \left(x\cos(xy),\frac{x}{y},-\frac{x}{y^2}\right){^T}\\
\end{align*}
È possibile, quindi, costruire anche la matrice di Jacobi corrispondente:
\[J(f(x,y)) = \left(
    \rowcolors{1}{white}{white}
    \begin{array}{cc}
        y \sin(xy) & x \cos(xy)\\
    \end{array}
\right)\]


\vspace{2em}
\noindent
\textbf{Osservazione 1}: Si osservi il caso particolare in $n=1$ e $m>1$, data una curva $\gamma(t)$ si ha che
\[\dif \gamma(t) : \mathbb{R} \longmapsto \mathbb{R}^2\]
allora si ha che il differenziale è proprio la derivata
\[\gamma'(t) = \left(x'(t),y'(t)\right){^T}\]
Per esempio, se si considera
\[\gamma(t)=(\cos(t),\sin(t)){^T} \hspace{1em} \text{con} \hspace{1em} t \in [0,2\pi]\]
allora il vettore tangente è
\[\gamma'(t)=\left(-\sin(t),\cos(t)\right){^T}\]

\vspace{2em}
\noindent
\textbf{Osservazione 2}: Si osservi il caso particolare in cui $n>1$ e $m=1$, con una funzione
\[f : A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m\]
in cui l'applicazione lineare si ha
\[\dif f(x) : \mathbb{R}^n \longmapsto \mathbb{R}\]
per cui si ottiene
\[\dif f(x) = \left(\frac{\partial f}{\partial x_1}(x), \frac{\partial f}{\partial x_2}(x), \dots, \frac{\partial f}{x_n}(x) \right)\]
ovverosia è una matrice di riga con il valore della derivata parziale della $f$ come elementi di colonna.\\
Allora il prodotto scalare seguente
\[\left<
    \left(
        \rowcolors{1}{white}{white}
        \begin{array}{c}
            \frac{\partial f}{\partial x_0}(x)\\
            \vdots\\
            \frac{\partial f}{\partial x_n}(x)
        \end{array}
    \right), v
\right>\]
si dice \textbf{gradiente} di $f$ in $x_0$ il vettore
\[\nabla f(x_0) = \mathcal{J}f(x_0){^T}\]

\vspace{2em}
\noindent
\subsubsection{Interpretazione geometrica con $n=2$ e $m=1$}
Il grafico dell'approssimante lineare, in una dimensione, era, ovviamente la retta tangente. Nel caso in cui $n=2$ e $m=1$, si ottiene che l'approssimante lineare
\[z=f(x_0,y_0) + \dif \widetilde{f}_{[x_0,y_0]}(x-x_0,y-y_0)\]
rappresenta il piano tangente al grafico della funzione $f$ nel punto $\left(x_0,y_0,f(x_0,y_0)\right){^T}$.\\
Altrimenti si sarebbe potuto anche scrivere
\[f(x,y) = \left<\nabla f(x,y), (x-x_0,y-y_0)\right>\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri la funzione
\[f(x,y)=x \log(y)\]
e si calcoli l'equazione del piano tangente al punto $(2,e){^T}$. Ovviamente si ha che
\[\nabla f(x,y) = \left(\log(y),\frac{x}{y}\right) \hspace{1em} \rightarrow \hspace{1em} \nabla f(2,1) = \left(1,\frac{2}{e}\right)\]
Allora si ha che l'equazione è
... continua ...

\vspace{1em}
\noindent
\subsection{Teorema sulla differenziabilità della combinazione lineare}
Siano date
\[f,g : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}^m\]
differenziabili in $x_0 \in A$. Siano $\alpha, \beta \in \mathbb{R}$, allora $\alpha f + \beta g$ è differenziabile e si ha che
\[\dif \left(\alpha f + \beta g\right)(x_0) = \alpha \cdot \dif f(x_0) + \beta \cdot g(x_0)\]

\vspace{1em}
\noindent
\subsection{Teorema sulla differenziabilità del prodotto}
Siano date
\[f,g : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}\]
differenziabili in $x_0$, allora il prodotto $f \cdot g$ è differenziabile e si ha che $\forall v \in \mathbb{R}^n$
\[\dif (f \cdot g)\]
... continua ...

\vspace{1em}
\noindent
\subsection{Teorema sulla differenziabilità della composta}
Siano date
\[f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}^k \hspace{1em} \text{e} \hspace{1em} g : B \subseteq \mathbb{R}^k \longmapsto \mathbb{R}^m\]
con $f(A) \in B$. Sia $f$ differenziabile in $x_0 \in \mathbb{R}$ e $f(x_0)=y_0 \in \mathbb{R}$ con $g$ differenziabile in $y_0$.\\
Allora la funzione composta
\[g \circ f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}^m\]
è differenziabile e si ha che
\[\dif(g \circ f)(x_0) = \dif g(f(x_0)) \circ \dif(f(x_0))\]
ovvero una composizione tra applicazioni lineari. Essendo la matrice della composta il prodotto righe per colonne tra matrici, si ha che
\[\mathcal{J}(g \circ f)(x_0) = \mathcal{J}g(f(x_0)) \cdot \mathcal{J}f(x_0)\]
In generale, infatti, si applica la seguente \textbf{regola della catena}:
\[\frac{\partial (g \circ f)_i}{\partial x_j} = \sum_{k=1}^K \frac{\partial g(f(x))_i}{\partial y_k} \cdot \frac{\partial f(x)_k}{\partial x_j}\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si vuole dimostrare che
\[g(f(x))-g(f(x_0))\]
per la differenziabilità della funzione $g$ si ha che
\[g(x) - g(x_0) = \dif g(x)(x-x_0) + o(x-x_0)\]
che, applicato alla funzione $f$, permette di ottenere
\[g(f(x)) - g(f(x_0)) = \dif g(f(x))(f(x)-f(x_0)) + o(f(x)-f(x_0))\]
similmente, per la differenziabilità della $f$ si evince che
\[f(x) - f(x_0) = \dif f(x)(x-x_0) + o(x-x_0)\]
ma ciò permette di affermare che
\[g(f(x)) - g(f(x_0)) = \dif g(f(x))(\dif f(x) (x-x_0))) + + o(f(x)-f(x_0))\]
La dimostrazione è conclusa se si dimostra che il secondo elemento è un o-piccolo, per cui si evince che
\[\lim_{x\ to x_0} \frac{\dif g(f(x))(f(x)-f(x_0))}{\vert \vert x-x_0 \vert \vert} + \frac{o \left(f(x)-f(x_0)\right)}{\vert \vert x-x_0 \vert \vert}\]
ma è ovvio che il primo termine tenda a $0$, per definizione di o-piccolo. Per quanto riguarda il secondo termine si ottiene che, moltiplicando e dividendo per la medesima quantità:
\[\frac{o \left(f(x)-f(x_0)\right)}{\vert \vert f(x) - f(x_0) \vert \vert} \cdot \frac{\vert \vert f(x) - f(x_0) \vert \vert}{\vert \vert x - x_0 \vert \vert}\]
il primo termine è infinitesimo. Per il secondo termine, si sfrutta ancora la definizione di differenziabilità, si ottiene che
\[\frac{\vert \vert f(x) - f(x_0) \vert \vert}{\vert \vert x - x_0 \vert \vert} = \frac{\vert \vert \dif f(x)(x-x_0) + o(x-x_0)}{\vert \vert x-x_0\vert \vert}\]
e scomponendo tale termine si ottiene
\[\left \vert \left \vert \frac{\dif f(x)(x-x_0)}{\vert \vert x-x_0\vert\vert} + \frac{o(x-x_0)}{\vert \vert x-x_0 \vert \vert}\right \vert \right \vert\]
in cui il secondo termine è ovviamente infinitesimo. Tuttavia, il primo termine si può scrivere come ...continua...

\vspace{2em}
\noindent
\textbf{Osservazione}: Si consideri la curva seguente
\[\gamma : I \subseteq \mathbb{R} \longmapsto \mathbb{R}^N\]
e il campo scalare seguente
\[f : \mathbb{R}^n \longmapsto \mathbb{R}\]
allora la composizione
\[\left(f \circ \gamma\right) : I \subseteq I \longmapsto \mathbb{R}\]
Allora si ha che il differenziale della composta è
\[\dif f(x) \circ \dif \gamma(x) = \left<\nabla f(x), \gamma'(x)\right>\]

\vspace{2em}
\noindent
Si supponga di considerare una curva piana rappresentabile come insieme degli zeri di un campo scalare
\[Z_f = \{(x,y){^T} \in \mathbb{R}^2 : f(x,y) = 0\}\]
e anche in forma parametrica come segue
\[\gamma : I \longmapsto \mathbb{R}^2 \hspace{1em} \text{e} \hspace{1em} \gamma(t) = (x(t),y(t)){^T}\]
È immediato evincere che
\[f \circ \gamma : I \longmapsto \mathbb{R} \hspace{1em} \text{con} \hspace{1em} f(\gamma(t)) = 0, \forall t\]
ma anche


\newpage
\noindent
\begin{center}
    28 Ottobre 2022
\end{center}

\vspace{1em}
\noindent
\textbf{Esercizio 1}: Si consideri la funzione seguente
\[f(x,y) = \dfrac{xy^2}{x^4+y^2}\]
e si calcoli il limite seguente
\[\lim_{(x,y){^T} \to (0,0){^T}} f(x,y)\]
Naturalmente, se il limite esiste è $0$, in quanto considerando la restrizione della funzione rispetto all'asse $x$ e all'asse $y$ la funzione vale $0$.\\
Considerando la restrizione della funzione ad una retta generica $y=mx$, si sta considerando la composta
\[h(x)=f(x,mx)=\frac{xm^2x^2}{x^4+m^2x^2}\]
per cui se $x \to 0$, allora la restrizione è nulla.\\
Allora si può pensare che la funzione ammetta limite. Per dimostrarlo, si ha che ... continua ...

\vspace{1em}
\noindent
\textbf{Esercizio 2}: Si consideri la funzione $f(x)=\arctan(x)$ e si consideri lo sviluppo in serie di tale funzione. Si può facilmente dimostrare che la funzione è sviluppabile in serie di Taylor.\\
Per poter scrivere la serie di Taylor si possono scrivere le derivate successive della $f$.\\
Tuttavia, è molto più semplice considerare la derivata della funzione:
\[f'(x)=\frac{1}{1+x^2}=\frac{1}{1-(-x^2)}\]
ma siccome è noto che la serie geometrica
\[\frac{1}{1-y}=\sum_{n=0}^{+\infty} y^n\]
con $\left \vert y \right \vert < 1$, è facile capire che
\[t'(t)=\sum_{n=0}^{+\infty}(-1)^n t^{2n}\]
e quindi per le proprietà degli integrali applicate alle serie
\[f(x)=\int_0^x f'(t) \dif t = \sum_{n=0}^{+\infty} (-1)^n \frac{1}{2n+1} x^{2n+1}\]

\vspace{1em}
\noindent
\textbf{Osservazione 1}: Data una funzione
\[f : A \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}\]
allora l'insieme delle curve di livello è
\[L_\alpha = \{(x,y){^T} \in A : f(x,y)=\alpha\}\]
per cui, se la funzione è
\[f(x,y) = 4x^2+y^2\]
presa l'equazione per determinare l'insieme delle curve di livello, si ottiene come
\[4x^2+y^2=\alpha \hspace{1em} \text{ è un ellisse, di equazione } \hspace{1em} \dfrac{x^2}{\left(\dfrac{\sqrt{\alpha}}{2}\right)^2} + \dfrac{y^2}{\left(\sqrt{2}\right)^2} = 1\]
allora è possibile esprimere tale funzione come una curva parametrica, del tipo
\[\gamma : I \rightarrow \mathbb{R}^2\]
... continua ...

\vspace{1em}
\noindent
\textbf{Osservazione 2}: Se si consideri una funzione, invece, che va da $\mathbb{R}^3$ in $\mathbb{R}$, come la seguente
\[f : A \subseteq \mathbb{R}^3 \longmapsto \mathbb{R}\]
allora l'insieme delle curve di livello è
\[L_\alpha = \{(x,y,z){^T} \in A : f(x,y,z)=\alpha\}\]
ora è possibile esprimere tale funzione come superficie, non più curva, parametrica
\[\sigma : I \times J \longmapsto \mathbb{R}^3\]
della forma
\[\sigma(s,t) = \left(x(s,t),y(s,t),z(s,t)\right){^T}\]
allora, supponendo che il sostegno $\Sigma$ di $\sigma$ coincida con $L_\alpha$. Se $f(x,y,z)$ è una sfera, per cui
\[f(x,y,z)=x^2+y^2+z^2\]
allora la $\sigma$ diviene
\[\sigma(s,y)= \left(\sin(s)\cos(t),\sin(s)\sin(t),\cos(s)\right)\]
Allora, per determinare il piano tangente alla sfera, si vanno a considerare i vettori tangenti fissando prima $s$ e poi $t$: se i vettori ottenuti sono linearmente indipendenti, essi generano il piano tangente cercato.
Allora, se si fissa $s$ e si muove $t$, il vettore tangente alla curva $\sigma(\overline{s},t)$ è
\[\frac{\partial \sigma}{\partial t} (\overline{s}, t)\]
e, similmente, se si fissa $t$ e si muove $s$, il vettore tangente alla curva $\sigma(s,\overline{t})$ è
\[\frac{\partial \sigma}{\partial s}(s,\overline{t})\]
Pertanto, nel punto $(x_0,y_0,z_0){^T} = \sigma(s_0,t_0)$ il piano tangente è il piano generato dai vettori
\[\frac{\partial \sigma}{\partial s}(s_0,t_0) \hspace{1em} \text{e} \hspace{1em} \frac{\partial \sigma}{\partial t} (s_0,t_0)\]
È facile capire, ora, che se si considera il prodotto vettoriale fra tali vettori, si ottiene un vettore a essi ortogonale, che consente di andare a scrivere le equazioni cartesiane della funzione di partenza.\\
Si osservi che per quanto visto
\[f\left(x(s,t),y(s,t),z(s,t)\right)=\alpha, \hspace{1em} \forall s, t\]
in quanto coincide con l'insieme delle curve di livello.\\
Ciò significa che
\[(f \circ \sigma)(s,t) = \alpha \hspace{1em} \text{costante}\]
per cui è evidente che il gradiente
\[\nabla(f \circ \sigma) = (0,0){^T}\]
ma per definizione di gradiente si ha che
\[\nabla(f \circ \sigma) = \left(\frac{\partial}{\partial s}(f \circ \sigma)(s,t), \frac{\partial }{\partial t}(f \circ \sigma)(s,t)\right){^T}\]
Ma è evidente che
\[\frac{\partial}{\partial s}(f \circ \sigma)(s,t) = 0\]
per cui
\[\left<\nabla f(\sigma,t), \frac{\partial}{\partial s} \sigma(s,t)\right> = 0\]
e si ha anche che
\[\frac{\partial}{\partial t}(f \circ \sigma)(s,t) = 0\]
per cui
\[\left<\nabla f(\sigma,t), \frac{\partial}{\partial t} \sigma(s,t)\right> = 0\]
Quindi si ha che $\nabla f(\sigma(s,t))$ è ortogonale al piano generato dai vettori
\[\frac{\partial \sigma}{\partial s}(s,t) \hspace{1em} \text{e} \hspace{1em} \frac{\partial \sigma}{\partial t} (s,t)\]
Quindi $\nabla f(x,y,z)$ è ortogonale alla superficie di livello $L_\alpha$ nel punto $(x,y,z){^T}$.

\vspace{2em}
\noindent
\textbf{Osservazione}: Si osservi che in un grafico di isoipse, dove le righe sono più fitte vi è maggiore pendenza e un percorso deve essere sempre ortogonale alle linee di livello

\vspace{1em}
\noindent
\subsection{Seconda proprietà geometrica del gradiente}
Sia $f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}$ differenziabile in $x_0 \in A$ e sia $\nabla f(x_0)$ il suo gradiente. Allora $\nabla f(x_0)$ indica la direzione di massimo incremento di $f$ nel punto e il suo modulo individua tale massima variazione. Cioè per ogni direzione $v$ si ha
\[\left \vert \frac{\partial f}{\partial v}(z_0) \right \vert \leq \left \vert \left \vert \nabla f(x_0) \right \vert \right \vert\]
e vale l'uguaglianza \textbf{se e solo se} $v$ è la direzione di $\nabla f(x_0)$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Applicando la disuguaglianza di Cauchy-Schwarz
\[\left \vert \frac{\partial}{\partial v} f(x) \right \vert = \left \vert \left<\nabla f(x), v \right> \right \vert \leq \vert \vert \nabla f(x) \vert \vert \cdot \underbrace{\vert \vert v \vert \vert}_{=1} = \vert \vert \nabla f(x) \vert \vert\]
Per determinare la direzione del gradiente di $f$, si considera il versore corrispondente
\[v=\frac{\nabla f(x)}{\vert \vert \nabla f(x) \vert \vert}\]
nell'ipotesi in cui $\vert \vert \nabla f(x) \vert \vert \neq 0$. Ma in particolare si ha che
\[\frac{\nabla}{\nabla v} f(x) = \left<\nabla f(x), \frac{\nabla f(x)}{\vert \vert \nabla f(x) \vert \vert}\right>\]
ma cià equivale a dire
\[\frac{1}{\vert \vert \nabla f(x) \vert \vert} \cdot \vert \vert \nabla f(x) \vert \vert^2 = \vert \vert \nabla f(x) \vert \vert\]

\vspace{2em}
\noindent
\textbf{Esercizio 1}: Si provi che è differenziabile in $(0,0){^T}$ la funzione
\[
    f(x,y)=\left\{
        \rowcolors{1}{white}{white}    
        \begin{array}{lll}
            \dfrac{x^2y^2}{\sqrt{x^2+y^2}} & \text{se} & (x,y){^t} \neq (0,0){^T}\\
            0   & \text{se} & (x,y){^t} = (0,0){^T}\\
        \end{array}
    \right\}
\]
Per la definizione di differenziabilità, la funzione data è differenziabile in $(0,0)^T$ se esiste un'applicazione lineare $L$
\[L : \mathbb{R}^2 \longmapsto \mathbb{R}\]
tale per cui
\[\lim_{(x,y){^T} \to (0,0){^T}} \dfrac{f(x,y) - f(0,0) - L(x,y)}{\vert \vert (x,y){^T} \vert \vert} = 0\]
Ma se la funzione è differenziabile, si ha che $L=\dif f(0,0)$, ovvero l'applicazione lineare cercata è il gradiente della funzione, per cui si può scrivere
\[L(x,y) = \left<\nabla f(0,0), (x,y){^T} \right>\]
pertanto bisogna dimostrare che
\[\lim_{(x,y){^T} \to (0,0){^T}} \dfrac{f(x,y) - f(0,0) - \left<\nabla f(0,0), (x,y){^T} \right>}{\vert \vert (x,y){^T} \vert \vert} = 0\]
ma è noto che $f(0,0)=0$ e
\begin{itemize}
    \item $\dfrac{\partial f}{\partial x} (0,0) = \lim_{t \to 0} \dfrac{f(x+t,0) - f(0,0)}{t} = 0$
    \item \item $\dfrac{\partial f}{\partial y} (0,0) = \lim_{t \to 0} \dfrac{f(0,y+t) - f(0,0)}{t} = 0$
\end{itemize}
Per cui appare evidente come
\[\left<\nabla f(0,0), (x,y){^T} \right> = \left<, (x,y){^T} \right> = 0\]
Bisogna, quindi, dimostrare che
\[\lim_{(x,y){^T} \to (0,0){^T}} \dfrac{f(x,y)}{\vert \vert (x,y){^T} \vert \vert} = 0 \hspace{1em} \rightarrow \hspace{1em} \dfrac{x^2y^2}{x^2+y^2} \cdot \dfrac{1}{\sqrt{x^2+y^2}}\]
... continua ...

\vspace{2em}
\noindent
\textbf{Esercizio 2}: Sia data una funzione differenziabile $f : A \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}$. Si conoscono le derivate direzionali della $f$ nel punto $(x_0,y_0)^T$ lungo due direzioni (non parallele) $u=(a,b){^T}$ e $v=(c,d){^T}$:
\[\frac{\partial f}{\partial u} (x_0,y_0)=p \hspace{1em} \text{e} \hspace{1em} \frac{\partial f}{\partial v} (x_0,y_0)=q\]
e si determini il gradiente della funzione $f$ nel punto $\partial f(x_0,y_0)$. In particolare, si ha che
\[\nabla f(x_0,y_0) = (\alpha, \beta){^T}\]
e si devono determinare $\alpha$ e $\beta$. È noto, per definizione, che
\begin{itemize}
    \item $p=\left<\nabla f(x,y), u\right>= \left<(\alpha,\beta){^T}, (a,b){^T}\right> = \alpha a + \beta b$
    \item $q=\left<\nabla f(x,y), v\right>= \left<(\alpha,\beta){^T}, (c,d){^T}\right> = \alpha c + \beta d$
\end{itemize}
per cui si ottiene che
\[
    \left\{
        \rowcolors{1}{white}{white}    
        \begin{array}{l}
            p=\alpha a + \beta b\\
            q=\alpha c + \beta d
        \end{array}
    \right.
\]
per cui è immediato evincere come
\[\alpha=\frac{dp-bq}{ad-bc} \hspace{1em} \text{e} \hspace{1em} \beta=\frac{aq-pc}{ad-bc}\]

\vspace{2em}
\noindent
\textbf{Esercizio 3}: Calcolare in $(0,0){^T}$ le derivate direzionali rispetto a $v=\dfrac{1}{\sqrt{2} (1,1){^T}}$ della funzione
\[
    f(x,y) =
    \left\{
        \rowcolors{1}{white}{white}    
        \begin{array}{lll}
            \dfrac{x^2y}{x^2+y^4} & \text{se} & (x,y){^t} \neq (0,0){^T}\\
            0 & \text{se} & (x,y){^t} = (0,0){^T}\\
        \end{array}
    \right.
\]
Allora appare evidente che
\begin{itemize}
    \item $\dfrac{\partial f}{\partial x}(0,0) = \lim_{x \to 0} \dfrac{f(x,0) - f(0,0)}{x} = 0$
    \item $\dfrac{\partial f}{\partial y}(0,0) = \lim_{y \to 0} \dfrac{f(0,y) - f(0,0)}{y} = 0$
    \item $\dfrac{\partial f}{\partial v}(0,0) = \lim_{t \to 0} \dfrac{f \left(\dfrac{t}{\sqrt{2},dfrac{t}{\sqrt{2}}}\right) - f(0,0)}{t} = \lim_{t \to 0} \dfrac{\dfrac{t^3}{2 \cdot \sqrt{2}}}{t} \cdot \left(\dfrac{t^2}{2}+\frac{t^4}{4}\right) = \dfrac{1}{\sqrt{2}}$
\end{itemize}
per cui la formula non vale perché in $0 \neq \dfrac{1}{\sqrt{2}}$, per cui la funzione non è differenziabile.

\vspace{2em}
\noindent
\textbf{Esercizio 4}: Sia $\phi : \mathbb{R} \longmapsto \mathbb{R}$ una qualsiasi funzione derivabile. Si verifichi che la funzione
\[u(x,t)=\phi \binom{y}{x}\]
è soluzione dell'equazione differenziale alle derivate parziali
\[x \dfrac{\partial u}{\partial x} + y \dfrac{\partial u}{\partial y} = 0\]
Per definizione della funzione $u(x,t)$, si ha che
\begin{itemize}
    \item $\dfrac{\partial}{\partial x} u(x,t) = \dfrac{\partial}{\partial x} \left[\phi \displaystyle{\binom{y}{x}}\right] = \phi' \displaystyle{\binom{y}{x}} \cdot \dfrac{\partial g}{\partial x} = \phi' \displaystyle{\binom{y}{x}} \cdot \left(-\dfrac{y}{x^2}\right)$
    \item $\dfrac{\partial}{\partial y} u(x,t) = \dfrac{\partial}{\partial y} \left[\phi \left(\dfrac{y}{x}\right)\right] = \phi' \left(\dfrac{y}{x}\right) \cdot \dfrac{\partial g}{\partial y} = \phi' \left(\dfrac{y}{x}\right) \cdot \left(\dfrac{1}{x}\right)$
\end{itemize}
per cui si ottiene che
\[x \dfrac{\partial u}{\partial x} + y \dfrac{\partial u}{\partial y} = x \phi' \left(\dfrac{y}{x}\right) \cdot \left(-\dfrac{y}{x^2}\right) + y \phi' \left(\dfrac{y}{x}\right) \cdot \left(\dfrac{1}{x}\right) = 0\]

\vspace{1em}
\noindent
\subsection{Teorema del valor medio per campi scalari}
Si espone di seguito il \textbf{teorema del valor medio per campi scalari}:


% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{TEOREMA DEL VALOR MEDIO PER CAMPI SCALARI}}\\
    \parbox{\linewidth}{Sia $A \subseteq \mathbb{R}^n$ un insieme convesso. Sia $f : A \longmapsto \mathbb{R}$ una funzione differenziabile. Allora, per ogni coppia di punti $P,Q \in A$, esiste un punto $\xi$ appartenente al segmento di estremi $P$ e $Q$, ossia tale che
    \[\exists \gamma : [0,1] \longmapsto \mathbb{R}^n \hspace{1em} \text{tale che} \hspace{1em} \phi(t) =P + t \cdot (Q-P)\]
    tale che
    \[f(Q)-f(P) = \left(\nabla f(\epsilon),Q-P\right)\]
    \vspace{-1mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Sia
\[(f \circ g) (t) : [0,1] \longmapsto \mathbb{R}\]
tale per cui
\[\gamma'(t) = Q-P\]
allora, per il teorema di Lagrange
\[\exists \eta \in ]0,1[ \hspace{1em} \text{tale che} \hspace{1em} (1-0) \cdot (f \circ g)'(\eta) = (f \circ g)(1) - \]
... continua ...


\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che tale teorema non risulta valido per i campi vettoriali, ma solo per il campi scalari.\\
Infatti, data una funzione
\[f : \mathbb{R}^n \longmapsto \mathbb{R}^m\]
non esiste un punto $\xi \in \overline{PQ}$ tale per cui
\[f(Q)-f(P)= \mathcal{J} f(\xi) (Q-P)\]
Per verificarlo, si consideri la funzione
\[f : [0,2\pi] \longmapsto \mathbb{R}^2\]
con
\[f(t)=(\cos(t),\sin(t)){^T}\]
in cui $P=0$ e $Q=2\pi$. Allora
\[f(2\pi)-f(0) = (1,0){^T} - {1,0}{^T} = (0,0){^T}\]
ma è impossibile, in quanto
\[(0,0)^{t} \neq (-\sin(t),\cos(t)){^T} - 2\pi\]

\vspace{1em}
\noindent
\subsection{Teorema del differenziale totale}
Si espone di seguito il teorema del differenziale totale:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{TEOREMA DEL DIFFERENZIALE TOTALE}}\\
    \parbox{\linewidth}{Sia $f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}^m$ ed esista
    \[\forall k \in \{1,\dots,n\}\]
    e $\forall x \in A$ esiste
    \[\dfrac{\partial f}{\partial x_k}(x)\]
    e siano funzioni continue in $x_0 \in A$. Allora $f$ è differenziabile in $x_0$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione}: Si dirà che $f$ è di classe $C^1$, ossia $f \in C^1(A,\mathbb{R}^m)$ se esistono e sono continue in $A$ tutte le derivate parziali $\dfrac{\partial f}{\partial x_k}$. Pertanto, se $f$ è $C^1$, allora $f$ è differenziabile.

\newpage
\noindent
\begin{center}
    31 Ottobre 2022
\end{center}
Di seguito si espone la dimostrazione del \textbf{teorema della differenziabilità totale}, ossia un criterio per stabilire quando una funzione è differenziabile, tramite la continuità delle derivate parziali.

\vspace{2em}
\noindent
\textbf{Osservazione}: Esistono funzioni differenziabili con derivate direzionali (e quindi anche parziali) non continue. Per dimostrarlo, è sufficiente considerare una funzione continua con derivata non continua, come la seguente:
\[
    f(x)=\left\{
        \rowcolors{1}{white}{white}    
        \begin{array}{lll}
            x^2 \cdot \sin \left(\dfrac{1}{x}\right) & \text{se} & x \neq 0\\
            0   & \text{se} & x = 0\\
        \end{array}
    \right.
\]
in cui è evidente che
\[
    f'(x)=\left\{
        \rowcolors{1}{white}{white}    
        \begin{array}{lll}
            2x \cdot \sin \left(\dfrac{1}{x}\right) - \cos\left(\dfrac{1}{x}\right) & \text{se} & x \neq 0\\
            0   & \text{se} & x = 0\\
        \end{array}
    \right.
\]
Sia $f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}^m$ ed esistano e siano continue in $x_0 \in A$ le derivate parziali di $f$, allora $f$ è differenziabile.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Sia, per ipotesi, che $n=2$ e $m=3$. Per definizione di differenziabilità, si ha che
\[f(x,y)-f(x_0,y_0)=f(x,y)-f(x,y_0)+f(x,y_0)-f(x_0,y_0)\]
in cui si è aggiunta e sottratta la medesima quantità $f(x,y_0)$, in cui nel primo caso si è fissato $x$, nel secondo si è fissato $y$. Pertanto si ottiene che
\[f(x,y)-f(x_0,y_0)=(f(x,y)-f(x,y_0))+(f(x,y_0)-f(x_0,y_0))=\dfrac{\partial f}{\partial y}(x,y_0) (y-y_0) + o(y-y_0) + \dfrac{\partial f}{\partial x}(x_0,y_0) (x-x_0) + o(x-x_0)\]
ma siccome $f$ è differenziabile in $(x_0,y_0){^T}$ significa che
\[\lim_{(x,y){^T} \to (x_0,y_0){^T}} \dfrac{f(x,y) - f(x_0,y_0) - \dfrac{\partial f}{\partial x}(x_x,y_0) (x-x_0) - \dfrac{\partial f}{\partial y}(x_0,y_0)(y-y_0)}{\vert \vert (x-x_0,y-y_0){^T} \vert \vert} = 0 \]
Ma quindi si ottiene che 
\[\dfrac{f(x,y) - f(x_0,y_0) - \dfrac{\partial f}{\partial x}(x_x,y_0) (x-x_0) - \dfrac{\partial f}{\partial y}(x_0,y_0)(y-y_0)}{\vert \vert (x-x_0,y-y_0){^T} \vert \vert} = \dfrac{1}{\vert \vert (x-x_0,y-y_0){^T} \vert \vert} \cdot \left(\dfrac{\partial f}{\partial y}(x,y_0)(y-y_0) - \dfrac{\partial f}{\partial y}(x_0,y_0)(y-y_0) \right) + \dfrac{o(y-y_0)}{\vert \vert (x-x_0,y-y_0){^T} \vert \vert} + \dfrac{o(x-x_0)}{\vert \vert (x-x_0,y-y_0){^T} \vert \vert}\]
ma ovviamente i due rapporti finali, per definizione di o-piccolo, essi tendono a $0$ quando $(x,y){^T} \to (x_0,y_0){^T}$. Pertanto si ottiene che
\[\cdot \left(\dfrac{\partial f}{\partial y}(x,y_0) - \dfrac{\partial f}{\partial y}(x_0,y_0)\right) \cdot \dfrac{y-y_0}{\vert \vert (x-x_0,y-y_0){^T} \vert \vert}\]
ma il primo fattore tende naturalmente a $0$ per la continuità della $f$.

\vspace{1em}
\noindent
\subsection{Funzioni con derivate nulle}
Sia $A \subseteq \mathbb{R}^n$ un insieme aperto e connesso (per archi); si data la funzione
\[f : A \longmapsto \mathbb{R}\]
tale per cui esistono tutte le derivate parziali e siano esse nulle, ovvero
\[\exists \dfrac{\partial f}{\partial x_i} \hspace{1em} \forall i=1,\dots,n \hspace{1em} \text{e} \hspace{1em}
\dfrac{\partial f}{\partial x_i}=0 \hspace{1em} \forall i, \forall x\]
allora la funzione $f$ è costante.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si considerino due punti $x_1,x_2 \in A$. Per definizione di insieme connesso (per archi)
\[\exists \gamma : [0,1] \longmapsto A\]
continua tale che $\gamma(0)=x_1$ e $\gamma(1)=x_2$. Si vuole dimostrare che $f(x_1)=f(x_2)$, ossia che la funzione è costante.\\
Si consideri la funzione composta seguente:
\[f \circ \gamma : [0,1] \longmapsto \mathbb{R}\]
allora per definizione
\[(f \circ \gamma)'(t) = \left<\nabla f(\gamma(t)), \gamma ' (t)\right> = 0, \hspace{1em} \forall t\]
dal momento che $\nabla f(\gamma(t)) = 0$ per ipotesi. Da notare che $f$ è differenziabile per il teorema della differenziabilità totale, in quanto le derivate parziali esistono e sono costantemente uguali a $0$, per cui sono continue.

\vspace{1em}
\noindent
\textbf{Osservazione}: Il problema, in questo caso, però, è che $\gamma$ potrebbe non essere derivabile. La dimostrazione vista vale per un insieme convesso, e non semplicemente connesso.\\
Per dimostrare che essa è derivabile si potrebbe procedere come segue
\begin{enumerate}
    \item Dimostrare che se $A$ è aperto e connesso per archi; allora è connesso anche per archi derivabili;
    \item Dimostrare che tale teorema vale sulle palle, con $\gamma$ continua.
\end{enumerate}
In generale, sfruttando la seconda ipotesi, ciò che si fa è considerare una curva $\gamma$ che congiunge due punti $x_1$ e $x_2$; si suppone che $f(x_1) \neq f(x_2)$, per ipotesi. Si prende il primo punto dove cambia il valore della funzione, in particolare
\[t_2 = \max \{t \in [0,1] : \gamma(s) = f(x_1), \hspace{1em} \forall s \in [0,t[\}\]

\vspace{2em}
\noindent
\textbf{Esercizio 1}: Si consideri la funzione seguente
\[f : \mathbb{R}^n \longmapsto \mathbb{R}^m\]
in cui $f(x)=A x$, con $A$ matrice $m \times n$. Allora, preso $x_0 \in A$, appare evidente che
\[\mathcal{J} f(x_0) = A\]
per cui, ovviamente, la matrice Jacobiana che rappresenta una funzione lineare è la matrice $A$ stessa.

\vspace{2em}
\noindent
\textbf{Esercizio 2}: Si consideri
\[p : \mathbb{R}^n \times \mathbb{R}^n \longmapsto \mathbb{R}\]
in cui
\[p(x,y) = \left<x,y\right>\]
allora appare evidente che
\[\nabla p(x_0,y_0) = (y_0,x_0){^T}\]

\vspace{2em}
\noindent
\textbf{Esercizio 3}: Si consideri
\[f : \mathbb{R}^n \longmapsto \mathbb{R}\]
allora, preso $v \in \mathbb{R}^n$ con $f(x)=\left<x,v\right>$, si ha che
\[\nabla f(x_0) = v\]

\vspace{2em}
\noindent
\textbf{Esercizio 4}: Si consideri
\[f : \mathbb{R}^n \longmapsto \mathbb{R}\]
allora, posto $f(x)=\left<x,x\right>$, si ha che
\[\nabla f(x_0) = 2x\]

\vspace{2em}
\noindent
\textbf{Esercizio 5}: Si consideri
\[f : \mathbb{R}^n \longmapsto \mathbb{R}\]
allora, posto $f(x)=\left<x,x\right>$, si ha che
\[\nabla f(x_0) = 2x\]

\vspace{2em}
\noindent
\textbf{Esercizio 7}: Per quanto visto in precedenza
\[\nabla \left(\left<x,x\right>\right) = 2x = \nabla \left(\vert \vert x \vert \vert^2\right)\]
Allora si ha che
\[\nabla \left(\vert \vert x \vert \vert\right) = \left(\sqrt{\left<x,x\right>}\right) = \dfrac{1}{2 \sqrt{\left<x,x\right>}} \cdot 2x = \dfrac{x}{\sqrt{\left<x,x\right>}}\]

\vspace{2em}
\noindent
\textbf{Esercizio 8}: Sia data la funzione seguente
\[f : \mathbb{R}^n \times \mathbb{R}^n \longmapsto \mathbb{R}\]
in cui si sono date
\begin{align*}
    &\phi : \mathbb{R}^n \times \mathbb{R}^n \longmapsto \mathbb{R}^m\\
    &\psi : \mathbb{R}^n \times \mathbb{R}^n \longmapsto \mathbb{R}^m\\
\end{align*}
allora se la funzione $f$ è definita come
\[f(x,y) = \left<\phi(x,y),\psi(x,y)\right>\]
appare evidente come
\[\mathcal{J} f = \left(\psi(x_0)\right)\]

\vspace{2em}
\noindent
\subsection{Derivate di ordine superiore}
Sia data $f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}^m$ e $\forall x \in A$ esista
\[\dfrac{\partial f}{\partial v}(x)\]
con $v \in \mathbb{R}^n$ versore. Allora sia dato il versore $w \in \mathbb{R}^n$ e ci si chiede se
\[\exists \dfrac{\partial }{\partial w} \left(\frac{\partial f}{\partial v}\right)(x_0) = \dfrac{\partial^2 f}{\partial w \partial v}(x_0)\]
Allora se esiste si chiamerà derivata direzionale di $f$ nel punto $x_0$ lungo le direzioni $v$ e $w$, in cui è fondamentale rispettare l'ordine.\\
In particolare, si parlerà di derivate parziali seconde
\[\dfrac{\partial^2 f}{\partial x_i \partial x_j} \hspace{1em} \text{se} \hspace{1em} i=j \hspace{1em} \dfrac{\partial^2 f}{\partial x_i^2} = \dfrac{\partial^2 f}{\partial x_i \partial x_i}\]
In particolare, $\forall k \in \mathbb{N}$, con $k \geq 1$ si possono definire le derivate parziali direzionali di ordine $k$
\[\dfrac{\partial^k f}{\partial x_{i,1}^{k_1} \partial x_{i,2}^{k_2} \dots \partial x_{i,l}^{k_l}}\]
in cui $i_1,i_2,\dots,i_l \in \{1,2,\dots,n\}$ e $k_1+k_2+\dots+k_l=k$.

\vspace{1em}
\noindent
\textbf{Esempio}: Per esempio, se si considera la funzione
\[f(x,y,z)=x-yz^2+xy^2z\]
allora si ha che
\[\dfrac{\partial f}{\partial x} 1+y^2z \hspace{1em} \text{e} \hspace{1em} \dfrac{\partial f}{\partial y} = -z^2+2xyz \hspace{1em} \text{e} \hspace{1em} \dfrac{\partial f}{\partial z} = -2xy + 2xyz\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che non è vero che le direzioni rispetto alle quali si eseguono derivate multiple siano intercambiabili. Infatti, se si considera la funzione seguente
\[
    f(x,y) = \left\{
        \rowcolors{1}{white}{white}    
        \begin{array}{lll}
            \dfrac{xy^3}{x^2+y^2} & \text{se} & (x,y){^T} \neq (0,0){^T}\\
            0   & \text{se} & (x,y){^T} = (0,0){^T}\\
        \end{array}
    \right.
\]
Allora si ha che
\[\dfrac{\partial f}{\partial x} = \dfrac{y^3 \cdot (x^2+y^2) - 2x^2y^3}{(x^2+y^2)^2} \hspace{1em} \text{e} \hspace{1em} \dfrac{\partial f}{\partial y} \dfrac{3xy^2 \cdot (x^2+y^2) - 2x^2y^4}{(x^2+y^2)^2}\]
È facile capire che tali derivate sono continue, in quanto in $(0,0){^T}$ valgono ambedue $0$. Si proceda al calcolo delle derivate seconde, da cui
\[\dfrac{\partial^2 f}{\partial y \partial x}(0,0) = \lim_{y \to 0} \dfrac{\dfrac{\partial f}{\partial x}(0,y) - \dfrac{\partial f}{\partial x}(0,0)}{y} = \lim_{y \to 0} \dfrac{\dfrac{y^4}{y^5}}{y} = 1\]
mentre si ha che
\[\dfrac{\partial^2 f}{\partial x \partial y}(0,0) = \lim_{x \to 0} \dfrac{\dfrac{\partial f}{\partial y}(x,0) - \dfrac{\partial f}{\partial y}(0,0)}{x} = \lim_{y \to 0} \dfrac{0-0}{x} = 0\]

\vspace{2em}
\subsection{Teorema di Schwarz sull'ordine di derivazione delle derivate di ordine superiore}
Sia $f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}$ e sia $x_0 \in A$; esistano tutte le derivate parziali di $f$ in $A$ di ordine $j=1,2,\dots,k$ e siano tutte continue in un punto $x_0 \in A$.\\
Allora tutte le derivate miste (ossia rispetto a variabili differenti) di ordine $\leq k$ nel punto $x_0$, eseguite rispetto alle medesime variabili, con la medesima molteplicità, sono uguali tra di loro. 

\vspace{2em}
\noindent
\textbf{Esempio}: È noto che la derivata direzionale di una funzione $f$ rispetto ad un versore $v$ può essere descritta come
\[\dfrac{\partial f}{\partial v} = \dfrac{\partial f}{\partial x}\dfrac{\partial f}{\partial y} \dots \dfrac{\partial f}{\partial z}\]
e se $f$ è differenziabile si ha che
\[\dfrac{\partial f}{\partial v} = \dif f(x_0) \cdot v\]
in cui $\dif f(x_0)$ è un'applicazione lineare, con
\[\dif f(x_0) : \mathbb{R}^n \longmapsto \mathbb{R}\]
Tuttavia, per poter descrivere la derivata seguente
\[\dfrac{\partial^2 f}{\partial w \partial v}\]
si necessità di un operatore della forma $\dif^2 f(x_0)$ che permette di calcolare le derivate direzionali seconde. Ovvero si sta cercando
\[\dif^2 f(x_0) : \mathbb{R}^n \times \mathbb{R}^n \longmapsto \mathbb{R}\]
che è un'applicazione bilineare. In generale, un'applicazione bilineare
\[\phi f(x_0) : \mathbb{R}^n \times \mathbb{R}^n \longmapsto \mathbb{R}\]
è una funzione del tipo
\[\phi(u,w) = \left<Au,w\right>\]
in cui $A$ è una matrice $n \times n$, dl tipo
\[A = \left(\rowcolors{1}{white}{white}    
\begin{array}{cc}
    a & b\\
    c & d
\end{array}\right)\]
per cui l'applicazione bilineare considerata è
\[\left<
    \left(\rowcolors{1}{white}{white}    
\begin{array}{cc}
    a & b\\
    c & d
\end{array}\right) \cdot
\left(\rowcolors{1}{white}{white}    
\begin{array}{c}
    u_1\\
    u_2
\end{array}\right),
\left(\rowcolors{1}{white}{white}    
\begin{array}{c}
    w_1\\
    w_2
\end{array}\right)
\right> = au_1w_1+bu_2w_1+cu_1w_2\]
per cui una forma quadratica si può sempre scrivere come
\[\phi(u,u) = \left<Au,u\right>\]
dove $A$ è una matrice simmetrica.

\newpage
\noindent
\begin{center}
    2 Novembre 2022
\end{center}
È importante conoscere le proprietà delle funzioni con gradiente nullo. Non solo, ma oltre al differenziale primo, esistono anche differenziali di ordini superiori.

\subsection{Differenziale secondo di un campo scalare}


% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{DIFFERENZIALE SECONDO DI UN CAMPO SCALARE}}\\
    \parbox{\linewidth}{Sia dato un campo scalare
    \[f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}\]
    differenziabile in $A$. Essendo differenziabile esiste il suo gradiente
    \[\nabla f : A \longmapsto \mathbb{R}^n\]
    Sia $x_0 \in A$, allora si dirà che $f$ è due volte differenziabile in $x_0$ se $\nabla f$ è differenziabile in $x_0$.\\
    Esiste, allora $\dif (\nabla f) (x_0)$ e la matrice Jacobiana $\mathcal{J}(\nabla f)(x_0)$: si chiamerà matrice hessiana di $f$ in $x_0$ la matrice Jacobiana di $\nabla f$
    \[\mathcal{H}(f(x_0)) = \mathcal{J}(\nabla f)(x_0)\]
    in cui $\mathcal{H} (f(x_0))$ è una matrice $n \times n$.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che, per definizione
\[\mathcal{H} f(x_0) = \mathcal{J}(\nabla f(x_0))\]
in cui il gradiente è, per definizione
\[\nabla f(x) = \left(\dfrac{\partial f}{\partial x_1}, \dfrac{\partial f}{\partial x_2}, \dots, \dfrac{\partial f}{\partial x_n}\right){^T}\]
Da ciò si evince che
\[\mathcal{J}(\nabla f)(x) = \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{cccc}

    \end{array}
\right)\]

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la funzione
\[f(x,y) = 3x^2y + \log(y)\]
Allora si ha che
\[\nabla f = \left(6xy,3x^2+\dfrac{1}{y} \right){^T}\]
Allora è immediato evincere che
\[\mathcal{H}(f(x,y)) = \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{cc}
        6y & 6x\\
        6x & -\dfrac{1}{y^2}
    \end{array}
\right)\]

\vspace{2em}
\noindent
\subsection{Differenziale secondo}
Si chiama \textbf{differenziale secondo} di $f$ in $x_0$ la forma bilineare o la forma quadratica associata alla matrice $\mathcal{H} f(x_0)$:
\begin{itemize}
    \item $\displaystyle{\dif^2 f(x_0) : \mathbb{R}^n \times \mathbb{R}^n \longmapsto \mathbb{B}}$ per cui si ottiene $\displaystyle{\dif^2 f(x_0)(v,w) = \left<\mathcal{H}f(x_0)\right>}$
\end{itemize}

\vspace{2em}
\noindent
Sia $f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}$ due volte diferenziabile in $x_0 \in A$; siano $v$ e $w$ due versori di $\mathbb{R}^n$, allora
\[\dfrac{\partial^2 f}{\partial w \partial v}(x_0) = \dif^2 f(x_0) (v,w)\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si rimandi a memoria che se
\[h : \mathbb{R}^n \longmapsto \mathbb{R} \hspace{1em} \text{e} \hspace{1em} v \in \mathbb{R}^n\]
si ottiene che
\[\nabla \left(\left< h(x), v \right>\right) = \mathcal{J} h(x){^T} \cdot v\]
Allora ciò permette di ottenere che
\[\dfrac{\partial}{\partial w} \left(\dfrac{\partial f}{\partial v}\right) (x_0) = \dfrac{\partial}{\partial w} \left(\left<\nabla f(x), v \right>\right) = \left<\nabla \left(\left< \nabla f(x), v \right>\right), w\right>\]
Ciò può essere riscritto, ponendo $x=x_0$, come segue
\[\left<\nabla \left(\left< \nabla f(x), v \right>\right), w\right> = \left<\mathcal{J}(\nabla f(x)){^T} \cdot v, w\right> = \left<\mathcal{H} f(x_0){^T}, w \right>\]

\vspace{1em}
\noindent
\subsection{Approssimante di ordine $k$}
Sia
\[f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}\]
con $x_0 \in A$; Si chiama approssimante di ordine $k$ in $x_0$ una funzione polinomiale di grado $\leq k$, chiamata $P_k$, tale che
\[\lim_{x \to x_0} \dfrac{f(x) - P_k(x)}{\vert \vert x-x_0 \vert \vert^k} = 0\]
in cui, ovviamente, se $k=1$ si ritrova la definizione di approssimante lineare. In generale, però, si ha che
\[P_k(v) = f(x_0) + \dif f(x_0)(v) + \dfrac{1}{2} \dif^2 f(x_0) v^2 + \dfrac{1}{3} \dif^3 f(x_0)v^3 + \dots + \dfrac{1}{k!} \dif^k f(x_0) v^k\]
in cui $\dif^k f(x_0)$ è un termine di grado $k$, in particolare un polinomio omogeneo di grado $k$.

\vspace{1em}
\noindent
\subsection{Teorema di esistenza dell'approssimante del secondo ordine}
Il teorema di esistenza dell'approssimante del secondo ordine corrisponde al teorema di Taylor di ordine $2$.


% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SERIE NUMERICA}}\\
    \parbox{\linewidth}{Sia
    \[f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}\]
    due volte differenziabile in $x_0 \in A$. Allora, posto
    \[P_2 = f(x_0) + \dif f(x_0) \cdot (x-x_0) + \dfrac{1}{2} \dif^2f(x_0) \cdot (x-x_0)^2\]
    che può essere scritto come
    \[P_2 = f(x_0) + \left<\nabla f(x_0,x-x_0\right> + \dfrac{1}{2} \left<\mathcal{H} f(x_0) (x-x_0, x-x_0)\right>\]
    si ha che $P_2(x)$ è approssimante di ordine $2$ di $f$ in $x_0$, cioé
    \[\lim_{x \to x_0} \dfrac{f(x) - P_2(x)}{\vert \vert x-x_0 \vert \vert^2} = 0\]
    che, pertanto, si può scrivere la formula di Taylor
    \[f(x)=P_2 + \epsilon(x) \cdot \vert \vert x-x_0 \vert \vert^2\]
    dove
    \[\lim_{x \to x_0} \epsilon(x)=0\]
    ossia l'errore $f(x)-P_2(x)$ è un infinitesimo di ordine $>2$.
    \vspace{3mm}}\\
    \hline
\end{tabularx}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si vuole provare che
\[\lim_{x \to x_0} \dfrac{f(x) - P_2(x)}{\vert \vert x-x_0 \vert \vert^2} = 0\]
sapendo che
\[f(x)-P_2(x) = f(x) - f(x_0) - \left<\nabla f(x_0), (x-x_0)\right> - \dfrac{1}{2} \left<\mathcal{H} f(x_0) (x-x_0), x-x_0\right>\]
Allora, posta $g(x)=f(x)-P_2(x)$, è immediato evincere che $g(x_0)=0$. Ciò permette di impiegare agevolmente la formula del valor medio, applicata a $g$, con $\xi$ appartenente al segmento che congiunge $x$ con $x_0$, come mostrato di seguito
\[g(x)=g(x)-g(x_0)=\left<\nabla g(\xi), x-x_0\right>\]
Dalla definizione di $g$, si può calcolare il suo gradiente
\[\nabla g(x) = \nabla f(x) - \nabla f(x_0) - \mathcal{H}f(x_0) \cdot (x-x_0)\]
Ma allora è possibile impiegare la formula del valor medio come segue
\[g(x)=\left<\nabla f(\xi) - \nabla f(x_0) - \mathcal{H}f(x_0) \cdot (\xi-x_0), x-x_0\right>=\left<\right>\]
ma è ovvio che, per la differenziabilità di $\nabla f$ in $x_0$, si ha
\[\nabla f(\xi) - \nabla f(x_0) = \mathcal{J}(\nabla f)(x_0) \cdot (x-x_0) + o(\xi-x_0)\]
pertanto si ha
\[g(x)=\left<\nabla f(\xi) - \nabla f(x_0) - \mathcal{H}f(x_0) \cdot (\xi-x_0), x-x_0\right>=\left<\right> = \left<o(\xi-x_0),x-x_0\right>\]
Allora è possibile calcolare
\[\lim_{x \to x_0} \dfrac{g(x)}{\vert \vert x - x_0 \vert \vert^2} = \lim_{x \to x_0} \dfrac{\left<o(\xi-x_0),x-x_0\right>}{\vert \vert x-x_0 \vert \vert^2}\]
In particolare, applicando la disuguaglianza di Cauchy-Schwarz, si ottiene che
\[\left \vert \dfrac{\left<o(\xi-x_0),x-x_0\right>}{\vert \vert x-x_0 \vert \vert^2} \right \vert \leq \dfrac{\vert \vert o(\xi-x_0) \vert \vert \cdot \vert \vert x-x_0 \vert \vert}{\vert \vert x-x_0 \vert \vert^2} = \left \vert \left \vert \dfrac{o(\xi-x_0)}{\vert \vert \xi-x_0 \vert \vert} \cdot \dfrac{\vert \vert \xi - x_0 \vert \vert}{\vert \vert x-x_0 \vert \vert} \right \vert \right \vert \to 0\]
Questo in quanto la prima frazione tende a $0$ per definizione di o-piccolo, mentre la seconda frazione è $\leq 1$ per ipotesi stessa della formula del valor medio.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si potrebbe dimostrare che l'esistenza dell'approssimante di ordine $2$ è equivalente alla due volte differenziabilità di $f$.

\vspace{1em}
\noindent
\subsection{Criterio di due volte differenziabilità di una funzione}

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{CRITERIO DI DUE VOLTE DIFFERENZIABILITÀ}}\\
    \parbox{\linewidth}{Sia
    \[f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}\]    
    tale che esistano tutte le derivate parziali secondo in $A$ e queste siano continue in $x_0 \in A$. Allora $f$ è due volte differenziabile in $x_0$.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Osservazione}: Si definisce
\[C^k(A,\mathbb{R}) = \{f \in A \longmapsto \mathbb{R} : \exists \text{ e sono continue le derivate parziali fino all'ordine } k\}\]

 \vspace{2em}
 \noindent
 \textbf{Esempio}: Si consideri la funzione seguente
 \[f(x,y) = 3xy+x^2-y^2\]
 Si scriva $P_2(x,y)$ in $(0,0){^T}$ e si calcoli
 \[\dfrac{\partial^2 f}{\partial v \partial 2}(0,0)\]
 posti
 \[v=\dfrac{1}{\sqrt{2}} (1,1){^T} \hspace{1em} \text{e} \hspace{1em} \dfrac{1}{\sqrt{2}} (-1,1){^T}\]
 Si calcoli il gradiente di $f$
 \[\nabla f = (3y+2x,3x-2y){^T}\]
 da cui si evince che la matrice hessiana è
 \[\mathcal{H} f = \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{cc}
        2 & 3\\
        3 & -2
    \end{array}
 \right)\]
 Allora il polinomio cercato è
 \[P_2=f(x_0,y_0)+\left<\nabla f(x_0,y_0), (x-x_0,y-y_0){^T}\right> + \dfrac{1}{2} \left<\mathcal{H}f(x_0,y_0)(x-x_0,y-y_0){^T},(x-x_0,y-y_0){^T}\right>\]
Inserendo all'interno i valori precedentemente ottenuti si ha
\[P_2(x,y) = \dfrac{1}{2} \left<
    \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{cc}
        2 & 3\\
        3 & -2
    \end{array}
 \right) \cdot \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{c}
        x\\
        y
    \end{array}
 \right),
 \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{c}
        x\\
        y
    \end{array}
 \right)
\right> = \dfrac{1}{2} \cdot \left(2x^2+6xy-2y^2\right) = x^2+3xy-y^2\]
che è esattamente la funzione di partenza, essendo essa stessa già un polinomio di ordine $2$.\\
Proseguendo si ottiene che
\[\dfrac{\partial^2 f}{\partial v \partial w} (0,0) = \left<
    \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{cc}
        2 & 3\\
        3 & -2
    \end{array}
 \right) \cdot \dfrac{1}{\sqrt{2}} \cdot \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{c}
        1\\
        1
    \end{array}
 \right), \dfrac{1}{\sqrt{2}} \cdot \left(
    \rowcolors{1}{white}{white}    
    \begin{array}{c}
        -1\\
        1
    \end{array}
 \right)
\right> = 2\]

\vspace{1em}
\noindent
\subsection{Ottimizzazione di campi scalari}
Si consideri la funzione seguente
\[f : E \subseteq \mathbb{R}^n \longmapsto \mathbb{R}\]
allora per definizione si ha che
\[\max f = \max f(E) \hspace{1em} \text{e} \hspace{1em} \min f = \min f(E)\]
Per quanto riguarda i punti di estremo assoluto
\begin{itemize}
    \item se $x_0 \in E$ è tale che $f(x_0) = \max f$ allora esso si chiama punto di massimo assoluto;
    \item se $x_0 \in E$ è tale che $f(x_0) = \min f$ allora esso si chiama punto di minimo assoluto;
\end{itemize}
Per quanto riguarda i punti di estremo relativo
\begin{itemize}
    \item $x_0 \in E$ si dirà punto di massimo relativo per $f$ se esiste un intorno $U$ di $x_0$ tale che
    \[f(x_0) = \max f|_{A \cap U}\]
    ossia tali che
    \[f(x_0) \geq f(x) \hspace{1em} \forall x \in E \cap U\]

    \item $x_0 \in E$ si dirà punto di minimo relativo per $f$ se esiste un intorno $U$ di $x_0$ tale che
    \[f(x_0) = \min f|_{A \cap U}\]
    ossia tali che
    \[f(x_0) \leq f(x) \hspace{1em} \forall x \in E \cap U\]
\end{itemize}
Se $n \geq 2$, allora un punto $x_0 \in E$ si dice un \textbf{punto di sella} se esistono due direzioni, ossia due versori, $v$ e $w$ e si ha
\[f(x_0+tv) : ]-\delta,\delta[ \longmapsto \mathbb{R}\]
presenta in $t=0$ un punto di minimo mentre la funzione
\[f(x_0+tw) : ]-\delta,\delta[ \longmapsto \mathbb{R}\]
presenta in $t=0$ un punto di massimo. Si suppone, ovviamente, che $x_0$ è interno al dominio, in quando si necessita di muoversi di $\delta$ lungo le direzioni specificate.

\vspace{1em}
\noindent
\textbf{Esempio}: Se si considera la funzione
\[f(x,y)=xy\]
si vede facilmente che $(0,0){^T}$ è un punto di sella. Basta considerare il versore
\[v=\dfrac{1}{\sqrt{2}}(1,1){^T}\]
allora
\[f((0,0){^T} + t \cdot (1,1){^T}) = t^2\]
e il versore
\[w=\dfrac{1}{\sqrt{2}}(-1,1){^T}\]
allora
\[f((0,0){^T} + t \cdot (-1,1){^T}) = -t^2\]
per cui nel primo caso $(0,0){^T}$ è punto di minimo, nel secondo caso è punto di massimo.

\vspace{1em}
\noindent
\textbf{Osservazione}: Sia
\[f : E \subseteq \mathbb{R}^n \longmapsto \mathbb{R}\]
con $x_0 \in E$ con $f$ differenziabile in $x_0$. Si dirà che $x_0$ è un punto critico per $f$ se
\[\nabla f(x_0) = O_v \in \mathbb{R}^n\]

\vspace{1em}
\subsection{Teorema di Fermat (test del gradiente)}
Sia $A \subseteq \mathbb{R}^n$ \textbf{aperto} e sia data
\[f : A \longmapsto \mathbb{R}\]
Posto $x_0$ un punto di minimo/massimo relativo per $f$, si supponga che
\[\exists \dfrac{\partial f}{\partial v}(x_0)\]
con $v$ versore. Allora 
\[\dfrac{\partial f}{\partial v}(x_0) = 0\]
In particolare, se $f$ è differenziabile in $x_0$, si ha che $\nabla f(x_0) = 0$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Per definizione stessa di derivata direzionale si ha che
\[\dfrac{\partial f}{\partial v} (x_0) = \dfrac{\dif}{\dif t} f(x_0+tv) \hspace{1em} \text{con} \hspace{1em} t \in ]-\delta,\delta[\]
allora $x_0$ è un punto di estremo relativo.

\vspace{1em}
\noindent
\textbf{Osservazione 1}: Ovviamente, non vale il viceversa.

\vspace{1em}
\noindent
\textbf{Osservazione 2}: Posto $n=2$, con $f$ differenziabile e sia dato $(x_0,y_0){^T}$ un punto di sella, allora $\nabla f(x_0,y_0) = (0,0){^T}$

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Se $(x_0,y_0){^T}$ è un punto di sella, $\exists v, w$ tale per cui
\begin{itemize}
    \item $f(x_0+tv)$ ha un minimo in $t=0$
    \item $f(x_0+tw)$ ha un massimo in $t=0$
\end{itemize}
ma naturalmente si ha che
\[\dfrac{\dif}{\dif t} f(x_0+tv) = \dfrac{\partial f}{\partial v}(x_0) \hspace{1em} \text{e} \hspace{1em} \dfrac{\partial f}{\partial w}(x_0)=0\]
Ma quindi se si si hanno due derivate direzionali, rispetto a due direzioni linearmente indipendenti, entrambe nulle, ciò implica che il gradiente sia nullo, ossia $\nabla f(x_0,y_0)=(0,0){^T}$.\\
Se $n \geq 3$, la cosa non funziona, in quanto si necessiterebbe di avere $3$ derivate direzionali nulle, in cui in ciascuna si deve avere o massimo o minimo.

\newpage
\noindent
\begin{center}
    7 Novembre 2022
\end{center}
È stato studiato l'approssimante del secondo ordine e il modo del suo ottenimento, nonché il criterio di esistenza.\\
Una particolare applicazione del differenziale secondo è quella di andare ad analizzare i problemi di massimo e minimo, andandone a studiare il segno. Tuttavia, il differenziale secondo è una funzione e bisogna definire che cosa significa \quotes{segno}.

\vspace{1em}
\subsection{Segno di una forma quadratica}
Si consideri una forma quadratica $\phi : \mathbb{R}^n \longmapsto \mathbb{R}$, con
\[\phi(v) = \left<Av,v\right>\]
in cui $A$ è una matrice quadrata simmetrica.

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che $\phi$ è una funzione lineare in quanto il prodotto scalare gode delle proprietà di bilinearità, ovvero $\phi(\alpha v) = \alpha^2 \cdot \phi(v)$. Non solo, ma $\phi$ è anche continua.

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{SEGNO DI UNA FORMA quadratica}}\\
    \parbox{\linewidth}{Si dice che $\phi$ è \textbf{definita positiva} se $\forall v \in \mathbb{R}^n$, con $v \neq 0$, allora $\phi(v) > 0$.\\
    Invece, si dice che $\phi$ è \textbf{definita negativa} se $\forall v \in \mathbb{R}^n$, con $v \neq 0$, allora $\phi(v) < 0$.\\
    Si dirà, invece, che $\phi$ è \textbf{indefinita} se esistono $v_1,v_2 \in \mathbb{R}^n$ e $\phi(v_1) < 0 < \phi(v_2)$.\vspace{3mm}}\\
    \hline
\end{tabularx}

\vspace{1em}
\noindent
\textbf{Esempio}: Un esempio di forma quadratica definita positiva è
\[\phi(x,y) = x^2 + y^2\]
mentre una funzione definita negativa è
\[\phi(x,y) = -x^2 - y^2\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che se $A$ è una matrice simmetrica, essa è \textbf{diagonalizzabile}. Pertanto, conoscendo gli \textbf{autovalori} di una matrice $A$, automaticamente è possibile stabilire se la forma quadratica $\phi(v) = \left<Av,v\right>$ è definita positiva o definita negativa. Infatti
\begin{itemize}
    \item se tutti gli autovalori sono $\geq 0$, allora è definita positiva;
    \item se tutti gli autovalori sono $\leq 0$, allora è definita negativa;
\end{itemize}

\vspace{1em}
\noindent
\subsubsection{Metodo di Jacobi-Sylvester}
Data una matrice $A$, quindi, la forma quadratica associata è definita positiva se $\det A > 0$ e $(A_{1,1}) > 0$, ossia l'elemento in posizione $1,1$ deve essere positivo.\\
Viceversa, la forma quadratica associata alla matrice $A$ è definita positiva se $\det A > 0$ e $(A_{1,1}) < 0$, ossia l'elemento in posizione $1,1$ deve essere negativo.\\
Da ultimo la forma quadratica associata è indefinita se $\det A < 0$.

\vspace{1em}
\noindent
\textbf{Osservazione}: Sia $\phi : \mathbb{R}^n \longmapsto \mathbb{R}$ una forma quadratica definita positiva. Allora esiste una costante $m > 0$ tale che $\phi(x) \geq m \cdot \vert \vert x \vert \vert^2, \hspace{0.25em} \forall x \in \mathbb{R}^n$. Ciò è fondamentale da capire in quanto permette di capire che
\[\lim_{x \to +\infty} \phi(x) = +\infty\]
Non solo, ma ammette sempre minimo, pari a $0$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si consideri la restrizione della forma quadratica al compatto $\{x \in \mathbb{R}^n : \vert \vert x \vert \vert = 1\}$, ossia
\[\phi \vert_{x \in \mathbb{R}^n : \vert \vert x \vert \vert = 1}\]
Si sta considerando una funzione continua definita su un compatto; allora per Weierstrass esiste massimo e minimo, in particolare esiste
\[\underbrace{\vert \vert x \vert \vert = 1}{\min \phi(x)} = m > 0\]
che deve essere maggiore di zero in quanto $m = \phi(x_0)$, ma $\vert \vert x_0 \vert \vert = 1$ con $\phi(x_0)>0$ essendo forma quadratica definita positiva.\\
...continua...

\vspace{1em}
\noindent
\subsection{Test del differenziale secondo per lo studio dei punti critici}
Si espone di seguito l'enunciato del \textbf{test del differenziale secondo per lo studio dei punti critici}:

% Tabella per le definizione di concetti, etc...
\vspace{1em}
\rowcolors{1}{black!5}{black!5}
\setlength{\tabcolsep}{14pt}
\renewcommand{\arraystretch}{2}
\noindent
\begin{tabularx}{\textwidth}{@{}|P|@{}}
    \hline
    {\textbf{...}}\\
    \parbox{\linewidth}{Sia dato il campo scalare seguente
    \[f : A \subseteq \mathbb{R}^n \longmapsto \mathbb{R}\]
    con $A$ aperto e $x_0 \in A$. Sia $f$ due volte differenziabili in $x_0$; si pone
    \[\phi(v) = \dif^2 f(x_0) \cdot v^2 = \left<H f(x_0) \cdot v, v\right>\]
    Allora
    \begin{enumerate}
        \item se $\phi$ è definita positiva, allora $x_0$ è un punto di minimo relativo stretto per $f$;
        \item se $\phi$ è definita negativa, allora $x_0$ è un punto di massimo relativo stretto per $f$;
        \item se $\phi$ è indefinita, allora $x_0$ è un punto di sella per $f$.
    \end{enumerate}
    \vspace{1mm}}\\
    \hline
\end{tabularx}

\vspace{2em}
\noindent
\textbf{Osservazione}: Attenzione che nulla si può dire in generale, se $\det H f(x_0) = 0$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 1}: Sia $\phi$ definita positiva. Allora si vuole dimostrare che $x_0$ è punto di minimo relativo stretto per $f$. Allora si scrive
\[f(x)-f(x_0) = \dif f(x_0) \cdot (x-x_0) + \dfrac{1}{2} \dif^2 f(x_0) \cdot (x-x_0)^2 + \epsilon(x) \cdot \vert \vert x - x_0 \vert \vert^2\]
tale per cui
\[\lim_{x \to x_0} \epsilon(x) = 0\]
L'espressione di cui sopra può essere scritta come
\[f(x)-f(x_0) = \dif f(x_0) \cdot (x-x_0) + \dfrac{1}{2} \dif^2 f(x_0) \cdot (x-x_0)^2 + \epsilon(x) \cdot \vert \vert x - x_0 \vert \vert^2 = \left< \nabla f(x_0), x-x_0 \right> + \dfrac{1}{2} \phi(x-x_0) + \epsilon(x) \cdot \vert \vert x - x_0 \vert \vert^2\]
ma $\left< \nabla f(x_0), x-x_0 \right> = 0$, mentre è noto che $\phi$ è definita positiva, per cui esiste $m>0$ tale che $\phi(v) \geq m \cdot \vert \vert v \vert \vert^2, \hspace{0.25em} \forall v \in \mathbb{R}^n$.\\
Allora si ha che
\[\left< \nabla f(x_0), x-x_0 \right> + \dfrac{1}{2} \phi(x-x_0) + \epsilon(x) \cdot \vert \vert x - x_0 \vert \vert^2 \geq \dfrac{1}{2} m \cdot \vert \vert x-x_0 \vert \vert^2 + \epsilon(x) \cdot \vert \vert x-x_0 \vert \vert^2 = \left(\dfrac{1}{2}m + \epsilon(x)\right) \cdot \vert \vert x-x_0 \vert \vert^2 > 0 \hspace{1em} \forall x \in U, x \neq x_0\]
Questo perché
\[\lim_{x \to x_0} \epsilon(x) = 0\]
e quindi esiste un intorno $U$ di $x_0$ tale che $\forall x \in U$ si ha che
\[\left \vert \epsilon(x) \right \vert < \dfrac{1}{2} m\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 2}: Nel caso in cui $\phi$ sia definita negativa, allora è possibile stimarla dall'alto. Infatti deve esistere $m > 0$ tale che
\[\phi(x) \leq -m \cdot \vert \vert x \vert \vert^2 \hspace*{0.25em} \forall x \in \mathbb{R}^n\]
ovvero la funzione
\[\phi \vert_{\{x \in \mathbb{R}^n : \vert \vert x \vert \vert = 0\}}\]
ha un massimo negativo che si denota con $m < 0$. Pertanto si ha che
\[f(x)-f(x_0) = \dfrac{1}{2} \phi(x-x_0) + \epsilon(x) \cdot \vert \vert x - x_0 \vert \vert^2 \leq \left(-\dfrac{1}{2} + \epsilon(x)\right) \cdot \vert \vert x-x_0 \vert \vert^2 < 0\]
se $x \in V$, con $V$ intorno di $x_0$, con $x \neq x_0$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione 3}: Sia $\phi$ indefinita; allora, per definizione, esistono $v_1,v_2$ tali che $\phi(v_1) < 0 < \phi(v_2)$.\\
Senza perdita di generalità è possibile supporre che $\vert \vert v_1 \vert \vert = \vert \vert v_2 \vert \vert = 1$, in quanto
\[\phi \left(\dfrac{v_1}{\vert \vert v_1 \vert \vert}\right) = \dfrac{1}{\vert \vert v_1 \vert \vert^2} \cdot \phi(v_1) < 0\]
Si consideri allora
\[h(t) = f(x_0 + tv_1) \hspace{1em} \text{con} \hspace{1em} h : \left]-\delta,\delta\right[ \longmapsto \mathbb{R}\]
allora $h'(0) = 0$, in quanto $h'(0) = \left<\nabla f(x_0),v_1\right> = 0$.\\
Calcolando la derivata seconda si ottiene che
\[h''(0) = \dfrac{\partial^2 f}{\partial v^2} (x_0) = \dif^2 f(x_0) \cdot v_1^2 = \phi(v_1) < 0\]
per cui $0$ è un punto di massimo per $h$, cioè $x_0$ è punto di massimo relativo per la $f$ ristretta alla direzione $v_1$.\\
Analogamente, si considera $k(t) = f(x_0+tv_2)$. Allora si ha sempre $k'(0)=0$ e $k''(0) = \phi(v_2) > 0$, per cui $x_0$ è un punto di minimo relativo per la funzione $f$ ristretta alla direzione $v_2$.

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la funzione seguente
\[f(x,y) = 3x^2+y^2-x^3y\]
essendo $f$ una funzione $f : \mathbb{R}^2 \longmapsto \mathbb{R}$.\\
Si individuino i punti critici. Per prima cosa si calcola il gradiente:
\[\nabla f(x,y) = \left(6x - 3x^2 y, 2y - x^3\right){^T}\]
Si risolva il sistema seguente
\[
    \left\{
    \begin{array}{l}
        3x \cdot (2-xy) = 0\\
        2y = x^3
    \end{array}
    \right.
\]
ciò permette di ottenere tre punti critici
\[(0,0){^T} \hspace{1em} \text{e} \hspace{1em} (-\sqrt{2},-\sqrt{2}){^T} \hspace{1em} \text{e} \hspace{1em} (\sqrt{2},\sqrt{2})\]
per determinarne la natura si calcola la matrice Hessiana:

\[
    Hf(x,y)
    \left(
    \begin{array}{cc}
        6-6xy & -3x^2\\
        -3x^2 & 2\\
    \end{array}
    \right)
\]
Per cui ora si procede come segue
\[
    Hf(0,0) = 
    \left(
    \begin{array}{cc}
        6 & 0\\
        0 & 2\\
    \end{array}
    \right) \hspace{1em} \rightarrow \hspace{1em} \det(Hf(0,0)) > 0
\]
per cui $(0,0){^T}$ è un punto di minimo per la funzione.\\
Analogamente
\[
    Hf(\pm \sqrt{2},\pm \sqrt{2}) = 
    \left(
    \begin{array}{cc}
        -6 & -6\\
        -6 & 2\\
    \end{array}
    \right) = 2 \cdot \left(
        \begin{array}{cc}
            -3 & -3\\
            -3 & 1\\
        \end{array}
        \right)
        \hspace{1em} \rightarrow \hspace{1em} \det(Hf(\pm \sqrt{2},\pm \sqrt{2})) < 0
\]
per cui i punti $(\sqrt{2},\sqrt{2})$ e $-\sqrt{2},- \sqrt{2})$ sono punti di sella.\\
Per capire se si tratta di punti di estremo assoluto, è sufficiente osservare che
\begin{align*}
    f(x,0) = 3x^2\\
    f(x,1) = 3x^2+1-3x^3
\end{align*}
per cui nel primo caso $f(x,0) \to +\infty$ e nel secondo caso $f(x,1) \to -\infty$, per $x \to +\infty$, per cui la funzione è illimitata sia dall'alto che dal basso. Gli estremi individuati sono, quindi, relativi e non assoluti.

\vspace{1em}
\noindent
\textbf{Esercizio 1}: Si consideri la funzione seguente
\[f(x,y) = 3x+x^3-2y^2+y^4\]
allora il gradiente di tale funzione è
\[\nabla f = \left(3-3x^2,-4y+4y^3\right){^T}\]
Dal sistema si ottiene che i punti critici sono i seguenti:
\[(-1,0){^T} \hspace{1em} \text{e} \hspace{1em} (-1,-1){^T} \hspace{1em} \text{e} \hspace{1em} (-1,1){^T} \hspace{1em} \text{e} \hspace{1em} (1,0){^T} \hspace{1em} \text{e} \hspace{1em} (1,-1){^T} \hspace{1em} \text{e} \hspace{1em} (1,1){^T}\]
per determinare la natura dei punti critici, si calcola l'Hessiana della $f$, ottenendo
\[H f(x,y) = \left(
    \begin{array}{cc}
        -6x & 0\\
        0 & -4+12y^2\\
    \end{array}
    \right)
\]
per cui si ha che
\begin{enumerate}
    \item $\displaystyle{
        H f(1,0) = \left(
            \begin{array}{cc}
                -6 & 0\\
                0 & -4\\
            \end{array}
            \right)
    } \rightarrow \text{masismo}$;
\end{enumerate}

\vspace{2em}
\noindent
\textbf{Esercizio 2}: Si consideri la funzione seguente
\[f(x,y,z) = x^3 - 3x^2 - y^2 - 3z^2\]
calcolando il gradiente della della $f$ si ottiene
\[\nabla f = \left(3x^2 - 6x, -2y, -6z\right){^T}\]
Tramite l'opportuno sistema, si ottiene che i punti critici sono
\[(0,0,0){^T} \hspace{1em} \text{e} \hspace{1em} (2,0,0){^T}\]
per determinarne la natura dei punti critici, si calcola l'Hessiana:
\[
    H f(x,y,z) = \left(
        \begin{array}{cc}
            -6 & 0\\
            0 & -4\\
        \end{array}
        \right)
\]

\vspace{1em}
\noindent
\subsection{Problemi di estremo vincolato}
Fino ad ora sono stati analizzati problemi di estremo libero, in cui il punto considerato era interno al dominio. Ora, invece, si considerano punti di estremo vincolato, ossia punti interni ad un compatto dove esistono certamente massimi e minimi.

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri un contenitore cilindrico. A parità di volume, bisogna rendere minimo il volume. Ovviamente, se si potesse procedere con qualsiasi forma, si sceglierebbe naturalmente la sfera.\\
Detto $r$ il raggio della base del cilindro e $h$ l'altezza dello stesso, si determini il rapporto $\dfrac{r}{h}$ ottimale. Dalla geometria euclidea è noto che
\[V = \pi r^2 h \hspace{1em} \text{e} \hspace{1em} A = 2\pi r h + 2\pi r^2\]
che possono essere trasformate in due funzioni distinte
\[f(x,y) = \pi x^2 y \hspace{1em} \text{e} \hspace{1em} g(x,y) = 2\pi xy + 2pi x^2\]
in cui $r=x$ e $h=y$. Ovviamente ha senso chiedere che
\[f,g : ]0,+\infty[ \times ]0,+\infty[ \longmapsto \mathbb{R}\]
Allora in questo caso bisogna scegliere ciò che si deve massimizzare e ciò che rappresenta un vincolo. Si considera, allora, come vincolo $A=g(x,y)$, e si definisce, per $A \in \mathbb{R}^{+}$ l'\textbf{insieme di livello} seguente:
\[L_A = \{(x,y){^T} \in \Omega : g(x,y) = A\}\]
che è una curva.\\
Allora ha senso cercare il massimo della funzione $f(x,y)$ ristretta alla curva $L_A$; ma essendo
\[L_A : 2\pi xy + 2pi x^2 = A \hspace{1em} \rightarrow \hspace{1em} y = \dfrac{A - 2\pi x^2}{2 \pi x}\]
si può considerare la restrizione di $f$ calcolata come segue
\[f(x,y) = \pi x^2 y \rightarrow f \vert_{L_A} \rightarrow h \left(x,\dfrac{A-2 \pi x^2}{2 \pi x} \right) = \pi x^2 \cdot \dfrac{A - 2\pi x^2}{2\pi x} = \dfrac{1}{2} \cdot \left(A x - 2 \pi x^3\right)\]
Si è ottenuta, quindi, una funzione $h : \mathbb{R} \longmapsto \mathbb{R}$ di cui si può calcolare la derivata
\[h'(x) = \dfrac{1}{2} \cdot (A - 6\pi x^2)\]
da cui si ricava che
\[x = \sqrt{\dfrac{A}{6 \pi}} \hspace{1em} \rightarrow \hspace{1em} y = \dfrac{A - 2\pi \cdot \dfrac{A}{6 \pi}}{2 \pi \cdot \sqrt{\dfrac{A}{6 \pi}}} = \dfrac{\dfrac{2}{3} \cdot A}{2\pi \cdot \sqrt{\dfrac{A}{6 \pi}}}\]
Calcolando, ora, il rapporto tra $x$ e $y$ si ottiene $\displaystyle{\dfrac{x}{y} = \dfrac{1}{2}}$.

\vspace{2em}
\noindent
\textbf{Esercizio}: Si consideri la funzione
\[f(x,y) = x+y\]
sul dominio compatto $\mathcal{B(0,1)}$. Si determinino i massimi e minimi assoluti. Tuttavia, si ha che
\[\nabla f(x,y) = (1,1){^T}\]
che non si potrà mai annullare, per cui non ci sono punti critici. Ciò non sorprende in quanto il dominio non è aperto, per cui non si applica il teorema di Fermat. Ciò, quindi, indica che gli estremi stanno agli estremi.\\
Si deve, quindi, considerare la curva di livello
\[L_1 : \{(x,y){^T} \in \mathbb{R}^2 : x^2+y^2=1\}\]
Da notare che tale curva non è una funzione, per cui no si può considerare la composizione come in precedenza. Pertanto si deve procedere tramite parametrizzazione, per cui
\[L_1 = \{\gamma(t) : t \in [0,2\pi]\}\hspace{1em} \text{con} \hspace{1em} \gamma(t) = (\cos(t),\sin(t)){^T}\]
Studiare $f$ ristretta a $L_1$ equivale a studiare la funzione composta
\[f \circ \gamma : [0,2 \pi] \longmapsto \mathbb{R}\]
con $h(t) = \cos(t) + \sin(t)$. Allora si ha che
\[h'(t) = - \sin(t) + \cos(t)\]
ma si ha che $h'(t) = 0$ quando $\sin(t)=\cos(t)$ che si verifica quando
\[t=\dfrac{\pi}{4} \hspace{1em} \text{e} \hspace{1em} t=\dfrac{5}{4} \pi\]
Sostituendo si ottiene che
\[h \left( \dfrac{\pi}{4}\right) = \sqrt{2} \hspace{1em} \text{e} \hspace{1em} h \left(\dfrac{5}{4} \pi\right) = - \sqrt{2}\]
ma è noto che Fermat può essere applicato solamente sull'aperto, per cui bisogna controllare anche gli estremi $0$ e $2\pi$ che producono come risultato $h(0)=1$ e $h(2\pi)=1$. Si conclude che
\[\min f = - \sqrt{2} \hspace{1em} \text{e} \hspace{1em} \max f = \sqrt{2}\]

\newpage
\noindent
\begin{center}
    8 Novembre 2022
\end{center}
Nel caso in cui vi siano $N=2$, i vincoli possibili. Il dominio singolare a tratti si ottiene che $D \subseteq \mathbb{R}^2$; ciò significa che $D = \overline{A}$, con $A$ aperto e la frontiera di $A$ è una curva regolare a tratti.

\vspace{1em}
\noindent
\textbf{Ossservazione}: Nel caso in cui $n=3$, ossia in tre dmensioni, allora si avrà una curva, la quale può essere descritta
\begin{itemize}
    \item in forma parametrica
    \[\gamma : I \longmapsto \mathbb{R}^3\]
    per cui si andrà a studiare la composta $f \circ \gamma$.

    \item nel caso in cui la curva sia espressa in forma parametrica, con $Z_{g_1} \cap Z_{g_2}$, si andrà a considerare
    \[\{(x,y,z){^T} \in \mathbb{R}^3 : g_1(x,y,z) = 0\}\]
\end{itemize}
si potrebbe trattare anche di una curva
\begin{itemize}
    \item nel caso in cui si abbia una curva \[\sigma : \Omega \subset \mathbb{R}^2 \longmapsto \mathbb{R}^3\]
    allora si andrà a considerare la composizione
    \[f \circ \sigma : \Omega \longmapsto \mathbb{R}\]
    per cui ci si riconduce al caso di $\mathbb{R}^2$.
\end{itemize}
Nel caso di una superficie implicita come segue
\[Z_g = \{(x,y,z){^T} \in \mathbb{R}^3 : g(x,y,z) = 0\}\]
si potrà procedere attraverso una parametrizzazione, per lo meno locale, tramite le coordinate sferiche.

\vspace{2em}
\noindent
\textbf{Osservazione}: Si osservi che quando si considera una curva $g(x,y)$ nel piano, poter descrivere la stessa, a tratti, tramite il grafico della funzione $y=f(x)$ in una sola variabile, non è possibile descrivere la curva quando la tangente è verticale; viceversa, se si considera una funzione $x=h(y)$ non potrà descrivere la curva a tratti dove la tangente è orizzontale.\\
Non sarà mai possibile descrivere il grafico della curva quando essa si interseca con se stessa.

\vspace{1em}
\subsection{Teorema della funzione implicita (U. Dimi) in dim $n=2$}
Sia $A \subseteq \mathbb{R}^2$ aperto, con $g : A \longmapsto \mathbb{R}$ continua.\\
Esista e sia continua in $A$ la funzione
\[\dfrac{\partial g}{\partial y} (x,y)\]
Sia ... tale che $g(x_0,y_0) = 0$. Inoltre
\[\dfrac{\partial g}{\partial y}(x_0,y_0) \neq 0\]
ovvero significa che la tengente la curva non deve essere verticale.\\
Inoltre esista un intorno $U$ di $x_0$ e un intorno $V$ di $y_0$ tale che $\forall x \in U$ esiste uno e un solo $y \in V$ tale per cui $g(x,y)=0$, quindi definiamo una nuova funzione
\[\phi : U \longmapsto V\]
tale per cui $\phi(x) = y$ \textbf{se e solo se} $g(x,\phi(x)) = 0$ (ovvero $\phi$ è la funzione implicitamente definita dall'equazione $g(x,y) = 0$).\\
Inoltre si ha che $\phi$ è continua e il grafico della funzione
\[\{(x,\phi(x)){^T} \in A : x \in U\} = Z_g \cap (U \times V)\]
ossia 

\vspace{2em}
\noindent
\textbf{Osservazione}: È noto che il gradiente è sempre ortogonale alla curva di livello, ma essendo definito
\[\nabla g(x_0,y_0) = \left(\dfrac{\partial g}{\partial x}(x_0,y_0), \dfrac{\partial g}{\partial y} (x_0,y_0)\right){^T}\]
per cui se il secondo termine è uguale a $0$, signfica necessariamente che il si ha una tangente orizzontale.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Alla base della dimostrazione\\
Si suppone che
\[\dfrac{\partial g}{\partial y}(x_0,y_0) > 0\]
È noto per ipotesi che è diverso da zero. Per la continuità della funzione
\[\dfrac{\partial g}{\partial y}\]
esiste un intorno $U_0 \times V_0$ di $(x_0,y_0){^T}$ (che si può naturalmente considerare come un rettangolo inscritto in un'opportuna palla) dove
\[\dfrac{\partial g}{\partial y}(x,y) > 0\]
per ogni $x \in U_0$ e per ogni $y \in V_0$. Si può considerare come intorno $V_0 = \left]y_0-r,y_0+r\right[$.\\
Studiando la funzione
\[h(y) = g(x_0,y) : \left]y_0-r,y_0+r\right[ \longmapsto \mathbb{R}\]
in cui $x$ è fissato e si muove $y$. Si ha, in particolare, che
\[h'(y) = \dfrac{\partial g}{\partial y} > 0 \hspace{1em} \forall y \in V_0\]
quindi $h$ è strettamente crescente. In particolare
\[g(x_0,y_0-r) < g(x_0,y_0) = 0 < g(x_0,y_0+r)\]
Si consideri, ora, la funzione
\[k(x) = g(\cdot,y_0-r) : U_0 \longmapsto \mathbb{R}\]
in cui è immediato che $k(x_0) < 0$, quindi per la continuità di $k(x)$ e per il teorema della permanenza del segno esiste un intorno $U_1$ di $x_0$ in cui si ha che
\[g(x,y_{0}-r) < 0 \hspace{1em} \forall x \in U_1\]
Si considera ora la funzione 
\[g(\cdot,y_0+r) : U_0 \longmapsto \mathbb{R}\]
in cui è immediato che $g(x_0,y_0+r) > 0$, quindi per la continuità della funzione e per il teorema della permanenza del segno esiste un intorno $U_2$ di $x_0$ in cui si ha che
\[g(x,y_{0}+r) > 0 \hspace{1em} \forall x \in U_2\]
In definitiva, preso $x \in U_0 \cap U_1 \cap U_2$ si ha che
\[g(x,y_0-r) < 0 < g(x,y_0+r)\]
Per il teorema di esistenza degli zeri di Bolzano applicato alla funzione $g(x,\cdot)$ in cui è fissata $x$ e si muove $y$, definita come
\[g(x, \cdot) : ]y_0-r,y_0+r[ \longmapsto \mathbb{R}\]
esiste uno zero $\phi(x) \in ]y_0-r,y_0+r[$ tale che $g(x,\phi(x)) = 0$.\\
Si osservi che tale zero è unico in quanto la funzione $g(x,\cdot)$ è strettamente crescente nell'intorno considerato, in quanto
\[\dfrac{\partial g}{\partial y}(x_0,y_0) > 0\]
per quanto assunto in principio. Si è, così, definita la funzione implicita
\[\phi : U = U_0 \cap U_1 \cap U_2 \longmapsto V = V_0\]
Per dimostrare che $\phi$ è continua, si considera arbitrariamente $x_1 \in U$ e si fissa $\epsilon>0$; si cerchi, allora, un intorno $\Omega$ di $x_1$ tale che
\[\left \vert \phi(x) - \phi(x_1) \right \vert < \epsilon \hspace{1em} \forall x \in \Omega\]
Sia $W$ l'intervallo $]\phi(x_1)-\epsilon,\phi(x_1)+\epsilon[$. È possibile supporre $\epsilon$ abbastanza piccolo tale per cui $W \subset V$.\\
Allora la funzione
\[g(x,\phi(x))\]
è strettamente crescente, per cui si ha che
\[g(x,\phi(x_1) - \epsilon) < g(x,\phi(x_1)) = 0 < g(x,\phi(x_1)+\epsilon)\]
Esiste, quindi, un intorno $\Omega$ di $x_1$ tale per cui $\forall x \in \Omega$ si ha che
\[g(x,\phi(x_1)-\epsilon) < 0 < g(x,\phi(x_1)+\epsilon)\]
Dal momento che che $g(x,\cdot)$ è strettamente crescente, si ha che
\[\phi(x_1)-\epsilon<\phi(x)\]
perchè se fosse $\phi(x_1) - \epsilon \geq \phi(x)$ si avrebbe che $g(x,\phi(x)) \leq g(x,\phi(x_1)-\epsilon) < 0$.\\
Similmente si avrebbe che $\phi(x) < \phi(x_1) + \epsilon$, in quanto se fosse $\phi(x) \geq \phi(x_1) + \epsilon$ si avrebbe che $0=g(x,\phi(x_1)) \geq g(x,\phi(x_1)+\epsilon) > 0$
quindi...

\vspace{1em}
\subsubsection{Teorema della funzione implicita (U. Dimi) in dim $n=3$}
Sia $A \subseteq \mathbb{R}^3$ aperto, con $g : A \longmapsto \mathbb{R}$ continua.\\
Esista $(x_0,y_0,z_0){^T} \in A$ tale per cui $g(x_0,y_0,z_0) = 0$. Esista e sia continua in $A$ la funzione
\[\dfrac{\partial g}{\partial z} (x,y,z)\]
Inoltre deve essere che
\[\dfrac{\partial g}{\partial z}(x_0,y_0,z_0) \neq 0\]
ovvero significa che la tangente la curva non deve essere verticale.\\
Allora esista un intorno $U_0$ di $(x_0,y_0){^T}$ e un intorno $V_0$ di $z_0$ tale che $\forall (x,y){^T} \in U_0$ esiste uno e un solo $z_0 \in V_0$ tale che $g(x,y,z) = 0$. Questo definisce la funzione
\[\phi : U_0 \longmapsto V_0\]
con $g(x,y,g(x,y)) = 0$ e $\phi$ continua. Inoltre si ha che
\[\{(x,y,\phi(x,y)){^T} \in A : (x,y){^T} \in U_0\} = Z_g \cap (U_0 \times V_0)\]

\vspace{1em}
\subsubsection{Teorema della funzione implicita (U. Dimi) in dim $n$}
Nel caso generale, si sta considerando la funzione
\[g : \mathbb{R}^{n+k} \longmapsto \mathbb{R}^k\]
in cui si va considerando
\[\{(x,y){^T} \in A : g(x,y) = 0\}\]
in cui, però, $x \in \mathbb{R}^n$ e $y \in \mathbb{R}^k$ e $0 \in \mathbb{R}^k$.

\vspace{1em}
\subsection{Teorema di derivabilità della funzione implicita}
Siano verificate tutte le ipotesi del teorema della funzione implicita e inoltre si supponga che $g \in C^1(A)$, ossia sia derivabile con derivata continua. Allora $\phi$ è derivabile su $U$ e si ha che
\[\phi'(x) = - \dfrac{\dfrac{\partial g}{\partial x} (x,\phi(x))}{\dfrac{\partial g}{\partial y}(x,\phi(x))}\]
in cui è possibile dividere per la derivata esseno non nulla per ipotesi.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Per definizione si ha che
\[\phi'(x) = \lim_{h \to 0} \dfrac{\phi(x+h) - \phi(x)}{h}\]
ma si ha che $\forall x \in U$
\[g(x,\phi(x)) = \]
... continua ...\\
Ma ciò signifca che
\[0 = g(x+h,\phi(x+h)) - g(x,\phi(x)) = \left<\nabla g(\xi,n), \left(h,\phi(x+h)-\phi(x)\right){^T} \right>\]
con $(\xi,n){^T}$ appartenente al segmento di estremi $(x,\phi(x)){^T}$ e $(x+j,\phi(x+h)){^T}$. Pertanto si ottiene che
\[\left<\nabla g(\xi,n), \left(h,\phi(x+h)-\phi(x)\right){^T} \right> = \dfrac{\partial g}{\partial x}(\xi,n) \cdot h + \dfrac{\partial g}{\partial y} (\xi,n) \cdot (\phi(x+h) - \phi(x))\]
dividendo per $h$ si ottiene che
\[\lim_{h \to 0} \dfrac{\phi(x+h) - \phi(x)}{h} = - \dfrac{\dfrac{\partial g}{\partial x} (\xi,n)}{\dfrac{\partial g}{\partial y}(\xi,n)}\]
ma siccome $x_0 < \xi < x_0+h$ e $\phi(x+h)$ e continua, quando $h \to 0$, si ha che $\xi = x$, mentre $\phi(x+h)=\phi(x)$
per cui si ottiene
\[\phi'(x) = - \dfrac{\dfrac{\partial g}{\partial x} (x,\phi(x))}{\dfrac{\partial g}{\partial y}(x,\phi(x))}\]

\vspace{1em}
\subsection{Teorema di parametrizzazione locale}
Sia data la funzione
\[g : A \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}\]
tale per cui $g \in C^1(A)$. Sia $(x_0,y_0){^T} \in A$ tale che $\nabla g(x_0,y_0) \neq (0,0){^T}$.\\
Allora localmente la curva $\mathcal{Z}_g$ è parametrizzabile, ossia esiste un intorno $U$ di $(x_0,y_0){^T}$ ed esista $\gamma \in I \subseteq \mathbb{R} \longmapsto \mathbb{U}$ regolare (quindi $\forall t \in I, \exists \gamma'(t) \neq 0$) tale che $\mathcal{Z}_g \cap U = \{\gamma(t) : t \in I\}$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Se si ha che
\[\dfrac{\partial g}{\partial y} (x_0,y_0) \neq 0\]
per il teorema della funzione implicita, esistono due intorni $U$ e $I$ con $\phi : U \longmapsto I$ tale che
\[\{(x,\phi(x)){^T : x \in U}\} = \mathcal{Z}_g \cap U\]
quindi la funzione è parametrizzabile.

\vspace{1em}
\noindent
Se si ha che
\[\dfrac{\partial g}{\partial y} (x_0,y_0) = 0\]
allora si può applicare nuovamente il teorema della funzione implicita semplicemente scambiando il ruolo delle variabile e in modo tale da ottenere una funzione implicita $\psi(y)$ tale che
\[\mathcal{Z}_g \cap U = \{(\phi(y),y){^T} : y \in I\}\]

\vspace{1em}
\noindent
Il medesimo ragionamento è applicabile alle superfici. Infatti se
\[g : A \subseteq \mathbb{R}^3 \longmapsto \mathbb{R}\]
con $g \in C^1(A)$ tale per cui
\[\nabla g(x_0,y_0,z_0){^T \neq (0,0,0){^T}}\]
allora localmente la funzione è parametrizzabile...continua...

\vspace{2em}
\noindent
\textbf{Osservazione}: Data la funzione
\[g : A \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}\]
supposta $g \in C^1(A)$
e presa in considerazione la linea di livello
\[L_\alpha = \mathcal{Z}_{g-\alpha} = \{(x,y){^T} \in A : g(x,y) - \alpha = 0\}\]
e per dimostrare che $\nabla g$ è ortogonale a $L_\alpha$, se $\gamma$ è parametrizzabile localmente in un intorno. Ciò significa che
\[g(x,y) = \alpha \hspace{1em} \forall (x,y){^T} \in A\]
e 
\[\dfrac{\partial g}{\partial x} = 0 \hspace{1em} \text{e} \hspace{1em}\]
ma ciò è vero in quanto
\[\left<\nabla g(x,y), \phi'(x)\right> = 0\]
questo in quanto dove il gradiente è nullo, allora il prodotto scalare è nullo. Dove, invece, non è nullo è possibile parametrizzare la curva.

\newpage
\noindent
\begin{center}
    9 Novembre 2022
\end{center}
È stato introdotto in precedenza il teorema della derivata della funzione implicita. Data, quindi la funzione
\[\phi : U \longmapsto V\]
sapendo che $g(x,\phi(x))=0$ per $\forall x \in U$. Per calcolare la derivata della funzione si scrive il limite del rapporto incrementale
\[\phi'(x) = \lim_{h \to 0} \dfrac{\phi(x+h) - \phi(x)}{h}\]
ma siccome è noto che $g(x,\phi(x))=0$ per $\forall x \in U$, si ha che
\[0 = g(x+h,\phi(x+h)) - g(x,\phi(x))\]
per cui per la formula del valor medio si ha che
\[g(x+h,\phi(x+h)) - g(x,\phi(x)) = \left<\nabla g(\xi,\eta),(h,\phi(x+h)-\phi(x)){^T} \right>\]
posto $(\xi,\eta){^T} \in \overline{(x,\phi(x)){^T}-(x+h,\phi(x+h)){^T}}$.
... continua...

\vspace{1em}
\noindent
\textbf{Osservazione}: Ovviamente si possono calcolare anche le derivate di ordine $n$ di $\phi$, nell'ipotesi in cui $g \in C^n(A)$. Nel caso in cui $n=2$ si ha che
\[\phi''(x) = \dfrac{\dif}{\dif x} \left[- \dfrac{g_x(x,\phi(x))}{g_y(x,\phi(x))}\right] = - \dfrac{1}{g_y(x,\phi(x))^2} \cdot \left[\dots\right]\]
in quanto
\[\dfrac{\dif }{\dif x} g_x(x,\phi(x)) = \left<\nabla g_x(x,\phi(x)), (1,\phi'(x)){^t}\right> = g_{xx}(x,\phi(x)) \cdot 1 + g_{xy} (x,\phi(x)) \cdot \phi'(x)\]
conoscendo $\phi'(x)$ si ottiene
\[g_{xx}(x,\phi(x)) \cdot 1 + g_{xy} (x,\phi(x)) \cdot \left[- \dfrac{g_x(x,\phi(x))}{g_y(x,\phi(x))}\right]\]
e, similmente,
\[\dfrac{\dif }{\dif x} g_y(x,\phi(x)) = \left<\nabla g_y(x,\phi(x)), (1,\phi'(x)){^t}\right> = g_{yx}(x,\phi(x)) \cdot 1 + g_{yy} (x,\phi(x)) \cdot \phi'(x)\]
conoscendo $\phi'(x)$ si ottiene
\[g_{yx}(x,\phi(x)) \cdot 1 + g_{yy} (x,\phi(x)) \cdot \left[- \dfrac{g_x(x,\phi(x))}{g_y(x,\phi(x))}\right]\]
che può essere riscritto come
\[=-\dfrac{1}{g_y^3} \cdot \left[g_{xx} \cdot g_y^2 - 2 \cdot g_{xy} \cdot g_x \cdot g_y + g_{yy} \cdot g_x^2\right]\]

\vspace{2em}
\noindent
\textbf{Esercizio 1}: Si consideri la funzione
\[g(x,y) = e^{xy}+x^2-y^2-e \cdot (x+1) + 1\]
\begin{itemize}
    \item Si trovino eventuali $y \in \mathbb{R}$ tali che $g(0,y)=0$.\\
    Si consideri, allora, la funzione
    \[h(y)=g(0,y)=e^{-y}-y^2-e+1\]
    calcolandone la derivata, si ottiene
    \[h'(y) = -e^{-y}-2y\]
    ma studiare il segno di tale funzione non è immediato, per cui si calcola la derivata seconda
    \[h''(y) = e^{-y}-2\]
    si osserva che $h''(y)<0, \hspace{0.25em} \forall y$ e $h'(-\log(2))=0$. Pertanto si ottiene che $y=-\log(2)$ è punto di massimo per la funzione e siccome la funzione è sempre decrescente, con $(0,-1){^T} \in \mathcal{Z}_g$.

    \item Pertanto, presa la funzione
    \[g(x,y) = e^{xy}+x^2-y^2-e \cdot (x+1) + 1\]
    ci si chiede se sia possibile rappresentare la curva nell'intorno del punto $(0,-1){^T}$. La risposta è sì se valgono le ipotesi del teorema di Dini. Per verificarlo, si ha che
    \[\nabla g(x,y) = \left(e^{x-y}+2x-e,-e^{x-y}-2y\right){^T}\]
    e calcolata nel punto di interesse si ottiene
    \[\nabla g(0,-1) = \left(0,-e+2\right){^T}\]
    ed essendo
    \[\dfrac{\partial}{\partial y} g(0,-1) \neq 0\]
    esiste la funzione implicita.
    
    \item È immediato osservare che $\phi'(0)=0$, in quanto
    \[\phi'(x)=-\dfrac{g_x(0,\phi(x))}{g_y(x,\phi(x))} \hspace{1em} \rightarrow \hspace{1em} \phi'(0) = - \dfrac{g_x(0,-1)}{g(y(0,-1))} = 0\]

    \item Calcolando la derivata seconda della funzione si ottiene che
    \[\phi''(x) = -\dfrac{1}{g_y^3} \cdot \left[g_{xx} \cdot g_y^2 - 2g_{x,y} g_x g_y + g_yy g_x^2\right]\]
\end{itemize}

\vspace{2em}
\noindent
\textbf{Esercizio 2}: Si consideri la funzione
\[g(x,y) = (y-1)^2-x^2 \cdot (1-x^2)\]
Per studiare tale funzione si chiede
\[(y-1)^2=x^2 \cdot (1-x^2)\]
ma siccome $(y-1)^2 \geq 0 \hspace{0.25em} \forall y$, ma allora deve essere
\[x^2 \cdot (1-x^2) \geq 0 \hspace{1em} \rightarrow \hspace{1em} 1-x^2 \geq 0 \hspace{1em} \rightarrow \hspace{1em} \vert x \vert < 1\]
Studiando la funzione $x^2 \cdot (1-x^2)$ e facendo una sostituzione con $t=x^2$ si ottiene che tale parabola presenta un massimo in $t=\dfrac{1}{2}$, pertanto si ha che il massimo della funzione è
\[\dfrac{1}{2} \cdot (1-\dfrac{1}{2}) = \dfrac{1}{4}\]
e quindi cià significa che
\[(y-1)^2 \leq \dfrac{1}{4} \hspace{1em} \rightarrow \hspace{1em} y \leq \dfrac{1}{2} + 1 = \dfrac{3}{2}\]
Pertanto si ottiene che la curva è limitata in un compatto. È possibile ora calcolare il gradiente della $g(x,y)$ per cui
\[\nabla g(x,y) = \left(2x \cdot (2x^2 - 1), 2 \cdot (y-1)\right){^T}\]
È immediato evincere che
\[\dfrac{\partial g}{\partial x} (x,y) = 0 \hspace{1em} \text{se} \hspace{1em} x=0 \hspace{1em} \text{oppure} \hspace{1em} x=\pm\dfrac{1}{\sqrt{2}}\]
da cui è immediato evincere che
\[\nabla g(0,1) = (0,0){^T}\]
per cui qui non vi si potrà calcolare la funzione implicita.\\
Studiando, invece, cosa accade quando $x=\dfrac{1}{\sqrt{2}}$ si ottiene
\[0 = (y-1)^2 - \dfrac{1}{2} \left(1-\dfrac{1}{2}\right) \hspace{1em} \rightarrow \hspace{1em} \vert y-1 \vert = \dfrac{1}{2}\]
che permette di ottenere
\[y=\frac{1}{2} \hspace{1em} \text{oppure} \hspace{1em} y=\frac{3}{2}\]

\vspace{1em}
\noindent
\subsection{Teorema del moltiplicatore di Lagrange per la determinazione dei punti di estremo vincolato ad una curva implicita}
In generale, se esiste un vincolo che si può parametrizzare, è possibile studiare la composizione della funzione rispetto a tale vincolo e ridurre la dimensione del problema.\\
Tuttavia non è sempre possibile parametrizzare il vincolo, per il teorema di cui sopra consente di individuare punti di estremo vincolato per una curva.\\\\

\vspace{1em}
\noindent
Sia $f,g : A \subseteq \mathbb{R}^2 \longmapsto \mathbb{R}$ con $A$ aperto. Siano $f,g \in C^1(A)$ e sia dato l'insieme
\[\mathcal{Z}_g = \{(x,y){^T} : g(x,y) = 0\}\]
Sia $(x_0,y_0){^T} \in \mathcal{Z}_g$ un punto di estremo per $f$ vincolato a $\mathcal{Z}_g$ e sia $\nabla g(x_0,y_0) \neq (0,0){^T}$, che assicura la parametrizzazione locale per la funzione.\\
Allora esiste $\lambda \in \mathbb{R}$ (chiamato \textbf{moltiplicatore di Lagrange}) tale che
\[\nabla f(x_0,y_0) = \lambda \cdot \nabla g(x_0,y_0)\]
cioè il punto $(x_0,y_0, \lambda){^T}$ è soluzione del sistema
\[
    \rowcolors{1}{white}{white}
    \left\{
    \begin{array}{l}
        \dfrac{\partial f}{\partial x} (x_0,y_0) = \lambda \cdot \dfrac{\partial g}{\partial x}(x_0,y_0)\\
        \dfrac{\partial f}{\partial y} (x_0,y_0) = \lambda \cdot \dfrac{\partial g}{\partial y}(x_0,y_0)\\
        g(x_0,y_0) = 0
    \end{array}
    \right.
\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Dal momento che $\nabla g(x_0,y_0) \neq (0,0){^T}$ esiste una parametrizzazione locale del vincolo, cioè esiste
\[\gamma : I \longmapsto A\]
e un intorno $\Omega$ di $(x_0,y_0){^T}$ tale che
\[\mathcal{Z}_g \cap \Omega = \{\gamma(t) : t \in I\}\]
ossia è il sostegno...\\
Si consideri, allora, la funzione composta
\[f \circ \gamma : I \longmapsto \mathbb{R}\]
la quale è derivabile in quanto $f$ è derivabile per ipotesi e $\gamma$ presenta una parametrizzazione regolare grazie al fatto che $f$ è derivabile.\\
Se, allora, si assume che $\gamma(0)=(x_0,y_0){^T}$ un punto di minimo vincolato per $f$, allora la funzione composta presenta un minimo in $t=0$. Quindi ciò significa che
\[\dfrac{\dif}{\dif t} (f \circ \gamma) (0) = 0\]
ma la derivata della funzione composta è
\[\dfrac{\dif}{\dif t} (f \circ \gamma) (0) = \left<\nabla f(\gamma(0)), \gamma'(0)\right> = 0\]
per cui il gradiente della composta è ortogonale al vincolo. Non solo, ma anche $\nabla g(x_0,y_0)$ è ortogonale al vincolo, ma ciò accade sempre. Ma siccome $\nabla f(x_0,y_0)$ e $\nabla g(x_0,y_0)$ sono paralleli tra loro, in quanto ortogonali al medesimo vincolo. Essendo paralleli, deve esistere $\alpha, \beta \in \mathbb{R}$ \textbf{non entrambi nulli} tali che
\[\alpha \cdot \nabla f(x_0,y_0) + \beta \cdot \nabla g(x_0,y_0) = 0\]
e siccome $\beta$ può essere nullo, $\alpha$ non può essere nullo perché altrimenti $\nabla g(x_0,y_0) =0$, che contrasta con l'ipotesi assunta. Ciò significa che è possibile dividere per $\alpha$, ottenendo
\[f(x_0,y_0) - \dfrac{\beta}{\alpha} \cdot g(x_0,y_0) = 0\]
in cui è sufficiente imporre
\[\lambda = - \dfrac{\beta}{\alpha}\]

\vspace{1em}
\noindent
\textbf{Esempio 1}: Siano date le due funzioni seguenti
\[f(x,y)=2x+y \hspace{1em} \text{e} \hspace{1em} g(x,y)=xy-1\]
e si cerchino ${\max}/{\min}$ di $f$ vincolata al vincolo $\mathcal{Z}_g=xy-1=0$.\\
Allora calcolando il gradiente delle due funzioni
\[\nabla f(x,y) = (2,1){^T} \hspace{1em} \text{e} \hspace{1em} \nabla g(x,y) = (y,x){^T}\]
Impostando il teorema di Lagrange si ottiene 
\[
    \rowcolors{1}{white}{white}
    \left\{
    \begin{array}{l}
        2=\lambda y\\
        1=\lambda x\\
        xy-1=0\\
    \end{array}
    \right.
\]
per cui si ottiene che $x=\pm\dfrac{1}{\sqrt{2}}$ e $y=\pm\sqrt{2}$

\vspace{1em}
\noindent
\textbf{Esempio 2}: Si consideri la funzione seguente
\[g(x,y) x^2-xy+y^2-1 = 0\]
il quale rappresenta un ellisse ruotato. Allora si consideri la funzione $f(x,y)=xy$. Naturalmente si sta cercando
\[\underbrace{(x,y){^T} \in E}{\max}/{\min} f(x,y)\]
in cui $E=\{(x,y){^T} \in \mathbb{R}^2 : x^2-xy+y^2=1\}$. Calcolando i gradienti delle due funzioni
\[\nabla f = (y,x){^T} \hspace{1em} \text{e} \hspace{1em} \nabla g = (2x-y,2y-x){^T}\]
allora si ottiene il sistema semplificato seguente
\[
    \rowcolors{1}{white}{white}
    \left\{
    \begin{array}{l}
        x+y=2\lambda y - x + 2\lambda x - y\\
        2 \cdot (x+y) = 2 \lambda \cdot (x+y)
    \end{array}
    \right.
\]
Dall'equazione seguente $2 \cdot (x+y) = 2 \lambda \cdot (x+y)$ si ha che $(x+y) \cdot (\lambda-1)=0$, per cui
\begin{itemize}
    \item se $x+y=0$, ovvero $y=-x$ si ottiene che inserendo tali valori nel vincolo
    \[g(x,-x) = -x (-x) + (-x)^2=1 \hspace{1em} \rightarrow \hspace{1em} x=\pm \dfrac{1}{\sqrt{3}} \hspace{1em} \text{e} \hspace{1em} y=\mp \dfrac{1}{\sqrt{3}}\]
    \item se $\lambda=1$ si ha che ... continua ...
\end{itemize}

\vspace{1em}
\noindent
\textbf{Esempio 3}: Si considerino le funzioni seguenti
\[g(x,y) = (x-1)^3 - y^2 \hspace{1em} \text{e} \hspace{1em} f(x,y) = x^2 + y^2\]
Studiando quando $g(x,y)=0$ si ottiene
\[(x-1)^3 = y^2\]
e siccome $y^2 \geq 0$, deve essere che $x \geq 1$. Studiano ora i gradienti si ottiene che
\[\nabla f = (2x,2y){^t} \hspace{1em} \text{e} \hspace{1em} \nabla g = (3 \cdot (x-1)^2, -2y){^T}\]
per cui si ottiene il sistema di Lagrange seguente
\[
    \rowcolors{1}{white}{white}
    \left\{
    \begin{array}{l}
        2x=3\lambda \cdot (x-1)^2\\
        2y = -2 \lambda y\\
        (x-1)^3 = y^2
    \end{array}
    \right.
\]
ma tale sistema non è risolvibile, ossia non ha soluzioni. Tuttavia non è vero che il sistema non ha soluzioni; infatti se si calcola
\[f(1,0)=1=\min f\]
che è un punto critico non individuato dal teorema di Lagrange in quanto $\nabla g(1,0) = (0,0){^T}$.

\newpage
11 Novembre 2022

\vspace{1em}
\noindent
... continua ...

\vspace{2em}
\subsection{Teorema dei moltiplicatori di Lagrange (caso in $\mathbb{R}$)}
Siano date due funzioni
\[f,g = {g_1, g_2}{^T} : A \longmapsto \mathbb{R}^2\]
con $f : \Omega \subseteq \mathbb{R}^3 \longmapsto \mathbb{R}$. Allora
\[\mathcal{Z}_g = \{(x,y,z) \in A : g(x,y,z) = g(x,y,z) = 0\}\]
con $(x_0,y_0,z_0){^T} \in \mathcal{Z}_g$ punto di estremo vincolato ...continua...\\
Sia, chiede, in particolare che a $\mathcal{Z}_g$ per $f$. Si deve richiedere che

\[
    \left(
        \begin{array}{ccc}

        \end{array}
    \right)
\]
in cui il rango della matrice deve essere massimo, ossia rg$(A)=2$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: La dimostrazione è per la lode... Da fareeeeee.

\vspace{2em}
\noindent
\textbf{Esempio}: Se si vuole trovare la formula per la distanza tra una retta $y=mx+q$ e l'origine $(0,0){^T}$ basta considerare il
\[\min \{d((x,y){^T},(0,0){^T}) \hspace{1em} \text{tale che} \hspace{1em} (x,y) \in \text{ retta }\}\]
Quindi sarà sufficiente considerare la funzione distanza
\[h(x,y)=\sqrt{x^2+y^2} \hspace{1em} \text{oppure} \hspace{1em} f(x,y) = x^2+y^2\]
vincolata, però, alla retta di equazione $y=mx+q$. Ovviamente si ha che
\[\mathcal{Z}_g = \{(x,y){^T} \in \mathbb{R}^2 : y=mx+q\}\]
per cui si ha che
\[g(x,y) = mx+q-y\]
Se ne calcola il gradiente, ottenendo
\[\nabla f(x,y) = (2x,2y){^T}\]
e per quanto riguarda la funzione $g$ si ottiene
\[\nabla g(x,y) = (m,-1){^T}\]
per cui si ottiene il sistema di Lagrange seguente
\[
   \left\{
        \begin{array}{l}
            2x = \lambda m\\
            2y = -\lambda\\
            mx+q-y=0
        \end{array}
   \right.
\]
per cui si ottiene che $y=-\dfrac{1}{2}\lambda$, mentre $x=\dfrac{1}{2}\lambda m$ e tramite l'ultima equazione, ossia l'equazione del vincolo, si ottiene che $\lambda=-\dfrac{2q}{m^2+1}$. Da ciò si ottiene che
\[y^*=\dfrac{q}{m^2+1} \hspace{1em} \text{e} \hspace{1em} x^*=-\dfrac{qm}{m^2+1}\]
Sono stati trovati, quindi, i punti cercati. Sarà sufficiente, ora calcolare
\[f(x^*,y^*) = \dfrac{q^2m^2}{(m^2+1)^2} + \dfrac{q^2}{(m^2+1)^2}\]
Ma ovviamente la funzione di partenza è $h = \sqrt{f}$, per cui si ottiene ...continua...

\vspace{2em}
\noindent
\textbf{Esercizio}: Si consideri il seguente insieme
\[E = \{(x,y,z){^T} \in \mathbb{R}^3 \hspace{1em} \text{tale che} \hspace{1em} z \in [0,1] \hspace{1em} \text{con} \hspace{1em} x^2 + y^2 \leq z\}\]
Si calcoli, allora, $\underset{E}{{\max}/{\min}} f(x,y,z)$ con $f(x,y,z) = x-z$

\vspace{2em}
\noindent
È immediato osservare che l'insieme $E$ è un compatto. Per studiare tale problema, si considera l'aperto
\[A=\overline{E}\]
Studiando il gradiente di $f$ si ottiene che
\[\nabla f(x,y,z) = (1,0,-1){^T} \neq (0,0,0){^T}\]
\[\{(x,y,z){^T} : z = x^2+y^2 \hspace{1em} \text{con} \hspace{1em} z \in [0,1]\}\]
Si sta considerando, quindi, il grafico di una funzione, per cui si considera la composta
\[f(x,y,x^2+y^2) = h(x,y)\]
in cui $h$, però, è definita solamente sul disco di raggio $1$, ossia
\[h : \{(x,y){^T} \in \mathbb{R}^2 : x^2+y^2 \leq 1\} \longmapsto \mathbb{R}\]
Si è ottenuto ancora una volta un compatto. Per cui bisogna studiare $h$ prima sulla parte interna, ossia su $\{(x,y){^T} \in \mathbb{R}^2 : x^2+y^2 < 1\}$, senza la frontiera. Per trovare i punti critici bisogna calcolare il gradiente
\[\nabla h = (1-2x,-2y){^T}\]
e si ha che $(1-2x,-2y){^T} = (0,0){^T}$ in $\left(\dfrac{1}{2},0\right){^T}$. Pertanto il punto
\[\left(\dfrac{1}{2},0,\dfrac{1}{4}\right){^T}\]
è un punto critico.\\
Studiando, ora, la frontiera
\[\{(x,y){^T} \in \mathbb{R}^2 : x^2+y^2 = 1\}\]
si è ottenuta una circonferenza, la quale può essere parametrizzata come
\[h(\cos(t),\sin(t)) \hspace{1em} \text{con} \hspace{1em} t \in [0,2\pi]\]
Ovviamente derivando si ottiene ...continua...
\[\nabla h = (-\sin(t),\cos(t))\]
che si annulla per ... continua ...

\vspace{1em}
\noindent
Studiando, ora, $f$ su
\[\{(x,y,z){^T} : x^2 + y^2 \leq 1\}\]
si impiegano le coordinate polari, ottenendo,
\[h(\rho,1) = \rho \cos(t) - 1\]
e calcolando il gradiente si ottiene
\[\nabla h(\rho,t) = (\cos(t), - \rho \sin(t)){^T}\]
e si annulla solamente in $(0,\pi,1){^T}$, il quale è un punto critico.

\vspace{1em}
\noindent
Si sono, quindi, ottenuti tutti i punti critici. Calcolandone il valore della $f$ si ottiene
\begin{align*}
    &\left(\dfrac{1}{3},0,\dfrac{1}{4}\right){^T} & \text{con} & f(\cdot) = \dfrac{1}{4}\\
    &\left(-1,0,1\right){^T} & \text{con} & f(\cdot) = -2\\
    &\left(1,0,1\right){^T} & \text{con} & f(\cdot) = 0\\
    &\left(0,0,1\right){^T} & \text{con} & f(\cdot) = -1\\
\end{align*}
in cui è facile capire che il primo è punto di massimo e il secondo di minimo, che esistono in quanto il dominio compatto.

\vspace{2em}
\noindent
\textbf{Esercizio 2}: Si consideri la superficie seguente
\[z^2-xy = 1\]
e si consideri la distanza $d(t,(0,0,0){^T})$
Ovviamente la funzione $f$ è la distanza
\[f(x,y,z) = x^2 + y^2 + z^2\]
mentre la funzione $g$ è
\[g = x^2 - xy - 1\]
e calcolandone i gradienti si ottiene
\[\nabla f = (2x,2y,2z){^T} \hspace{1em} \text{e} \hspace{1em} \nabla g(-y,-x,2z){^T}\]
Si può, quindi, impostare il sistema di Lagrange seguente
\[
    \begin{array}{l}
        2x = -\lambda y\\
        2y = -\lambda x\\
        2z = \lambda 2z\\
        x^2 - xy = 1\\
    \end{array}
\]
Si ottiene dalla terza equazione che $z=0$ oppure $\lambda=1$.\\
Dalle prime due equazioni si ottiene che
\[\dfrac{2x}{2y} = \dfrac{y}{x}\]
per cui
\[x^2-y^2 = 0\]
solamente quando $x=\pm y$. Pertanto si ha che
\begin{itemize}
    \item se $z=0$ ovviamente si ha che $x=\pm 1$, e quindi $y=\mp 1$, da cui si ottengono i punti seguenti
    \[(-1,1,0){^T} \hspace{1em} \text{e} \hspace{1em} (1,-1,0){^T}\]
    \item se $z \neq 0$, allora $\lambda=1$, per cui ovviamente si ha che $x=y=0$, da cui si evince che $z=\pm 1$. Si ottengono i punti seguenti
    \item \[(0,0,1){^T} \hspace{1em} \text{e} \hspace{1em} (0,0,-1){^T}\]
\end{itemize}
Calcolando, ora, il valore della funzione, si ottiene che
\[f()\]
...continua...

\vspace{1em}
\noindent
\textbf{Esercizio 3}: Si considerino le funzioni seguenti
\[f(x,y,z)=xy-2x \hspace{1em} \text{e} \hspace{1em} g_1(x,y,z)=x+y+z \hspace{1em} \text{e} \hspace{1em} g_2(x,y,z)=x^2+y^2+z^2-24\]
e si vuole ottenere il massimo e il minimo della $f$ nell'intersezione delle due funzioni.\\
Si calcola il gradiente di ciascuna funzione
\[\nabla f = (y,x,2){^T} \hspace{1em} \text{e} \hspace{1em} \nabla g_1=(1,1,1) \hspace{1em} \text{e} \hspace{1em} \nabla g_2 = (2x,2y,2z){^T}\]
Si imposta, allora, il sistema di Lagrange, ricordando che deve essere $\nabla f = \lambda \nabla g_1 + \mu \nabla g_2$ ottenendo
\[
    \left\{
        \begin{array}{l}
            y=\lambda + \mu 2x\\
            x=\lambda +\mu 2y\\
            2 = \lambda + \mu 2z\\
            x+y+z=0\\
            x^2+y^2+z^2=24
        \end{array}
    \right.
\]
Dalle prime due equazioni appare evidente che $x=y$ oppure $\mu=-\dfrac{1}{2}$. Dividendo le casistiche si ottiene che
\begin{itemize}
    \item se $x=y$ si evince che $z=-2x$ ed essendo $x^2=4$ si ottiene che $x=\pm 2$. Per cui si ottengono i punti seguenti
    \[(-2,-2,4){^T} \hspace{1em} \text{e} \hspace{1em} (2,2,-4){^T}\]

    \item se $\mu=-\dfrac{1}{2}$ si ottiene facilmente che $\lambda=1$; da ciò si desume che $z=-1$, mentre $x=\dfrac{1 - 3 \cdot \sqrt{5}}{2}$ e $y=\dfrac{1+3 \cdot \sqrt{5}}{2}$
\end{itemize}
Calcolando il valore della funzione $f=2x+2z$ nei punti precedentemente determinati si ottiene
\begin{align*}
    &f(-2,-2.4) = 12\\
    &f(2,2,-4) = -4\\
    &f \left(\dfrac{1-3 \cdot \sqrt{5}}{2}, \dfrac{1+3 \cdot \sqrt{5}}{2}, -1\right) = -13
\end{align*}
Non è difficile evincere che il primo è un punto di massimo mentre il terzo di minimo, che esistono in quanto l'intersezione delle due superficie è un compatto.

\vspace{1em}
\subsection{Integrali dipendenti da parametri}
Si consideri una funzione continua $g(x)$, allora è possibile considerare la funzione
\[f(x) = \int_{\alpha(x)}^{\beta(x)} g(t) \dif t\]
allora se $\alpha(x)$ e $\beta(x)$ sono derivabili si ha che
\[f'(x) = g(\beta(x)) \cdot \beta'(x) - g(\alpha(x)) \cdot \alpha'(x)\]
Tuttavia, se si considera la funzione
\[f(y) = \int_a^b g(x,y) \dif x\]
allora siccome una successione di funzioni $f_n(x)$ si può pensare come una funzione a due variabili della forma $g(x,n)$. È noto che vale la formula seguente
\[\lim_{n \to +\infty} \int_a^b f_n(x) \dif x = \int_a^b \lim_{n \to +\infty} f_n(x)\]
solamente quando la successione converge uniformemente. Esiste una cosa simile anche in questo caso

\vspace{1em}
\subsubsection{Teorema sull'integrale}
Sia data la funzione $g: [a,b] \times I \longmapsto \mathbb{R}$ una funzione continua. Allora la funzione $f : I \longmapsto \mathbb{R}$ definita come
\[f(x) = \int_A^b g(x,y) \dif x\]
è continua su $I$.

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Sia $y_0 \in I$ un punto interno ad $I$ (se $y_0$ fosse un estremo, sarebbe ancora più semplice). Allora si considerano
\[c < y_0 < d \hspace{1em} \text{tale che} \hspace{1em} [c,d] \subset I\]
allora, ovviamente $f \vert_{[a,b] \times [c,d]}$ è uniformemente continua, ovvero che
\[\forall \epsilon > 0, \exists \delta > 0 \hspace{1em} \text{tale che} \hspace{1em} \forall (x_1,y_1){^T}, (x_2,y_2){^T} \in [a,b] \times [c,d] \hspace{1em} \text{con} \hspace{1em} \vert \vert (x_2-x_1,y_2-y_1){^T} \vert \vert < \delta \hspace{1em} \text{si ha che} \hspace{1em} \vert g(x_2,y_2) - g(x_1,y_2) \vert < \epsilon\]
In particolare si ha che
\[\vert f(y) - f(y_0) \vert = \vert \int_a^b (g(x,y) - g(x,y_0)) \cdot \dif x \vert \leq \int_a^b \vert g(x,y) - g(x,y_0) \vert \cdot \dif x < \epsilon \cdot (b-a)\]
che è minore di $\epsilon$ se $\vert \vert (x_2-x_1,y_2-y_1){^T} \vert \vert < \delta$.\\
...continua...

\vspace{1em}
\subsubsection{Teorema sulla derivata}
Sia
\[g : [a,b] \times I \longmapsto \mathbb{R}\]
con $g \in C^1$ e sia data
\[f : I \longmapsto \mathbb{R}\]
con
\[f(y) = \int_a^b g(x,y) \cdot \dif x\]
allora $f$ è derivabile e si ha la formula
\[f'(y) = \lim_{h \to 0} \dfrac{f(y+h) - f(y)}{h} = \lim_{h \to 0} \dfrac{1}{h} \cdot \left(\int_a^b \left[g(x,y+h) - g(x,y)\right] \cdot \dif x\right)\]
ma siccome la funzione $g \in C^1$ si ottiene, per Lagrange, che
\[g(x,y+h) - g(x,y) = \dfrac{\partial g}{\partial y} (x,\eta)\cdot h\]
con $\eta$ appartenente all'intervallo di estremi $y$ e $y+h$. Si ottiene, quindi
\[\lim_{h \to 0} \int_a^b \dfrac{\partial g}{\partial y} (x,\eta) \cdot \dif x = \int_a^b \dfrac{\partial g}{\partial y}(x,y) \cdot \dif x\]
questo per la continuità della derivata $\dfrac{\partial g}{\partial y} g(x,\eta)$

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri la funzione seguente
\[\dfrac{\dif}{\dif y} \int_0^1 \sin(xy^2) \dif x\]
che può essere schematizzata come
\[f(y) = \int_a^y g(x,y) \dif x\]
per cui se si considera la funzione
\[h(u,v) = \int_a^u g(x,y) \cdot dif x\]
si può calcolare l'integrale di partenza come
\[f(x) - h(x,y)\]
allora ovviamente si ottiene che
\[\dfrac{\partial h}{\partial u} (u,v) = g(u,v)\]
e quindi
\[\dfrac{\partial h}{\partial v} (x,yv) = \int_a^v \dfrac{\partial g}{\partial y} (x,v) \dif x\]
in questo modo si ottiene facilmente che
\[f'(x) = \dfrac{\partial h}{\partial u} (u,y) \cdot 1 + \dfrac{\partial h}{\partial v}(y,v) \cdot 1\]
ottenendo
\[g(y,y) = \int_a^y \dfrac{\partial g}{\partial y}(x,y) \cdot \dif x\]

\vspace{2em}
\noindent
\textbf{Osservazione}: Se l'integrale di partenza fosse stato
\[f(y) = \int_{\alpha(y)}^{\beta(y)} g(x,y) \cdot \dif x\]
si sarebbe ottenuta la formula
\[f'(y) = g(\beta(y), y) \cdot \beta'(y) - g(\alpha(y),y) \cdot \alpha'(x) + \int_{\alpha(x)}^{\beta(y)} \dfrac{\partial g}{\partial y} (x,y) \cdot \dif x\]

\vspace{2em}
\noindent
\textbf{Esempio}: Si consideri l'integrale seguente
\[f(y) = \int_{-y^2}^{\sin(y)} \log(x^2 + y) \cdot \dif x\]
allora si ha che
\[f'(y) = \log(\sin(y))^2 + y) \cdot \cos(y) + \log(y^2 + y) \cdot 2y + \int_{-y^2}^{\sin(y)} \dfrac{1}{x^2+y} \cdot \dif x\]

\newpage
\noindent
\begin{center}
    14 Novembre 2022
\end{center}
\textbf{Esercizio 1}: Si consideri l'ellisse $E \subset \mathbb{R}^3$ definito dall'intersezione del cilindro di equazione $x^2+y^2=1$ e del piano di equazione $x+y+z=1$.\\
Si determini la minima e la massima distanza di $E$ dall'origine.\\
Naturalmente si ha che
\begin{itemize}
    \item Il cilindro è $x^2+y^2-1=g_1(x,y,z)$
    \item il piano è $x+y+z-1=g_2(x,y,z)$
    \item Distanza è $x^2+y^2+z^2 = f(x,y,z)$
\end{itemize}
In particolare si calcolano i gradienti, da cui
\[\nabla f = (2x,2y,2z) \hspace{1em} \text{e} \hspace{1em} \nabla g_1 = (2x,2y,0) \hspace{1em} \text{e} \hspace{1em} \nabla g_2 = (1,1,1)\]
Realizzando il sistema di Lagrange si ottiene
\[
    \left\{
    \rowcolors{1}{white}{white}
    \begin{array}{l}
        2x=\lambda 2x + \mu\\
        2y=\lambda 2y + \mu\\
        2y = \mu\\
        x^2+y^2-1=0\\
        x+y+z-1=0
    \end{array}
    \right.
\]
Risolvendo il sistema per $x=y$ si ottengono i punti
\[\left(\dfrac{\sqrt{2}}{2},\dfrac{\sqrt{2}}{2},1-\sqrt{2}\right){^T} \hspace{1em} \text{e} \hspace{1em} \left(-\dfrac{\sqrt{2}}{2},-\dfrac{\sqrt{2}}{2},1+\sqrt{2}\right){^T}\]
mentre per $x=1-y$ si ottengono i punti seguenti
\[(1,1,0){^T} \hspace{1em} \text{e} \hspace{1em} (0,1,0){^T}\]
Sostituendo i punti ottenuti all'interno della funzione si ottiene
\begin{itemize}
    \item $f \left(\dfrac{\sqrt{2}}{2},\dfrac{\sqrt{2}}{2},1-\sqrt{2}\right) = 4 - 2 \sqrt{2}$
    \item $f \left(-\dfrac{\sqrt{2}}{2},-\dfrac{\sqrt{2}}{2},1+\sqrt{2}\right) = 4 + 2 \sqrt{2}$
    \item $f(1,0,0) = f(0,1,0)=1$
\end{itemize}
Pertanto il minimo della funzione è $1$, mentre il suo massimo è $4+2 \sqrt{2}$.

\vspace{1em}
\noindent
\textbf{Esercizio 2}: Si consideri la funzione seguente
\[f(x,y) = x y^2 e^{-x^2 - y^2}\]
con
\[f : \mathbb{R}^2 \longmapsto \mathbb{R}\]
Allora calcolando il gradiente della funzione si ottiene
\[\nabla f = \left(y^2 e^{-x^2-y^2} \cdot (1-2x), 2xy e^{-x^2-y^2} \cdot (1-2y)\right)\]

\vspace{1em}
\noindent
Si dimostri il seguente limite
\[\lim_{((x,y){^T} \to (+\infty,+\infty){^T})} f(x,y) = 0\]
Per definizione si deve dimostrare che
\[\forall \epsilon > 0, \exists \delta > 0 : \vert \vert (x,y){^T} \vert \vert < \delta \hspace{1em} \text{si ha che} \hspace{1em} \vert f(x) \vert < \epsilon\]
... continua ...

\vspace{1em}
\noindent
Si calcoli, allora, l'Hessiana seguente
\[H f(t,0) = \left(
    \rowcolors{1}{white}{white}
    \begin{array}{cc}
        0 & 0\\
        0 & 5
    \end{array}
\right)\]

\vspace{1em}
\noindent
...continua... Dal momento che in precedenza si era dimostrato che
\[\lim_{((x,y){^T} \to (+\infty,+\infty){^T})} f(x,y) = 0\]
all'interno di una palla di centro $(0,0){^T}$ esiste massimo e minimo per Weierstrass.

\vspace{1em}
\noindent
Studiando il punto $(0,0){^T}$, è facile osservare che essendo $f(x,y)=xy^2 e^{-x^2-y^2}$ per $x>0$ la funzione è positiva, mentre per $x<0$ la funzione è negativa. Ciò suggerisce che l'origine non è un punto né di massimo né di minimo.

\vspace{1em}
\subsection{Calcolo integrale in $\mathbb{R}^n$}
Gli integrali che si considerano sono gli integrali di Riemann, in cui si considerano funzioni definite su un intervallo e limitate, della forma $f : [a,b] \longmapsto \mathbb{R}$ limitate.\\
Ovviamente il concetto di intervallo deve essere esteso alla dimensione $\mathbb{R}^n$, ottenendo
\[[a_1,b_1] \times [a_2,b_2] \times \dots \times [a_n,b_n]\]
chiamato $n$-rettangolo. In $\mathbb{R}$ ogni intervallo veniva suddiviso tramite una \textbf{decomposizione} in cui gli estremi di ogni sotto-intervallo prendevano il nome di nodi. In generale, in $\mathbb{R}^2$ una decomposizione di $[a_1,b_1] \times [a_2,b_2]$ è data dal prodotto cartesiano di due decomposizioni $\delta_i$ di $[a_1,b_1]$ della forma $\delta_i = \{I_i \hspace{1em} \text{con} \hspace{1em} i=1,\dots,n\}$ e $\delta_j$ di $[a_2,b_2]$ della forma $\delta_j = \{J_j \hspace{1em} \text{con} \hspace{1em} j=1,\dots,n\}$ 

\vspace{1em}
\subsubsection{Somma superiore e somma inferiore}
Per definire il concetto di somma superiore, sia $\delta \in \Delta(R)$ con $R$ un $n$-rettangolo e $\Delta(R)$ l'insieme di tutte le $\delta$ decomposizioni dell'$n$-rettangolo $R$.\\
Si definisce, quindi
\[S(f,\delta) = \sum_{i=1,\dots,n} \sum_{j=1,\dots,k} \underset{R_{i,j}}{\sup f(x,y)} \cdot m(R_{i,j})\]
con $R_{i,j} = I_i \times J_j$ e $m(R_{i,j}) = (x_i - x_{i-1}) \cdot (y_j - y_{j-1})$.\\
Per definire il concetto si somma inferiore, si considera l'estremo inferiore della $f$ nell'intervallo, ottenendo
\[s(f,\delta) = \sum_{i=1}^n \sum_{j=1}^j \underset{R_{i,j}}{\inf f(x,y)} \cdot m(R_{i,j})\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Ovviamente non tutte le decomposizioni sono confrontabili; tuttavia, si definisce il concetto di \textbf{finezza}: la decomposizione $\delta_1$ è più fine di $\delta_2$ se tutti i nodi di $\delta_2$ sono nodi di $\delta_1$.\\
Si espongono, quindi, le proprietà seguenti;
\begin{itemize}
    \item se $\delta_1$ è più fine di $\delta_2$, allora
    \begin{itemize}
        \item $s(f,\delta_1) \geq s(f,\delta_2)$
        \item $S(f,\delta_1) \leq S(f,\delta_2)$
    \end{itemize}
    \item $\forall d$ si ha che $s(f,\delta) \leq S(f,\delta)$
    \item $\forall \delta_1,\delta_2$ si ha che
    \[s(f,\delta_1) \leq S)f,\delta_2\]
\end{itemize}
Definendo i due insiemi seguenti
\[\sigma = \{s(f,\delta) : \delta \in \Delta(R)\} \hspace{1em} \text{e} \hspace{1em} \Sigma = \{S(f,\delta) : \delta \in \Delta (R)\}\]
per cui $\sigma$ e $\Sigma$ sono insiemi separati in $\mathbb{R}$.\\
Si definisce, allora, l'\textbf{integrale inferiore} di $f$ su $\mathbb{R}$
\[\int_R^- f \dif m = \text{sup} \sigma\]
e l'\textbf{integrale superiore} di $f$ su $\mathbb{R}$ come
\[\int_R^+ f \dif m = \text{inf} \Sigma\]
Se questi coincidono, allora $f$ è integrabile secondo Riemann su $R$ e si ha che
\[\int_R f \dif m = \int_R^- f \dif m = \int_R^+ f \dif m\]
ciò accade quando $\sigma$ e $\Sigma$ sono insiemi contigui.

\vspace{1em}
\noindent
\textbf{Esercizio}: Sia $f$ una funzione definita come segue
\[f : R \subset \mathbb{R}^2 \longmapsto \mathbb{R}\]
con $f(x,y) \geq 0$, allora si ha che
\[\int_R f \dif m\]
rappresenta il volume delle ragioni di piano occupata da $R$ e il grafico di $f$.

\vspace{1em}
\noindent
\textbf{Esempio}: Sia data $f(x,y) = 1$, allora
\[\int_{[a,b] \times [c,d]} f \dif m = (b-a) \cdot (d-c)\]
ossia l'area di $R$.\\
Ovviamente la notazione
\[\int_R f \dif m = \iint_R f(x,y) \dif m\]

\vspace{1em}
\subsubsection{Proprietà degli integrali}
Sia dato
\[\mathcal{R}(R) = \{f : \mathbb{R} \longmapsto \mathbb{R}, \hspace{1em} \text{con} \hspace{0.25em} f \hspace{0.25em} \text{integrabile}\}\]
ossia l'insieme delle funzioni integrabili secondo Riemann su $R$.\\
Si hanno le seguenti proprietà
\begin{itemize}
    \item Date due funzioni $f,g \in \mathcal{R}(R)$ e $\alpha, \beta \in \mathbb{R}$ e $\alpha f + \beta g \in \mathcal{R}(R)$ vale la linearità, ossia
    \item \[\int_R (\alpha f + \int_R \beta g) \dif m = \alpha \cdot \int_R f \dif m + \beta \int_R g \dif m\]

    \item Il prodotto di due funzioni integrabili è integrabile, ossia
    \[f,g \in \mathcal{R}(R) \longmapsto f \cdot g \in \mathbb{R}(R)\]

    \item integrabilità della restrizione: dato $R_1 \subset R$, allora se $f \in \mathcal{R}(R)$, allora $f \vert_{R_1} \in \mathcal{R}(R_1)$

    \item additività dell'integrale: posto $R = R_1 \cup R_2$ e l'insieme $R_1 \cap R_2$ ha \textbf{interno vuoto}, allora
    \[f \in \mathcal{R}(R) \hspace{1em} \textbf{se e solo se} \hspace{1em} f \vert_{R_1} \in \mathcal{R}(R_1) \hspace{1em} \text{e} \hspace{1em} f \vert_{R_2} \in \mathcal{R}(R_2)\]
    e inoltre si ha che
    \[\int_{R} f \dif m = \int_{R_1} f \vert_{R_1} \dif m + \int_{R_2} f \vert_{R_2} \dif m\]

    \item la monotonia, per cui $f,g \in \mathcal{R}(R)$, con $x \in \mathbb{R}^n$, allora se
    \item \[f(x) \leq g(x), \hspace{1em} \forall x \in R\]
    allora si ha che
    \[\int_R f \dif m \leq \int_R g \dif m\]
    inoltre se si ha che
    \[f(x) < g(x) \hspace{1em} \text{con} \hspace{1em} x \in R\]
    e $f,g$ \textbf{continue}, allora
    \[\int_R f \dif m < \int_R g \dif m\]
\end{itemize}

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Si ha che $g(x) - f(x) > 0$, perché sono funzioni continue, per cui per il teorema della permanenza del segno esistono $\epsilon > 0$ e un intorno di $R_1$ di $x_0$ tale che $g(x)-f(x) \geq \epsilon$ con $\forall x \in R_1$.\\
Pertanto si può scrivere
\[\int_R f \dif m = \int_{R_1} f \dif m + \int_{R-R_1} f \dif m \leq \int_{R_1} (g-\epsilon) \dif m + \int_{R-R_1} (g) \dif m = \epsilon \cdot m(R_1) + \int_R f \dif m\]

\newpage
\noindent
\begin{center}
    15 Novembre 2022
\end{center}
\textbf{Esercizio 1}: Si consideri la funzione seguente
\[f(x,y) = \sqrt{1-x^2-y^2}-x\]
\begin{itemize}
    \item si determini il dominio, il gradiente e la matrice hessiana di $f$. Si considerino i punti critici di $f$ e anche la loro natura, ma anche gli estremi assoluti della $f$ su $D$
    
    \vspace{1em}
    \noindent
    Il dominio della $f$ è dato dalla non negatività dell'argomento della radice, per cui $x^2+y^2 \leq 1$.\\
    Il gradiente della $f$ è dato da
    \[\nabla f = \left(\dfrac{x}{\sqrt{1-x^2-y^2}}-1, \dfrac{y}{\sqrt{1-x^2-y^2}}\right){^T}\]
    La matrice Hessiana è
    \[\left(
    \rowcolors{1}{white}{white}    
    \begin{array}{cc}
        a & b\\
        c & d
    \end{array}
    \right)\]
    Per la determinazione dei punti critici si vede quando si annulla il gradiente
    \[\left\{
    \rowcolors{1}{white}{white}    
    \begin{array}{ll}
        -x=\sqrt{1-x^2-y^2}\\
        y =0\\
    \end{array}
    \right.\]
    per cui si ottiene $\left(-\dfrac{1}{\sqrt{2}}, 1\right){^T}$. Tuttavia Fermat permette di considerare punti critici solamente sull'aperto ed essendo il dominio compatto bisogna considerare anche il bordo, ossia $x^2+y^2=1$; si tratta di un problema vincolato, per cui si parametrizza il vincolo, ottenendo
    \[x=\cos(t) \hspace{1em} \text{e} \hspace{1em} y=\sin(t)\]
    ecco che quindi la funzione diviene
    \[f(\cos(t),\sin(t))=-\cos(t)\]
    si ottengono, quindi, i seguenti punti critici: ...continua...\\
    Si consideri la seguente restrizione
    \[E = \{(x,y){^T} \in 0 : y \leq x\}\]
    È immediato osservare che il punto di minimo che si sta cercando è lo stesso della funzione di partenza. Similmente, il punto di massimo può stare solamente sulla bisettrice, in quanto già in precedenza è stato effettuato lo studio su tutto il dominio. Si consideri, allora, la restrizione della funzione $f$ alla retta $y=x$, per cui si ottiene
    \[f \vert_{x=y} = \sqrt{1-2x^2}-x\]
    Calcolandone la derivata si ottiene che essa si annulla per $x=\pm \dfrac{\sqrt{6}}{6}$. Tuttavia $x$ deve essere negativo, per cui si può scartare la soluzione positiva. Per vedere se la soluzione negativa è dentro al dominio, si considerano le intersezioni di $y=x$ con la circonferenza $x^2+y^2=1$, ottenendo $x=\pm \dfrac{1}{\sqrt{2}}$: ovviamente si ha che $-\dfrac{1}{\sqrt{2}} \leq \dfrac{1}{\sqrt{6}} \leq \dfrac{1}{\sqrt{2}}$, per cui il punto considerato è valido. Tuttavia, ancora una volta, si è applicato Fermat per individuare il punto $\dfrac{1}{\sqrt{6}}$, per cui bisogna considerare anche gli estremi. Pertanto si ottiene
    che il punto di massimo è
    \[\dfrac{1}{\sqrt{6}}\]
\end{itemize}

\vspace{1em}
\subsubsection{Integrabilità del valore assoluto}
Sia $f \in \mathcal{R}(R)$ allora si ha che
\[\vert f \vert \in \mathcal{R}(R)\]
Si ha che
\[\left \vert \int_{R} f \dif m \right \vert \leq \int_R \vert f \vert \dif m\]

\vspace{1em}
\subsubsection{Teorema della media}
Sia $f \in \mathcal{R}(R)$, allora si ha che
\[\underset{R}{\text{inf}} f \leq \dfrac{\displaystyle{\int_R f \dif m}}{m(R)} \leq \underset{R}{\text{sup}} f\]
Inoltre, se $f$ è continua, esiste un punto $x_0 \in R$ tale che
\[f(x_0) = \dfrac{\displaystyle{\int_R f \dif m}}{m(R)}\]

% Formattazione per la dimostrazione, etc.
\vspace{2em}
\noindent
\normalfont \normalsize
\textsc{Dimostrazione}: Naturalmente si ha che
\[\underset{R}{\text{inf}} f \cdot m(R) \leq \int_R f \dif m \leq \underset{R}{\text{sup}} f \cdot m(R)\]
ma naturalmente si ha che $\underset{R}{\text{inf}} f \cdot m(R) = s(f,\delta)$ e $\underset{R}{\text{sup}} f \cdot m(R) = S(f,\delta)$ con $\delta$ decomposizione banale $\{R\}$.\\
Naturalmente, poi, essendo $R$ compatto, $\exists$ $\min f$ e $\max f$; non solo, ma essendo $R$ connesso, anche $f(R)$ è un intervallo, per cui
\[f(R) = [\min f, \max f]\]
quindi, poiché
\[\dfrac{\displaystyle{\int_R f \dif m}}{m(R)} \in f(R)\]
esiste un punto $x_0$ ... continua ...

\vspace{2em}
\noindent
\textbf{Esercizio}: Si calcoli l'integrale seguente
\[\iint_{[0,1] \times [0,1]} x \dif x \dif y = 1\]
in cui si è usata la geometria per calcolare l'integrale, in quanto l'integrale doppio rappresenta il volume.\\
Tuttavia, non è sempre possibile impiegare la geometria per il calcolo dell'integrale.

\vspace{2em}
\noindent
\subsection{Teorema di riduzione di Fubini per gli integrali doppi sui rettangoli}
Sia $f : [a_1,b_1] \times [a_2,b_2] \longmapsto \mathbb{R}$ integrabile. Si supponga che $\forall y \in [a_2,b_2]$
la funzione
\[f(\cdot,y) : [a_1,b_1] \longmapsto \mathbb{R}\]
ossia la funzione in cui $x$ si muove e $y$ rimane costante, sia integrabile su $[a_1,b_1]$. È possibile, quindi, definire
\[g : [a_1,b_1] \longmapsto \mathbb{R}\]
come
\[g(y) = \int_{[a_1,b_1]} f(x,y) \dif x\]
Allora la funzione $g$ è integrabile su $[a_2,b_2]$ e si ha che
\[\int_{[a_2,b_2]} g(y) \dif y = \iint_R f \dif m\]
In definitiva, si ha quindi
\[\int_{[a_2,b_2]} \left(\int_{[a_1,b_1]} f(x,y) \dif x\right) \dif y = \iint_{[a_1,b_1] \times [a_2,b_2]} f(x,y) \dif x \dif y\]

\vspace{1em}
\noindent
\textbf{Osservazione}: Si osservi che il teorema risulta essere ugualmente valido cambiando il ruolo di $x$ e $y$. Pertanto sia $f : [a_1,b_1] \times [a_2,b_2] \longmapsto \mathbb{R}$ integrabile. Si supponga che $\forall x \in [a_1,b_1]$
la funzione
\[f(x,\cdot) : [a_2,b_2] \longmapsto \mathbb{R}\]
ossia la funzione in cui $y$ si muove e $x$ rimane costante, sia integrabile su $[a_2,b_2]$. È possibile, quindi, definire
\[h : [a_2,b_2] \longmapsto \mathbb{R}\]
come
\[h(x) = \int_{[a_2,b_2]} f(x,y) \dif y\]
Allora la funzione $h$ è integrabile su $[a_1,b_1]$ e si ha che
\[\int_{[a_1,b_1]} h(x) \dif x = \iint_R f \dif m\]

\vspace{2em}
\noindent
\textbf{Esempio 1}: Si consideri l'integrale seguente
\[\iint_{\left[0,\frac{\pi}{2}\right] \times \left[0,\pi\right]} \sin(x+y) \dif x \dif y\]
Tale integrale può essere ridotto per Fubini, come
\[\int_{\left[0,\pi\right]} \left(\iint_{\left[0,\frac{\pi}{2}\right]} \sin(x+y) \dif x \right) \dif y\]
Ovviamente si ha che
\[\int_{\left[0,\frac{\pi}{2}\right]} \sin(x+y) \dif x = \left[-\cos(x+y)\right]_0^{\frac{\pi}{2}} = - \cos\left(\frac{\pi}{2}+y\right) + \cos(y)\]
Ora si procede ad integrare per $y$, da cui
\[\int_{[0,\pi]} \left[- \cos\left(\frac{\pi}{2}+y\right) + \cos(y)\right] \dif y = -\left[\sin\left(\frac{\pi}{2}+y\right)\right]_0^\pi + \left[\sin(y)\right]_0^\pi = 2\]
Ovviamente si sarebbe potuto anche considerare l'integrazione prima in $y$ e poi in $x$.

\vspace{2em}
\noindent
\textbf{Esempio 2}: Si consideri l'integrale seguente
\[\iint_{\left[0,2\right] \times \left[0,1\right]} xy \dfrac{x^2-y^2}{(x^2+y^2)^3} \dif x \dif y\]
Naturalmente per Fubini tale integrale può essere ridotto come
\[\int_0^2 \left(\int_0^1 xy \dfrac{x^2-y^2}{(x^2+y^2)^3} \dif y\right) \dif x\]
Si risolva, quindi, l'integrale seguente
\[\int_0^1 xy \dfrac{x^2-y^2}{(x^2+y^2)^3} \dif y\]
È possibile effettuare un cambio di variabile, ponendo $u=x^2+y^2$ da cui $\dif u = 2y \dif y$ e, ovviamente $x^2-y^2=2x^2-u$; non solo, calcolando gli estremi di integrazione si ottiene
\begin{itemize}
    \item se $y=0$, allora, essendo $x^2+y^2=u$, si ottiene $u=x^2$;
    \item se $y=1$, allora, essendo $x^2+y^2=u$, si ottiene $u=x^2+1$.
\end{itemize}
Pertanto si ottiene che
\[\int_{x^2}^{x^2+1}\]
 ... continua...

\vspace{2em}
\noindent
\textbf{Osservazione}: Come si può osservare, i due integrali risultano differenti; questo perché la funzione non è integrabile sul rettangolo $R=[0,2] \times [0,1]$, in quanto è illimitata in $0,0$.\\
Per vederlo è sufficiente considerare la restrizione $x=2y$, per cui la funzione si comporta come $\dfrac{1}{y^2}$.

\vspace{2em}
\noindent
\textbf{Esercizio}: Si consideri l'integrale seguente
\[\iint_{[0,1]\times[1,2]} e^{\frac{x}{y}} y^{-3} \dif x \dif y\]
Se si procedesse ad effettuare il calcolo seguente
\[\int_0^1 \left(\int_1^2 e^{\frac{x}{y}} y^{-3} \dif y\right) \dif x\]
non se ne uscirebbe. Tuttavia, è possibile procedere anche ad un integrazione diversa, quale
\[\int_1^2 \left(\int_0^1 e^{\frac{x}{y}} y^{-3} \dif x\right) \dif y\]
che è molto più semplice, in quanto
\[\int_0^1 e^{\frac{x}{y}} y^{-3} \dif x = \left[e^{\frac{x}{y}} y^{-2}\right]_0^1 = e^{\frac{1}{y}} y^{-2} - y^{-2}\]
per cui ora si esegue
\[\int_1^2 \left[e^{\frac{1}{y}} y^{-2} - y^{-2}\right] \dif y = \left[-e^{\frac{1}{y}}\right]_1^2 + \left[\dfrac{1}{y}\right]_1^2 = e-\sqrt{e} -\dfrac{1}{2}\]

\vspace{1em}
\subsection{Teorema di Fubini per gli integrali tripli sui $3$-rettangoli per corde e per sezioni}
È possibile, ora, estendere il ragionamento precedentemente formulato per gli integrali doppi anche per gli integrali tripli, effettuando un'\textbf{integrazione per corde} o un'\textbf{integrazione per sezioni}

\vspace{1em}
\noindent
\textbf{Per corde}: Sia $R = [a_1,b_1] \times [a_2,b_2] \times [a_3,b_3]$ e sia $f : R \longmapsto \mathbb{R}$ integrabile. Per ogni punto
\[(x,y){T} \in [a_1,b_1] \times [a_2,b_2]\]
si supponga che la funzione
\[f(x,y,\cdot) : [a_3,b_3] \longmapsto \mathbb{R}\]
sia integrabile. Allora la funzione
\[g : [a_1,b_1] \times [a_2,b_2] \longmapsto \mathbb{R}\]
definita come
\[g(x,y) = \int_{[a_3,b_3]} f(x,y,z) \dif z\]
è integrabile e vale la formula
\[\iint_{[a_1,b_1] \times [a_2,b_2]} g(x,y) \dif x \dif y = \iint_R f \dif m\]
per cui in definitiva si ha che
\[\iiint_R f(x,y,z) \dif x \dif y \dif z = \int_{[a_1,b_1] \times [a_2,b_2]} \left(\int_{a_3,b_3} f(x,y,z) \dif x\right) \dif x \dif y\]

\vspace{1em}
\noindent
\textbf{Per sezioni}: Sia $R = [a_1,b_1] \times [a_2,b_2] \times [a_3,b_3]$ e sia $f : R \longmapsto \mathbb{R}$ integrabile. Per ogni punto
\[z \in [a_3,b_3]\]
si supponga che la funzione
\[f(\cdot,\cdot,z) : [a_1,b_1] \times [a_2,b_2] \longmapsto \mathbb{R}\]
sia integrabile. Allora la funzione
\[g : [a_3,b_3] \longmapsto \mathbb{R}\]
definita come
\[g(z) = \int_{[a_1,b_1]\times[a_2,b_2]} f(x,y,z) \dif x \dif y\]
è integrabile e vale la formula
\[\int_{[a_3,b_3]} g(z) \dif z = \iint_R f \dif m\]
per cui in definitiva si ha che
\[\iiint_R f(x,y,z) \dif x \dif y \dif z = \int_{[a_3,b_3]} \left(\iint_{[a_1,b_1] \times [a_2,b_2]} f(x,y,z) \dif x \dif y\right) \dif z\]

\vspace{1em}
\noindent
\textbf{Esempio}: Si consideri il rettangolo seguente $R=[0,1]\times[0,2]\times[0,3]$ e la funzione
\[f(x,y,z) = x^3 \cdot (y^2 + z)\]
Si può effettuare l'integrale per corde, per cui
\[\iiint_R f \dif m = \int_0^1 \left(\iint_{[0,2] \times [0,3]} f(x,y,z) \dif y \dif z\right) \dif x\]
naturalmente l'integrale doppio interno può essere ulteriormente ridotto, ottenendo
\[\int_0^1 \left(\int_0^2 \left( \int_0^3 f(x,y,z) \dif z \right) \dif y\right) \dif x\]
Ora, quindi, si può scrivere
\[\int_0^1 x^3 \cdot \left(\int_0^2 \left( \int_0^3 y^2 + z \dif z \right) \dif y\right) \dif x = \int_0^1 x^3 \cdot \left(\int_0^2 \left[y^2z + \dfrac{z^2}{2}\right]_0^3 \dif y\right) \dif x\]
passando al successivo integrale
\[\int_0^1 x^3 \cdot \left(\int_0^2 3y^2 + \dfrac{9}{2} \dif y\right) \dif x = \int_0^1 x^3 \cdot \left[y^3 + \dfrac{9}{2}y\right]_0^2 \dif x\]
passando al successivo integrale
\[\int_0^1 17x^3 \dif x = \dfrac{17}{4}\]
\end{document}